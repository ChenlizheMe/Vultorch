{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vultorch Tutorial","text":"<p>A step-by-step guide through Vultorch, one example at a time. Each chapter maps to a runnable script in the <code>examples/</code> folder.</p> Chapter Topic Key concepts 01 \u2014 Hello Tensor Minimal display View, Panel, Canvas, bind, run 02 \u2014 Multi-Panel Multiple panels &amp; canvases Layout, side, multi-canvas 03 \u2014 Training Test Fit a tiny network to a GT image custom dock layout, create_tensor, per-pixel optimization <p>More chapters coming soon.</p>"},{"location":"01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>Example file: <code>examples/01_hello_tensor.py</code></p> <p>Tired of every research repo inventing its own janky tensor viewer with matplotlib hacks and <code>cv2.imshow</code> spaghetti?  Yeah, us too.</p> <p>Vultorch gets your CUDA tensor on screen in four lines \u2014 no saving PNGs, no CPU round-trips, no <code>plt.pause(0.001)</code> nonsense.</p>"},{"location":"01_hello_tensor/#the-mental-model","title":"The mental model","text":"<p>There are exactly four objects you need to know:</p> Object What it is One-liner View The OS window <code>vultorch.View(\"title\", w, h)</code> Panel A dockable sub-window inside the View <code>view.panel(\"name\")</code> Canvas A GPU image slot inside a Panel <code>panel.canvas(\"name\")</code> bind() Connects a tensor to a Canvas <code>canvas.bind(t)</code> <p>Chain them together, call <code>run()</code>, done.</p>"},{"location":"01_hello_tensor/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\n# A 256\u00d7256 RGB gradient \u2014 any (H,W,C) float32 CUDA tensor works\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # blocks until you close the window\n</code></pre> <p>That's it. No event loop boilerplate, no <code>begin_frame()</code> / <code>end_frame()</code>.</p>"},{"location":"01_hello_tensor/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 we made an RGB gradient on CUDA. Vultorch accepts <code>(H,W)</code>,    <code>(H,W,1)</code>, <code>(H,W,3)</code>, or <code>(H,W,4)</code>, in float32 / float16 / uint8.    It handles RGBA expansion for you.</p> </li> <li> <p>Object tree \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>.    The canvas auto-fills its panel by default (<code>fit=True</code>).</p> </li> <li> <p>Run \u2014 <code>view.run()</code> enters a blocking event loop. Every frame the    canvas re-uploads the bound tensor and renders it. Close the window    to exit.</p> </li> </ol> <p>Tip</p> <p>The four setup lines collapse into a one-liner if you're feeling fancy: <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"02_multi_panel/","title":"02 \u2014 Multi-Panel","text":"<p>Example file: <code>examples/02_imgui_controls.py</code></p> <p>One panel is nice. But in practice you want to see your loss map, gradient field, and output side-by-side \u2014 without writing a single line of layout code.</p> <p>Good news: Vultorch panels are dockable. Just create them and they'll arrange themselves. Users can drag, resize, and rearrange at will.</p> <p>This chapter shows two patterns:</p> <ul> <li>One canvas per panel \u2014 three panels stacked, each with its own tensor.</li> <li>Multiple canvases in one panel \u2014 one panel, three canvases, auto-split.</li> </ul>"},{"location":"02_multi_panel/#layout","title":"Layout","text":"<p>Here's what the window looks like:</p> Left side (main area) Right side (<code>side=\"right\"</code>) Red panel \u2014 <code>red_img</code> Combined panel Green panel \u2014 <code>green_img</code> <code>c_red</code> canvas Blue panel \u2014 <code>blue_img</code> <code>c_green</code> canvas <code>c_blue</code> canvas <p>Left: 3 separate panels, each one canvas. Right: 1 panel, 3 canvases sharing space.</p>"},{"location":"02_multi_panel/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# Three different tensors\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# Left: 3 panels, each with one canvas\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# Right: 1 panel with 3 canvases (auto-split vertically)\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"02_multi_panel/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Auto-layout \u2014 panels without <code>side=</code> stack vertically.    Add <code>side=\"right\"</code> (or <code>\"left\"</code>) + <code>width=0.5</code> to dock to one side.</p> </li> <li> <p>Multi-canvas \u2014 call <code>panel.canvas()</code> multiple times.    When several canvases have <code>fit=True</code> (the default), they split the    vertical space equally. No manual height math.</p> </li> <li> <p>Still no callback \u2014 static data only needs <code>bind()</code> + <code>run()</code>.    Dynamic updates come in a later chapter.</p> </li> <li> <p>Drag &amp; drop \u2014 all panels are dockable. Users can rearrange,    float, or resize them at runtime.</p> </li> </ol> <p>Note</p> <p>The same tensor can be bound to multiple canvases at once \u2014 <code>red</code> appears in both the left panel and the right combined panel.</p>"},{"location":"03_training_test/","title":"03 \u2014 Training Test","text":"<p>Example file: <code>examples/03_training_test.py</code></p> <p>Ever stared at a wall of decreasing loss numbers in your terminal for ten minutes, feeling confident, only to discover the model's output is a solid grey rectangle? Yeah, us too.</p> <p>Reading loss values off a scrolling console is about as reliable as reading tea leaves. This chapter puts GT and prediction side by side on screen so you can see whether the network is actually learning.</p>"},{"location":"03_training_test/#what-were-building","title":"What we're building","text":"<p>A tiny MLP (2 \u2192 64 \u2192 64 \u2192 3) fitting a 256\u00d7256 PyTorch logo in real time. The window has three panels:</p> Area Content Left GT panel \u2014 the target image (what you're fitting) Right Prediction panel \u2014 live network output, updated every frame Bottom Info panel \u2014 FPS, loss, iteration, progress bar, and a slider <p>Everything on screen, nothing buried in the terminal.</p>"},{"location":"03_training_test/#new-friends","title":"New friends","text":"<p>Chapters 01 and 02 were all static \u2014 <code>bind()</code> + <code>run()</code>, done. This time we bring two new toys:</p> New thing What it does How to use on_frame Per-frame callback \u2014 train and update here <code>@view.on_frame</code> create_tensor GPU shared-memory tensor <code>vultorch.create_tensor(H, W, ...)</code> <p>Write any PyTorch code inside the callback; Vultorch handles the tensor-to-screen dance every frame.</p>"},{"location":"03_training_test/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\nui = vultorch.ui\n\ntry:\n    from PIL import Image\nexcept ImportError as exc:\n    raise RuntimeError(\"Please install pillow: pip install pillow\") from exc\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# Load the target image\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\nimg = Image.open(img_path).convert(\"RGB\").resize((256, 256), Image.BILINEAR)\ngt = torch.from_numpy(np.asarray(img, dtype=np.float32) / 255.0).to(device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)  # (H*W, 2)\ntarget = gt.reshape(-1, 3)                               # (H*W, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ngt_panel = view.panel(\"GT\")\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# 4 channels \u2014 zero-copy GPU display path\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n    \"layout_done\": False,\n}\n\n\n@view.on_frame\ndef train_and_render():\n    # ---- first-frame layout: top row left/right, bottom info ----\n    if not state[\"layout_done\"]:\n        dockspace_id = ui.dock_space_over_viewport(flags=8)\n        ui.dock_builder_remove_node(dockspace_id)\n        ui.dock_builder_add_node(dockspace_id, 1 &lt;&lt; 10)\n        ui.dock_builder_set_node_size(dockspace_id, 1280.0, 760.0)\n\n        info_node, top_node = ui.dock_builder_split_node(dockspace_id, 3, 0.28)\n        left_node, right_node = ui.dock_builder_split_node(top_node, 0, 0.5)\n\n        ui.dock_builder_dock_window(\"GT\", left_node)\n        ui.dock_builder_dock_window(\"Prediction\", right_node)\n        ui.dock_builder_dock_window(\"Info\", info_node)\n        ui.dock_builder_finish(dockspace_id)\n        state[\"layout_done\"] = True\n\n    # ---- train a few steps ----\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    # ---- write prediction into the display tensor ----\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n    # ---- Info panel ----\n    ui.begin(\"Info\", True, 0)\n    ui.text(f\"FPS: {view.fps:.1f}\")\n    ui.text(f\"Iteration: {state['iter']}\")\n    ui.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    ui.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = ui.slider_int(\n        \"Steps / Frame\", state[\"steps_per_frame\"], 1, 32\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    ui.progress_bar(progress, overlay=f\"Training progress {progress * 100:.1f}%\")\n    ui.text_wrapped(\n        \"Left is GT, right is prediction. Increase 'Steps / Frame' for faster fitting.\"\n    )\n    ui.end()\n\n\nview.run()\n</code></pre> <p>That's it. Run it and watch the grey blob on the right morph into the PyTorch logo in a few seconds.</p>"},{"location":"03_training_test/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 PIL loads the image, we convert it to a float32 CUDA tensor    for GT. Pixel coordinates get <code>meshgrid</code>'d into <code>(H*W, 2)</code>, normalized    to <code>[-1, 1]</code>.</p> </li> <li> <p>Model \u2014 a two-hidden-layer MLP (64 wide). Takes <code>(x, y)</code>, outputs    <code>(r, g, b)</code>. Small enough to run inside a per-frame callback without    tanking your framerate.</p> </li> <li> <p>on_frame callback \u2014 called once per frame. It does three things:    set up the dock layout on the first frame, run N training steps, then    write the prediction into <code>pred_rgba</code>.</p> </li> <li> <p>Info panel \u2014 drawn with ImGui's <code>ui.begin()</code> / <code>ui.end()</code>. Text,    sliders, progress bars \u2014 any widget you want, right in the window.</p> </li> </ol>"},{"location":"03_training_test/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 you can run arbitrary PyTorch code in the    callback. At the end of each frame, Vultorch uploads every bound    tensor to the screen automatically.</p> </li> <li> <p><code>create_tensor</code> \u2014 looks and feels like <code>torch.zeros</code>, but the    underlying memory is Vulkan/CUDA shared. Display is zero-copy.</p> </li> <li> <p>Manual docking \u2014 <code>dock_builder_split_node</code> lets you slice the    window any way you like. Directions: <code>0=left, 1=right, 2=up, 3=down</code>,    ratio is a float.</p> </li> <li> <p>No terminal spam \u2014 all live stats live in the Info panel.    Your console stays clean for warnings and tracebacks.</p> </li> </ol> <p>Tip</p> <p>Crank <code>Steps / Frame</code> up to 32 for blazing-fast convergence. But don't get too greedy \u2014 go too high and your framerate will drop because each frame spends more time training.</p> <p>Note</p> <p><code>create_tensor</code> is called once at init, not every frame. After that you just write into the tensor each frame \u2014 practically free.</p>"},{"location":"zh/","title":"Vultorch \u6559\u7a0b","text":"<p>\u9010\u6b65\u5b66\u4e60 Vultorch\uff0c\u6bcf\u4e2a\u7ae0\u8282\u5bf9\u5e94 <code>examples/</code> \u76ee\u5f55\u4e2d\u7684\u4e00\u4e2a\u53ef\u8fd0\u884c\u811a\u672c\u3002</p> \u7ae0\u8282 \u4e3b\u9898 \u6838\u5fc3\u6982\u5ff5 01 \u2014 Hello Tensor \u6700\u5c0f\u793a\u4f8b View, Panel, Canvas, bind, run 02 \u2014 \u591a\u9762\u677f \u591a\u9762\u677f\u4e0e\u591a\u753b\u5e03 \u5e03\u5c40, side, \u591a\u753b\u5e03 03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5 \u62df\u5408 GT \u56fe\u50cf \u81ea\u5b9a\u4e49\u505c\u9760\u5e03\u5c40, create_tensor, \u9010\u50cf\u7d20\u4f18\u5316 <p>\u66f4\u591a\u7ae0\u8282\u5373\u5c06\u63a8\u51fa\u3002</p>"},{"location":"zh/01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/01_hello_tensor.py</code></p> <p>\u4f60\u662f\u5426\u53d7\u591f\u4e86\u6bcf\u4e2a\u4ed3\u5e93\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u5b9e\u73b0\u7684\u53ef\u89c6\u5316\u65b9\u6848 \u2014\u2014 \u8fd9\u4e2a\u7528 matplotlib \u624b\u52a8\u5237\u65b0\uff0c \u90a3\u4e2a\u7528 <code>cv2.imshow</code> \u7136\u540e <code>waitKey(1)</code>\uff0c\u8fd8\u6709\u7684\u76f4\u63a5\u5b58 PNG \u8ba9\u4f60\u5f00\u4e2a\u56fe\u7247\u67e5\u770b\u5668\uff1f</p> <p>Vultorch \u53ea\u9700 \u56db\u884c\u4ee3\u7801 \u5c31\u628a CUDA tensor \u642c\u5230\u5c4f\u5e55\u4e0a\u3002\u4e0d\u5b58\u56fe\u3001\u4e0d\u8fc7 CPU\u3001 \u4e0d\u9700\u8981 <code>plt.pause(0.001)</code> \u8fd9\u79cd\u9ed1\u9b54\u6cd5\u3002</p>"},{"location":"zh/01_hello_tensor/#_1","title":"\u4f60\u9700\u8981\u8bb0\u4f4f\u7684\u4e1c\u897f","text":"<p>\u4e00\u5171\u5c31\u56db\u4e2a\u5bf9\u8c61\uff1a</p> \u5bf9\u8c61 \u662f\u4ec0\u4e48 \u5199\u6cd5 View \u64cd\u4f5c\u7cfb\u7edf\u7a97\u53e3 <code>vultorch.View(\"title\", w, h)</code> Panel View \u91cc\u53ef\u505c\u9760\u7684\u5b50\u7a97\u53e3 <code>view.panel(\"name\")</code> Canvas Panel \u91cc\u7684 GPU \u56fe\u50cf\u69fd\u4f4d <code>panel.canvas(\"name\")</code> bind() \u628a tensor \u8fde\u5230 Canvas \u4e0a <code>canvas.bind(t)</code> <p>\u94fe\u8d77\u6765\uff0c\u8c03 <code>run()</code>\uff0c\u6536\u5de5\u3002</p>"},{"location":"zh/01_hello_tensor/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\n# \u4e00\u5f20 256\u00d7256 \u7684 RGB \u6e10\u53d8 \u2014 \u4efb\u4f55 (H,W,C) float32 CUDA tensor \u90fd\u884c\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # \u963b\u585e\uff0c\u76f4\u5230\u4f60\u5173\u95ed\u7a97\u53e3\n</code></pre> <p>\u6ca1\u4e86\u3002\u4e0d\u9700\u8981\u624b\u5199\u4e8b\u4ef6\u5faa\u73af\uff0c\u4e0d\u9700\u8981 <code>begin_frame()</code> / <code>end_frame()</code>\u3002</p>"},{"location":"zh/01_hello_tensor/#_3","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 \u6211\u4eec\u5728 CUDA \u4e0a\u9020\u4e86\u4e00\u5f20 RGB \u6e10\u53d8\u3002Vultorch \u652f\u6301    <code>(H,W)</code> / <code>(H,W,1)</code> / <code>(H,W,3)</code> / <code>(H,W,4)</code>\uff0c    float32 / float16 / uint8 \u90fd\u884c\uff0cRGBA \u6269\u5c55\u5b83\u81ea\u5df1\u641e\u5b9a\u3002</p> </li> <li> <p>\u5bf9\u8c61\u6811 \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>\u3002    \u753b\u5e03\u9ed8\u8ba4\u94fa\u6ee1\u6574\u4e2a\u9762\u677f\uff08<code>fit=True</code>\uff09\u3002</p> </li> <li> <p>\u8fd0\u884c \u2014 <code>view.run()</code> \u8fdb\u5165\u963b\u585e\u4e8b\u4ef6\u5faa\u73af\uff0c\u6bcf\u5e27\u91cd\u65b0\u4e0a\u4f20 tensor \u5e76\u6e32\u67d3\u3002    \u5173\u7a97\u53e3\u5c31\u9000\u51fa\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u56db\u884c\u521d\u59cb\u5316\u53ef\u4ee5\u538b\u6210\u4e00\u884c\uff0c\u5982\u679c\u4f60\u559c\u6b22\u70ab\u6280\u7684\u8bdd\uff1a <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"zh/02_multi_panel/","title":"02 \u2014 \u591a\u9762\u677f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/02_imgui_controls.py</code></p> <p>\u4e00\u4e2a\u9762\u677f\u633a\u597d\u7684\u3002\u4f46\u5b9e\u9645\u505a\u7814\u7a76\u7684\u65f6\u5019\uff0c\u4f60\u60f3\u540c\u65f6\u770b loss map\u3001\u68af\u5ea6\u573a\u3001 \u6a21\u578b\u8f93\u51fa \u2014\u2014 \u800c\u4e14\u6700\u597d\u4e0d\u7528\u81ea\u5df1\u5199\u4efb\u4f55\u5e03\u5c40\u4ee3\u7801\u3002</p> <p>\u597d\u6d88\u606f\uff1aVultorch \u7684\u9762\u677f\u662f\u53ef\u505c\u9760\u7684\u3002\u4f60\u53ea\u7ba1\u521b\u5efa\uff0c\u5b83\u4eec\u81ea\u5df1\u6392\u597d\u3002 \u7528\u6237\u8fd0\u884c\u65f6\u8fd8\u80fd\u968f\u4fbf\u62d6\u3001\u968f\u4fbf\u62c9\u3002</p> <p>\u672c\u7ae0\u6f14\u793a\u4e24\u79cd\u6a21\u5f0f\uff1a</p> <ul> <li>\u6bcf\u4e2a\u9762\u677f\u4e00\u4e2a\u753b\u5e03 \u2014 \u4e09\u4e2a\u9762\u677f\u5782\u76f4\u5806\u53e0\uff0c\u5404\u81ea\u4e00\u5f20\u56fe\u3002</li> <li>\u4e00\u4e2a\u9762\u677f\u591a\u4e2a\u753b\u5e03 \u2014 \u4e00\u4e2a\u9762\u677f\u91cc\u585e\u4e09\u4e2a\u753b\u5e03\uff0c\u81ea\u52a8\u5747\u5206\u3002</li> </ul>"},{"location":"zh/02_multi_panel/#_1","title":"\u5e03\u5c40","text":"<p>\u7a97\u53e3\u957f\u8fd9\u6837\uff1a</p> \u5de6\u4fa7\uff08\u4e3b\u533a\u57df\uff09 \u53f3\u4fa7\uff08<code>side=\"right\"</code>\uff09 Red \u9762\u677f \u2014 <code>red_img</code> Combined \u9762\u677f Green \u9762\u677f \u2014 <code>green_img</code> <code>c_red</code> \u753b\u5e03 Blue \u9762\u677f \u2014 <code>blue_img</code> <code>c_green</code> \u753b\u5e03 <code>c_blue</code> \u753b\u5e03 <p>\u5de6\u8fb9\uff1a3 \u4e2a\u72ec\u7acb\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\u3002\u53f3\u8fb9\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\u5171\u4eab\u7a7a\u95f4\u3002</p>"},{"location":"zh/02_multi_panel/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# \u4e09\u5f20\u4e0d\u540c\u7684 tensor\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# \u5de6\u4fa7\uff1a3 \u4e2a\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# \u53f3\u4fa7\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\uff08\u81ea\u52a8\u5782\u76f4\u5747\u5206\uff09\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"zh/02_multi_panel/#_3","title":"\u8981\u70b9","text":"<ol> <li> <p>\u81ea\u52a8\u5e03\u5c40 \u2014 \u4e0d\u5199 <code>side=</code> \u7684\u9762\u677f\u81ea\u52a8\u5782\u76f4\u5806\u53e0\u3002    \u52a0 <code>side=\"right\"</code> + <code>width=0.5</code> \u5c31\u505c\u9760\u5230\u53f3\u8fb9\u5360\u4e00\u534a\u3002</p> </li> <li> <p>\u591a\u753b\u5e03 \u2014 \u5bf9\u540c\u4e00\u4e2a\u9762\u677f\u591a\u6b21\u8c03\u7528 <code>panel.canvas()</code>\u3002    \u591a\u4e2a <code>fit=True</code>\uff08\u9ed8\u8ba4\uff09\u7684\u753b\u5e03\u4f1a\u81ea\u52a8\u5747\u5206\u5782\u76f4\u7a7a\u95f4\uff0c\u4e0d\u7528\u624b\u7b97\u9ad8\u5ea6\u3002</p> </li> <li> <p>\u4f9d\u7136\u4e0d\u9700\u8981\u56de\u8c03 \u2014 \u9759\u6001\u6570\u636e\u53ea\u9700 <code>bind()</code> + <code>run()</code>\u3002    \u52a8\u6001\u66f4\u65b0\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8bb2\u3002</p> </li> <li> <p>\u968f\u4fbf\u62d6 \u2014 \u6240\u6709\u9762\u677f\u90fd\u652f\u6301\u505c\u9760\u3002\u7528\u6237\u53ef\u4ee5\u62d6\u6807\u9898\u680f\u91cd\u6392\u3001    \u62c9\u51fa\u6765\u53d8\u6d6e\u52a8\u7a97\u53e3\u3001\u6216\u8005\u62d6\u8fb9\u6846\u8c03\u5927\u5c0f\u3002</p> </li> </ol> <p>\u8bf4\u660e</p> <p>\u540c\u4e00\u4e2a tensor \u53ef\u4ee5\u540c\u65f6\u7ed1\u5b9a\u591a\u4e2a\u753b\u5e03 \u2014\u2014 <code>red</code> \u540c\u65f6\u51fa\u73b0\u5728\u5de6\u8fb9\u7684 Red \u9762\u677f\u548c\u53f3\u8fb9\u7684 Combined \u9762\u677f\u91cc\u3002</p>"},{"location":"zh/03_training_test/","title":"03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/03_training_test.py</code></p> <p>\u4f60\u6709\u6ca1\u6709\u7ecf\u5386\u8fc7\u8fd9\u79cd\u4e8b\uff1a\u8bad\u7ec3\u8dd1\u4e86\u534a\u5c0f\u65f6\uff0c\u7ec8\u7aef\u91cc loss \u6570\u5b57\u54d7\u54d7\u5f80\u4e0b\u6389\uff0c \u770b\u8d77\u6765\u633a\u6b63\u5e38 \u2014\u2014 \u7ed3\u679c\u4e00\u51fa\u56fe\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5168\u662f\u7070\u7684\uff1f</p> <p>\u76ef\u7740\u547d\u4ee4\u884c\u91cc\u7684\u6570\u5b57\u731c\u6a21\u578b\u72b6\u6001\uff0c\u8ddf\u770b\u80a1\u7968 K \u7ebf\u731c\u660e\u5929\u6da8\u8dcc\u4e00\u6837\u4e0d\u9760\u8c31\u3002 \u672c\u7ae0\u76f4\u63a5\u628a GT \u548c\u9884\u6d4b\u5e76\u6392\u653e\u5728\u5c4f\u5e55\u4e0a\u3002\u7f51\u7edc\u5230\u5e95\u6709\u6ca1\u6709\u5728\u5b66\uff0c\u4e00\u773c\u5c31\u77e5\u9053\u3002</p>"},{"location":"zh/03_training_test/#_1","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u5f88\u5c0f\u7684 MLP\uff082 \u2192 64 \u2192 64 \u2192 3\uff09\u53bb\u5b9e\u65f6\u62df\u5408\u4e00\u5f20 256\u00d7256 \u7684 PyTorch logo\u3002 \u7a97\u53e3\u5206\u4e09\u5757\uff1a</p> \u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 GT \u9762\u677f \u2014 \u76ee\u6807\u56fe\uff08\u4f60\u8981\u62df\u5408\u7684\u4e1c\u897f\uff09 \u53f3\u4fa7 Prediction \u9762\u677f \u2014 \u7f51\u7edc\u5b9e\u65f6\u8f93\u51fa\uff0c\u6bcf\u5e27\u5237\u65b0 \u4e0b\u65b9 Info \u9762\u677f \u2014 FPS\u3001loss\u3001\u8fed\u4ee3\u6b21\u6570\u3001\u8fdb\u5ea6\u6761\uff0c\u8fd8\u80fd\u62d6\u6ed1\u6761 <p>\u6240\u6709\u6570\u503c\u90fd\u5728\u753b\u9762\u91cc\uff0c\u4e0d\u7528\u518d\u5728\u7ec8\u7aef\u91cc\u7ffb\u6765\u7ffb\u53bb\u627e\u3002</p>"},{"location":"zh/03_training_test/#_2","title":"\u65b0\u670b\u53cb","text":"<p>\u524d\u4e24\u7ae0\u90fd\u662f\u9759\u6001\u6570\u636e \u2014\u2014 <code>bind()</code> + <code>run()</code>\uff0c\u5b8c\u4e8b\u3002 \u8fd9\u6b21\u6211\u4eec\u8981\u5f15\u5165\u4e24\u4e2a\u65b0\u4e1c\u897f\uff1a</p> \u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u5199\u6cd5 on_frame \u6bcf\u5e27\u56de\u8c03\uff0c\u8bad\u7ec3 + \u66f4\u65b0\u5728\u8fd9\u91cc\u641e <code>@view.on_frame</code> create_tensor \u521b\u5efa GPU \u5171\u4eab\u663e\u5b58 tensor <code>vultorch.create_tensor(H, W, ...)</code> <p>\u56de\u8c03\u51fd\u6570\u91cc\u968f\u4fbf\u5199 PyTorch \u4ee3\u7801\uff0cVultorch \u6bcf\u5e27\u5e2e\u4f60\u628a tensor \u642c\u4e0a\u5c4f\u5e55\u3002</p>"},{"location":"zh/03_training_test/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\nui = vultorch.ui\n\ntry:\n    from PIL import Image\nexcept ImportError as exc:\n    raise RuntimeError(\"Please install pillow: pip install pillow\") from exc\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u52a0\u8f7d\u76ee\u6807\u56fe\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\nimg = Image.open(img_path).convert(\"RGB\").resize((256, 256), Image.BILINEAR)\ngt = torch.from_numpy(np.asarray(img, dtype=np.float32) / 255.0).to(device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)  # (H*W, 2)\ntarget = gt.reshape(-1, 3)                               # (H*W, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ngt_panel = view.panel(\"GT\")\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# \u9884\u6d4b\u7528 4 \u901a\u9053 \u2014 GPU \u96f6\u62f7\u8d1d\u663e\u793a\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n    \"layout_done\": False,\n}\n\n\n@view.on_frame\ndef train_and_render():\n    # ---- \u9996\u5e27\u624b\u52a8\u5e03\u5c40\uff1a\u4e0a\u9762\u5de6\u53f3\u5206\uff0c\u4e0b\u9762\u653e Info ----\n    if not state[\"layout_done\"]:\n        dockspace_id = ui.dock_space_over_viewport(flags=8)\n        ui.dock_builder_remove_node(dockspace_id)\n        ui.dock_builder_add_node(dockspace_id, 1 &lt;&lt; 10)\n        ui.dock_builder_set_node_size(dockspace_id, 1280.0, 760.0)\n\n        info_node, top_node = ui.dock_builder_split_node(dockspace_id, 3, 0.28)\n        left_node, right_node = ui.dock_builder_split_node(top_node, 0, 0.5)\n\n        ui.dock_builder_dock_window(\"GT\", left_node)\n        ui.dock_builder_dock_window(\"Prediction\", right_node)\n        ui.dock_builder_dock_window(\"Info\", info_node)\n        ui.dock_builder_finish(dockspace_id)\n        state[\"layout_done\"] = True\n\n    # ---- \u8bad\u7ec3\u51e0\u6b65 ----\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    # ---- \u628a\u9884\u6d4b\u5199\u8fdb\u663e\u793a tensor ----\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n    # ---- Info \u9762\u677f ----\n    ui.begin(\"Info\", True, 0)\n    ui.text(f\"FPS: {view.fps:.1f}\")\n    ui.text(f\"Iteration: {state['iter']}\")\n    ui.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    ui.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = ui.slider_int(\n        \"Steps / Frame\", state[\"steps_per_frame\"], 1, 32\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    ui.progress_bar(progress, overlay=f\"Training progress {progress * 100:.1f}%\")\n    ui.text_wrapped(\n        \"\u5de6\u8fb9\u662f GT\uff0c\u53f3\u8fb9\u662f\u9884\u6d4b\u3002\u60f3\u66f4\u5feb\u6536\u655b\u5c31\u63d0\u9ad8 Steps / Frame\u3002\"\n    )\n    ui.end()\n\n\nview.run()\n</code></pre> <p>\u641e\u5b9a\u3002\u8dd1\u8d77\u6765\u4e4b\u540e\u4f60\u4f1a\u770b\u5230\u53f3\u8fb9\u90a3\u5768\u7070\u8272\u5728\u51e0\u79d2\u5185\u53d8\u6210 PyTorch logo\u3002</p>"},{"location":"zh/03_training_test/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 PIL \u8bfb\u56fe\uff0c\u8f6c\u6210 float32 CUDA tensor \u5f53 GT\u3002    \u5750\u6807\u7528 <code>meshgrid</code> \u5c55\u6210 <code>(H*W, 2)</code>\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684 <code>(x, y)</code> \u5f52\u4e00\u5316\u5230 <code>[-1, 1]</code>\u3002</p> </li> <li> <p>\u6a21\u578b \u2014 \u4e24\u5c42 64 \u5bbd\u7684 MLP\uff0c\u8f93\u5165 <code>(x, y)</code>\uff0c\u8f93\u51fa <code>(r, g, b)</code>\u3002    \u8fd9\u4e2a\u7f51\u7edc\u5c0f\u5230\u53ef\u4ee5\u8dd1\u5728\u56de\u8c03\u91cc\u4e0d\u6389\u5e27\u3002</p> </li> <li> <p>on_frame \u56de\u8c03 \u2014 \u6bcf\u5e27\u8c03\u7528\u4e00\u6b21\u3002\u91cc\u9762\u505a\u4e86\u4e09\u4ef6\u4e8b\uff1a    \u9996\u5e27\u641e\u5b9a\u5e03\u5c40\uff0c\u7136\u540e\u8dd1 N \u6b65\u8bad\u7ec3\uff0c\u6700\u540e\u628a\u9884\u6d4b\u5199\u56de <code>pred_rgba</code>\u3002</p> </li> <li> <p>Info \u9762\u677f \u2014 \u7528 ImGui \u7684 <code>ui.begin()</code> / <code>ui.end()</code> \u624b\u52a8\u753b\u4e86\u7b2c\u4e09\u4e2a\u9762\u677f\u3002    \u53ef\u4ee5\u653e\u6587\u5b57\u3001\u6ed1\u6761\u3001\u8fdb\u5ea6\u6761 \u2014\u2014 \u4ec0\u4e48 widget \u90fd\u884c\u3002</p> </li> </ol>"},{"location":"zh/03_training_test/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 \u56de\u8c03\u91cc\u53ef\u4ee5\u8dd1\u4efb\u610f PyTorch \u4ee3\u7801\u3002    \u6bcf\u5e27\u7ed3\u675f\u65f6 Vultorch \u81ea\u52a8\u628a\u7ed1\u5b9a\u7684 tensor \u642c\u5230\u5c4f\u5e55\uff0c\u4e0d\u7528\u4f60\u7ba1\u3002</p> </li> <li> <p><code>create_tensor</code> \u2014 \u8ddf\u666e\u901a <code>torch.zeros</code> \u4e00\u6837\u7528\uff0c    \u4f46\u5e95\u5c42\u662f Vulkan/CUDA \u5171\u4eab\u663e\u5b58\uff0c\u663e\u793a\u7684\u65f6\u5019\u96f6\u62f7\u8d1d\u3002</p> </li> <li> <p>\u624b\u52a8\u5e03\u5c40 \u2014 <code>dock_builder_split_node</code> \u53ef\u4ee5\u628a\u7a97\u53e3\u5207\u6210\u4efb\u610f\u4f60\u60f3\u8981\u7684\u6837\u5b50\u3002    \u65b9\u5411\uff1a<code>0=\u5de6, 1=\u53f3, 2=\u4e0a, 3=\u4e0b</code>\uff0c\u6bd4\u4f8b\u7528 float\u3002</p> </li> <li> <p>\u4e0d\u5237\u7ec8\u7aef \u2014 \u6240\u6709\u72b6\u6001\u4fe1\u606f\u90fd\u5728 Info \u9762\u677f\u91cc\uff0c    \u4f60\u7684\u7ec8\u7aef\u53ef\u4ee5\u7559\u7740\u770b warning \u548c traceback\uff0c\u5e72\u51c0\u591a\u4e86\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p><code>Steps / Frame</code> \u6ed1\u6761\u62c9\u5230 32 \u6536\u655b\u98de\u5feb\u3002 \u4f46\u4e5f\u522b\u592a\u8d2a \u2014\u2014 \u62c9\u592a\u9ad8\u5e27\u7387\u4f1a\u6389\u4e0b\u6765\uff0c\u56e0\u4e3a\u6bcf\u5e27\u7684\u8bad\u7ec3\u65f6\u95f4\u53d8\u957f\u4e86\u3002</p> <p>\u8bf4\u660e</p> <p><code>create_tensor</code> \u53ea\u5728\u521d\u59cb\u5316\u65f6\u8c03\u7528\u4e00\u6b21\uff0c\u4e0d\u5728\u5e27\u5faa\u73af\u91cc\u3002 \u4e4b\u540e\u6bcf\u5e27\u53ea\u9700\u8981\u5f80\u8fd9\u4e2a tensor \u91cc\u5199\u6570\u636e\uff0c\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002</p>"}]}