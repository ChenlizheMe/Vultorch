{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vultorch Tutorial","text":"<p>A step-by-step guide through Vultorch, one example at a time. Each chapter maps to a runnable script in the <code>examples/</code> folder.</p> Chapter Topic Key concepts 01 \u2014 Hello Tensor Minimal display View, Panel, Canvas, bind, run 02 \u2014 Multi-Panel Multiple panels &amp; canvases Layout, side, multi-canvas 03 \u2014 Training Test Fit a tiny network to a GT image custom dock layout, create_tensor, per-pixel optimization <p>More chapters coming soon.</p>"},{"location":"01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>Example file: <code>examples/01_hello_tensor.py</code></p> <p>Tired of every research repo inventing its own janky tensor viewer with matplotlib hacks and <code>cv2.imshow</code> spaghetti?  Yeah, us too.</p> <p>Vultorch gets your CUDA tensor on screen in four lines \u2014 no saving PNGs, no CPU round-trips, no <code>plt.pause(0.001)</code> nonsense.</p>"},{"location":"01_hello_tensor/#the-mental-model","title":"The mental model","text":"<p>There are exactly four objects you need to know:</p> Object What it is One-liner View The OS window <code>vultorch.View(\"title\", w, h)</code> Panel A dockable sub-window inside the View <code>view.panel(\"name\")</code> Canvas A GPU image slot inside a Panel <code>panel.canvas(\"name\")</code> bind() Connects a tensor to a Canvas <code>canvas.bind(t)</code> <p>Chain them together, call <code>run()</code>, done.</p>"},{"location":"01_hello_tensor/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\n# A 256\u00d7256 RGB gradient \u2014 any (H,W,C) float32 CUDA tensor works\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # blocks until you close the window\n</code></pre> <p>That's it. No event loop boilerplate, no <code>begin_frame()</code> / <code>end_frame()</code>.</p>"},{"location":"01_hello_tensor/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 we made an RGB gradient on CUDA. Vultorch accepts <code>(H,W)</code>,    <code>(H,W,1)</code>, <code>(H,W,3)</code>, or <code>(H,W,4)</code>, in float32 / float16 / uint8.    It handles RGBA expansion for you.</p> </li> <li> <p>Object tree \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>.    The canvas auto-fills its panel by default (<code>fit=True</code>).</p> </li> <li> <p>Run \u2014 <code>view.run()</code> enters a blocking event loop. Every frame the    canvas re-uploads the bound tensor and renders it. Close the window    to exit.</p> </li> </ol> <p>Tip</p> <p>The four setup lines collapse into a one-liner if you're feeling fancy: <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"02_multi_panel/","title":"02 \u2014 Multi-Panel","text":"<p>Example file: <code>examples/02_imgui_controls.py</code></p> <p>One panel is nice. But in practice you want to see your loss map, gradient field, and output side-by-side \u2014 without writing a single line of layout code.</p> <p>Good news: Vultorch panels are dockable. Just create them and they'll arrange themselves. Users can drag, resize, and rearrange at will.</p> <p>This chapter shows two patterns:</p> <ul> <li>One canvas per panel \u2014 three panels stacked, each with its own tensor.</li> <li>Multiple canvases in one panel \u2014 one panel, three canvases, auto-split.</li> </ul>"},{"location":"02_multi_panel/#layout","title":"Layout","text":"<p>Here's what the window looks like:</p> Left side (main area) Right side (<code>side=\"right\"</code>) Red panel \u2014 <code>red_img</code> Combined panel Green panel \u2014 <code>green_img</code> <code>c_red</code> canvas Blue panel \u2014 <code>blue_img</code> <code>c_green</code> canvas <code>c_blue</code> canvas <p>Left: 3 separate panels, each one canvas. Right: 1 panel, 3 canvases sharing space.</p>"},{"location":"02_multi_panel/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# Three different tensors\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# Left: 3 panels, each with one canvas\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# Right: 1 panel with 3 canvases (auto-split vertically)\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"02_multi_panel/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Auto-layout \u2014 panels without <code>side=</code> stack vertically.    Add <code>side=\"right\"</code> (or <code>\"left\"</code>) + <code>width=0.5</code> to dock to one side.</p> </li> <li> <p>Multi-canvas \u2014 call <code>panel.canvas()</code> multiple times.    When several canvases have <code>fit=True</code> (the default), they split the    vertical space equally. No manual height math.</p> </li> <li> <p>Still no callback \u2014 static data only needs <code>bind()</code> + <code>run()</code>.    Dynamic updates come in a later chapter.</p> </li> <li> <p>Drag &amp; drop \u2014 all panels are dockable. Users can rearrange,    float, or resize them at runtime.</p> </li> </ol> <p>Note</p> <p>The same tensor can be bound to multiple canvases at once \u2014 <code>red</code> appears in both the left panel and the right combined panel.</p>"},{"location":"03_training_test/","title":"03 \u2014 Training Test","text":"<p>Example file: <code>examples/03_training_test.py</code></p> <p>No more guessing if your tiny net is actually learning.</p> <p>This chapter trains a lightweight MLP to fit a target image in real time:</p> <ul> <li>Left panel: GT image (<code>docs/images/pytorch_logo.png</code>)</li> <li>Right panel: model prediction</li> <li>Bottom panel: live text info (FPS, loss, iteration, steps/frame)</li> </ul>"},{"location":"03_training_test/#layout","title":"Layout","text":"Area Content Left GT panel (ground truth image) Right Prediction panel (network output, updated every frame) Bottom Info panel (FPS, loss, iteration, progress)"},{"location":"03_training_test/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\nui = vultorch.ui\n\ntry:\n    from PIL import Image\nexcept ImportError as exc:\n    raise RuntimeError(\"Please install pillow: pip install pillow\") from exc\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\nimg = Image.open(img_path).convert(\"RGB\").resize((256, 256), Image.BILINEAR)\ngt = torch.from_numpy(np.asarray(img, dtype=np.float32) / 255.0).to(device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ngt_panel = view.panel(\"GT\")\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n    \"layout_done\": False,\n}\n\n\n@view.on_frame\ndef train_and_render():\n    if not state[\"layout_done\"]:\n        dockspace_id = ui.dock_space_over_viewport(flags=8)\n        ui.dock_builder_remove_node(dockspace_id)\n        ui.dock_builder_add_node(dockspace_id, 1 &lt;&lt; 10)\n        ui.dock_builder_set_node_size(dockspace_id, 1280.0, 760.0)\n\n        info_node, top_node = ui.dock_builder_split_node(dockspace_id, 3, 0.28)\n        left_node, right_node = ui.dock_builder_split_node(top_node, 0, 0.5)\n\n        ui.dock_builder_dock_window(\"GT\", left_node)\n        ui.dock_builder_dock_window(\"Prediction\", right_node)\n        ui.dock_builder_dock_window(\"Info\", info_node)\n        ui.dock_builder_finish(dockspace_id)\n        state[\"layout_done\"] = True\n\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n    ui.begin(\"Info\", True, 0)\n    ui.text(f\"FPS: {view.fps:.1f}\")\n    ui.text(f\"Iteration: {state['iter']}\")\n    ui.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    ui.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = ui.slider_int(\n        \"Steps / Frame\", state[\"steps_per_frame\"], 1, 32\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    ui.progress_bar(progress, overlay=f\"Training progress {progress * 100:.1f}%\")\n    ui.text_wrapped(\n        \"Left is GT, right is prediction. Increase 'Steps / Frame' for faster fitting.\"\n    )\n    ui.end()\n\n\nview.run()\n</code></pre>"},{"location":"03_training_test/#notes","title":"Notes","text":"<ul> <li>No per-iteration console spam; all key metrics are shown in the bottom panel.</li> <li><code>create_tensor(..., channels=4)</code> keeps prediction display on the GPU path.</li> <li>Increase <code>Steps / Frame</code> if you want convergence to happen faster.</li> </ul>"},{"location":"zh/","title":"Vultorch \u6559\u7a0b","text":"<p>\u9010\u6b65\u5b66\u4e60 Vultorch\uff0c\u6bcf\u4e2a\u7ae0\u8282\u5bf9\u5e94 <code>examples/</code> \u76ee\u5f55\u4e2d\u7684\u4e00\u4e2a\u53ef\u8fd0\u884c\u811a\u672c\u3002</p> \u7ae0\u8282 \u4e3b\u9898 \u6838\u5fc3\u6982\u5ff5 01 \u2014 Hello Tensor \u6700\u5c0f\u793a\u4f8b View, Panel, Canvas, bind, run 02 \u2014 \u591a\u9762\u677f \u591a\u9762\u677f\u4e0e\u591a\u753b\u5e03 \u5e03\u5c40, side, \u591a\u753b\u5e03 03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5 \u62df\u5408 GT \u56fe\u50cf \u81ea\u5b9a\u4e49\u505c\u9760\u5e03\u5c40, create_tensor, \u9010\u50cf\u7d20\u4f18\u5316 <p>\u66f4\u591a\u7ae0\u8282\u5373\u5c06\u63a8\u51fa\u3002</p>"},{"location":"zh/01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/01_hello_tensor.py</code></p> <p>\u4f60\u662f\u5426\u53d7\u591f\u4e86\u6bcf\u4e2a\u4ed3\u5e93\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u5b9e\u73b0\u7684\u53ef\u89c6\u5316\u65b9\u6848 \u2014\u2014 \u8fd9\u4e2a\u7528 matplotlib \u624b\u52a8\u5237\u65b0\uff0c \u90a3\u4e2a\u7528 <code>cv2.imshow</code> \u7136\u540e <code>waitKey(1)</code>\uff0c\u8fd8\u6709\u7684\u76f4\u63a5\u5b58 PNG \u8ba9\u4f60\u5f00\u4e2a\u56fe\u7247\u67e5\u770b\u5668\uff1f</p> <p>Vultorch \u53ea\u9700 \u56db\u884c\u4ee3\u7801 \u5c31\u628a CUDA tensor \u642c\u5230\u5c4f\u5e55\u4e0a\u3002\u4e0d\u5b58\u56fe\u3001\u4e0d\u8fc7 CPU\u3001 \u4e0d\u9700\u8981 <code>plt.pause(0.001)</code> \u8fd9\u79cd\u9ed1\u9b54\u6cd5\u3002</p>"},{"location":"zh/01_hello_tensor/#_1","title":"\u4f60\u9700\u8981\u8bb0\u4f4f\u7684\u4e1c\u897f","text":"<p>\u4e00\u5171\u5c31\u56db\u4e2a\u5bf9\u8c61\uff1a</p> \u5bf9\u8c61 \u662f\u4ec0\u4e48 \u5199\u6cd5 View \u64cd\u4f5c\u7cfb\u7edf\u7a97\u53e3 <code>vultorch.View(\"title\", w, h)</code> Panel View \u91cc\u53ef\u505c\u9760\u7684\u5b50\u7a97\u53e3 <code>view.panel(\"name\")</code> Canvas Panel \u91cc\u7684 GPU \u56fe\u50cf\u69fd\u4f4d <code>panel.canvas(\"name\")</code> bind() \u628a tensor \u8fde\u5230 Canvas \u4e0a <code>canvas.bind(t)</code> <p>\u94fe\u8d77\u6765\uff0c\u8c03 <code>run()</code>\uff0c\u6536\u5de5\u3002</p>"},{"location":"zh/01_hello_tensor/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\n# \u4e00\u5f20 256\u00d7256 \u7684 RGB \u6e10\u53d8 \u2014 \u4efb\u4f55 (H,W,C) float32 CUDA tensor \u90fd\u884c\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # \u963b\u585e\uff0c\u76f4\u5230\u4f60\u5173\u95ed\u7a97\u53e3\n</code></pre> <p>\u6ca1\u4e86\u3002\u4e0d\u9700\u8981\u624b\u5199\u4e8b\u4ef6\u5faa\u73af\uff0c\u4e0d\u9700\u8981 <code>begin_frame()</code> / <code>end_frame()</code>\u3002</p>"},{"location":"zh/01_hello_tensor/#_3","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 \u6211\u4eec\u5728 CUDA \u4e0a\u9020\u4e86\u4e00\u5f20 RGB \u6e10\u53d8\u3002Vultorch \u652f\u6301    <code>(H,W)</code> / <code>(H,W,1)</code> / <code>(H,W,3)</code> / <code>(H,W,4)</code>\uff0c    float32 / float16 / uint8 \u90fd\u884c\uff0cRGBA \u6269\u5c55\u5b83\u81ea\u5df1\u641e\u5b9a\u3002</p> </li> <li> <p>\u5bf9\u8c61\u6811 \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>\u3002    \u753b\u5e03\u9ed8\u8ba4\u94fa\u6ee1\u6574\u4e2a\u9762\u677f\uff08<code>fit=True</code>\uff09\u3002</p> </li> <li> <p>\u8fd0\u884c \u2014 <code>view.run()</code> \u8fdb\u5165\u963b\u585e\u4e8b\u4ef6\u5faa\u73af\uff0c\u6bcf\u5e27\u91cd\u65b0\u4e0a\u4f20 tensor \u5e76\u6e32\u67d3\u3002    \u5173\u7a97\u53e3\u5c31\u9000\u51fa\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u56db\u884c\u521d\u59cb\u5316\u53ef\u4ee5\u538b\u6210\u4e00\u884c\uff0c\u5982\u679c\u4f60\u559c\u6b22\u70ab\u6280\u7684\u8bdd\uff1a <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"zh/02_multi_panel/","title":"02 \u2014 \u591a\u9762\u677f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/02_imgui_controls.py</code></p> <p>\u4e00\u4e2a\u9762\u677f\u633a\u597d\u7684\u3002\u4f46\u5b9e\u9645\u505a\u7814\u7a76\u7684\u65f6\u5019\uff0c\u4f60\u60f3\u540c\u65f6\u770b loss map\u3001\u68af\u5ea6\u573a\u3001 \u6a21\u578b\u8f93\u51fa \u2014\u2014 \u800c\u4e14\u6700\u597d\u4e0d\u7528\u81ea\u5df1\u5199\u4efb\u4f55\u5e03\u5c40\u4ee3\u7801\u3002</p> <p>\u597d\u6d88\u606f\uff1aVultorch \u7684\u9762\u677f\u662f\u53ef\u505c\u9760\u7684\u3002\u4f60\u53ea\u7ba1\u521b\u5efa\uff0c\u5b83\u4eec\u81ea\u5df1\u6392\u597d\u3002 \u7528\u6237\u8fd0\u884c\u65f6\u8fd8\u80fd\u968f\u4fbf\u62d6\u3001\u968f\u4fbf\u62c9\u3002</p> <p>\u672c\u7ae0\u6f14\u793a\u4e24\u79cd\u6a21\u5f0f\uff1a</p> <ul> <li>\u6bcf\u4e2a\u9762\u677f\u4e00\u4e2a\u753b\u5e03 \u2014 \u4e09\u4e2a\u9762\u677f\u5782\u76f4\u5806\u53e0\uff0c\u5404\u81ea\u4e00\u5f20\u56fe\u3002</li> <li>\u4e00\u4e2a\u9762\u677f\u591a\u4e2a\u753b\u5e03 \u2014 \u4e00\u4e2a\u9762\u677f\u91cc\u585e\u4e09\u4e2a\u753b\u5e03\uff0c\u81ea\u52a8\u5747\u5206\u3002</li> </ul>"},{"location":"zh/02_multi_panel/#_1","title":"\u5e03\u5c40","text":"<p>\u7a97\u53e3\u957f\u8fd9\u6837\uff1a</p> \u5de6\u4fa7\uff08\u4e3b\u533a\u57df\uff09 \u53f3\u4fa7\uff08<code>side=\"right\"</code>\uff09 Red \u9762\u677f \u2014 <code>red_img</code> Combined \u9762\u677f Green \u9762\u677f \u2014 <code>green_img</code> <code>c_red</code> \u753b\u5e03 Blue \u9762\u677f \u2014 <code>blue_img</code> <code>c_green</code> \u753b\u5e03 <code>c_blue</code> \u753b\u5e03 <p>\u5de6\u8fb9\uff1a3 \u4e2a\u72ec\u7acb\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\u3002\u53f3\u8fb9\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\u5171\u4eab\u7a7a\u95f4\u3002</p>"},{"location":"zh/02_multi_panel/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# \u4e09\u5f20\u4e0d\u540c\u7684 tensor\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# \u5de6\u4fa7\uff1a3 \u4e2a\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# \u53f3\u4fa7\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\uff08\u81ea\u52a8\u5782\u76f4\u5747\u5206\uff09\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"zh/02_multi_panel/#_3","title":"\u8981\u70b9","text":"<ol> <li> <p>\u81ea\u52a8\u5e03\u5c40 \u2014 \u4e0d\u5199 <code>side=</code> \u7684\u9762\u677f\u81ea\u52a8\u5782\u76f4\u5806\u53e0\u3002    \u52a0 <code>side=\"right\"</code> + <code>width=0.5</code> \u5c31\u505c\u9760\u5230\u53f3\u8fb9\u5360\u4e00\u534a\u3002</p> </li> <li> <p>\u591a\u753b\u5e03 \u2014 \u5bf9\u540c\u4e00\u4e2a\u9762\u677f\u591a\u6b21\u8c03\u7528 <code>panel.canvas()</code>\u3002    \u591a\u4e2a <code>fit=True</code>\uff08\u9ed8\u8ba4\uff09\u7684\u753b\u5e03\u4f1a\u81ea\u52a8\u5747\u5206\u5782\u76f4\u7a7a\u95f4\uff0c\u4e0d\u7528\u624b\u7b97\u9ad8\u5ea6\u3002</p> </li> <li> <p>\u4f9d\u7136\u4e0d\u9700\u8981\u56de\u8c03 \u2014 \u9759\u6001\u6570\u636e\u53ea\u9700 <code>bind()</code> + <code>run()</code>\u3002    \u52a8\u6001\u66f4\u65b0\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8bb2\u3002</p> </li> <li> <p>\u968f\u4fbf\u62d6 \u2014 \u6240\u6709\u9762\u677f\u90fd\u652f\u6301\u505c\u9760\u3002\u7528\u6237\u53ef\u4ee5\u62d6\u6807\u9898\u680f\u91cd\u6392\u3001    \u62c9\u51fa\u6765\u53d8\u6d6e\u52a8\u7a97\u53e3\u3001\u6216\u8005\u62d6\u8fb9\u6846\u8c03\u5927\u5c0f\u3002</p> </li> </ol> <p>\u8bf4\u660e</p> <p>\u540c\u4e00\u4e2a tensor \u53ef\u4ee5\u540c\u65f6\u7ed1\u5b9a\u591a\u4e2a\u753b\u5e03 \u2014\u2014 <code>red</code> \u540c\u65f6\u51fa\u73b0\u5728\u5de6\u8fb9\u7684 Red \u9762\u677f\u548c\u53f3\u8fb9\u7684 Combined \u9762\u677f\u91cc\u3002</p>"},{"location":"zh/03_training_test/","title":"03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/03_training_test.py</code></p> <p>\u4e0d\u7528\u518d\u731c\u201c\u5230\u5e95\u5b66\u6ca1\u5b66\u5230\u201d\uff0c\u76f4\u63a5\u770b\u3002</p> <p>\u672c\u7ae0\u7528\u4e00\u4e2a\u5f88\u8f7b\u91cf\u7684 MLP \u53bb\u5b9e\u65f6\u62df\u5408\u76ee\u6807\u56fe\uff1a</p> <ul> <li>\u5de6\u4fa7\u9762\u677f\uff1aGT\uff08<code>docs/images/pytorch_logo.png</code>\uff09</li> <li>\u53f3\u4fa7\u9762\u677f\uff1a\u7f51\u7edc\u9884\u6d4b</li> <li>\u4e0b\u65b9\u9762\u677f\uff1a\u5b9e\u65f6\u6587\u5b57\u4fe1\u606f\uff08FPS\u3001loss\u3001iter\u3001steps/frame\uff09</li> </ul>"},{"location":"zh/03_training_test/#_1","title":"\u5e03\u5c40","text":"\u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 GT \u9762\u677f\uff08\u771f\u503c\u56fe\uff09 \u53f3\u4fa7 Prediction \u9762\u677f\uff08\u7f51\u7edc\u8f93\u51fa\uff0c\u6bcf\u5e27\u66f4\u65b0\uff09 \u4e0b\u65b9 Info \u9762\u677f\uff08FPS\u3001loss\u3001\u8fed\u4ee3\u3001\u8fdb\u5ea6\uff09"},{"location":"zh/03_training_test/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\nui = vultorch.ui\n\ntry:\n    from PIL import Image\nexcept ImportError as exc:\n    raise RuntimeError(\"Please install pillow: pip install pillow\") from exc\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\nimg = Image.open(img_path).convert(\"RGB\").resize((256, 256), Image.BILINEAR)\ngt = torch.from_numpy(np.asarray(img, dtype=np.float32) / 255.0).to(device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ngt_panel = view.panel(\"GT\")\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n    \"layout_done\": False,\n}\n\n\n@view.on_frame\ndef train_and_render():\n    if not state[\"layout_done\"]:\n        dockspace_id = ui.dock_space_over_viewport(flags=8)\n        ui.dock_builder_remove_node(dockspace_id)\n        ui.dock_builder_add_node(dockspace_id, 1 &lt;&lt; 10)\n        ui.dock_builder_set_node_size(dockspace_id, 1280.0, 760.0)\n\n        info_node, top_node = ui.dock_builder_split_node(dockspace_id, 3, 0.28)\n        left_node, right_node = ui.dock_builder_split_node(top_node, 0, 0.5)\n\n        ui.dock_builder_dock_window(\"GT\", left_node)\n        ui.dock_builder_dock_window(\"Prediction\", right_node)\n        ui.dock_builder_dock_window(\"Info\", info_node)\n        ui.dock_builder_finish(dockspace_id)\n        state[\"layout_done\"] = True\n\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n    ui.begin(\"Info\", True, 0)\n    ui.text(f\"FPS: {view.fps:.1f}\")\n    ui.text(f\"Iteration: {state['iter']}\")\n    ui.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    ui.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = ui.slider_int(\n        \"Steps / Frame\", state[\"steps_per_frame\"], 1, 32\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    ui.progress_bar(progress, overlay=f\"Training progress {progress * 100:.1f}%\")\n    ui.text_wrapped(\n        \"\u5de6\u8fb9\u662f GT\uff0c\u53f3\u8fb9\u662f\u9884\u6d4b\u3002\u60f3\u66f4\u5feb\u6536\u655b\u5c31\u63d0\u9ad8 Steps / Frame\u3002\"\n    )\n    ui.end()\n\n\nview.run()\n</code></pre>"},{"location":"zh/03_training_test/#_3","title":"\u8bf4\u660e","text":"<ul> <li>\u4e0d\u518d\u5728\u63a7\u5236\u53f0\u5237\u8fed\u4ee3\u65e5\u5fd7\uff1b\u5173\u952e\u4fe1\u606f\u5168\u90e8\u653e\u5728\u4e0b\u65b9 <code>Info</code> \u9762\u677f\u3002</li> <li>\u9884\u6d4b\u56fe\u901a\u8fc7 <code>create_tensor(..., channels=4)</code> \u8d70 GPU \u663e\u793a\u8def\u5f84\u3002</li> <li>\u60f3\u66f4\u5feb\u770b\u5230\u62df\u5408\u6548\u679c\uff0c\u8c03\u5927 <code>Steps / Frame</code>\u3002</li> </ul>"}]}