{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vultorch Tutorial","text":"<p>A step-by-step guide through Vultorch, one example at a time. Each chapter maps to a runnable script in the <code>examples/</code> folder.</p> Chapter Topic Key concepts 01 \u2014 Hello Tensor Minimal display View, Panel, Canvas, bind, run 02 \u2014 Multi-Panel Multiple panels &amp; canvases Layout, side, multi-canvas 03 \u2014 Training Test Fit a tiny network to a GT image custom dock layout, create_tensor, per-pixel optimization 04 \u2014 Conway's Game of Life GPU cellular automaton create_tensor for simulation, filter=\"nearest\", sidebar, buttons, color pickers 05 \u2014 Image Viewer Load, transform &amp; save images imread, imwrite, Canvas.save, combo, input_text, filter toggle <p>More chapters coming soon.</p>"},{"location":"01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>Example file: <code>examples/01_hello_tensor.py</code></p> <p>Tired of every research repo inventing its own janky tensor viewer with matplotlib hacks and <code>cv2.imshow</code> spaghetti?  Yeah, us too.</p> <p>Vultorch gets your CUDA tensor on screen in four lines \u2014 no saving PNGs, no CPU round-trips, no <code>plt.pause(0.001)</code> nonsense.</p>"},{"location":"01_hello_tensor/#the-mental-model","title":"The mental model","text":"<p>There are exactly four objects you need to know:</p> Object What it is One-liner View The OS window <code>vultorch.View(\"title\", w, h)</code> Panel A dockable sub-window inside the View <code>view.panel(\"name\")</code> Canvas A GPU image slot inside a Panel <code>panel.canvas(\"name\")</code> bind() Connects a tensor to a Canvas <code>canvas.bind(t)</code> <p>Chain them together, call <code>run()</code>, done.</p>"},{"location":"01_hello_tensor/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\n# A 256\u00d7256 RGB gradient \u2014 any (H,W,C) float32 CUDA tensor works\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # blocks until you close the window\n</code></pre> <p>That's it. No event loop boilerplate, no <code>begin_frame()</code> / <code>end_frame()</code>.</p>"},{"location":"01_hello_tensor/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 we made an RGB gradient on CUDA. Vultorch accepts <code>(H,W)</code>,    <code>(H,W,1)</code>, <code>(H,W,3)</code>, or <code>(H,W,4)</code>, in float32 / float16 / uint8.    It handles RGBA expansion for you.</p> </li> <li> <p>Object tree \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>.    The canvas auto-fills its panel by default (<code>fit=True</code>).</p> </li> <li> <p>Run \u2014 <code>view.run()</code> enters a blocking event loop. Every frame the    canvas re-uploads the bound tensor and renders it. Close the window    to exit.</p> </li> </ol> <p>Tip</p> <p>The four setup lines collapse into a one-liner if you're feeling fancy: <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"02_multi_panel/","title":"02 \u2014 Multi-Panel","text":"<p>Example file: <code>examples/02_imgui_controls.py</code></p> <p>One panel is nice. But in practice you want to see your loss map, gradient field, and output side-by-side \u2014 without writing a single line of layout code.</p> <p>Good news: Vultorch panels are dockable. Just create them and they'll arrange themselves. Users can drag, resize, and rearrange at will.</p> <p>This chapter shows two patterns:</p> <ul> <li>One canvas per panel \u2014 three panels stacked, each with its own tensor.</li> <li>Multiple canvases in one panel \u2014 one panel, three canvases, auto-split.</li> </ul>"},{"location":"02_multi_panel/#layout","title":"Layout","text":"<p>Here's what the window looks like:</p> Left side (main area) Right side (<code>side=\"right\"</code>) Red panel \u2014 <code>red_img</code> Combined panel Green panel \u2014 <code>green_img</code> <code>c_red</code> canvas Blue panel \u2014 <code>blue_img</code> <code>c_green</code> canvas <code>c_blue</code> canvas <p>Left: 3 separate panels, each one canvas. Right: 1 panel, 3 canvases sharing space.</p>"},{"location":"02_multi_panel/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# Three different tensors\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# Left: 3 panels, each with one canvas\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# Right: 1 panel with 3 canvases (auto-split vertically)\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"02_multi_panel/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Auto-layout \u2014 panels without <code>side=</code> stack vertically.    Add <code>side=\"right\"</code> (or <code>\"left\"</code>) + <code>width=0.5</code> to dock to one side.</p> </li> <li> <p>Multi-canvas \u2014 call <code>panel.canvas()</code> multiple times.    When several canvases have <code>fit=True</code> (the default), they split the    vertical space equally. No manual height math.</p> </li> <li> <p>Still no callback \u2014 static data only needs <code>bind()</code> + <code>run()</code>.    Dynamic updates come in a later chapter.</p> </li> <li> <p>Drag &amp; drop \u2014 all panels are dockable. Users can rearrange,    float, or resize them at runtime.</p> </li> </ol> <p>Note</p> <p>The same tensor can be bound to multiple canvases at once \u2014 <code>red</code> appears in both the left panel and the right combined panel.</p>"},{"location":"03_training_test/","title":"03 \u2014 Training Test","text":"<p>Example file: <code>examples/03_training_test.py</code></p> <p>Ever stared at a wall of decreasing loss numbers in your terminal for ten minutes, feeling confident, only to discover the model's output is a solid grey rectangle? Yeah, us too.</p> <p>Reading loss values off a scrolling console is about as reliable as reading tea leaves. This chapter puts GT and prediction side by side on screen so you can see whether the network is actually learning.</p>"},{"location":"03_training_test/#what-were-building","title":"What we're building","text":"<p>A tiny MLP (2 \u2192 64 \u2192 64 \u2192 3) fitting a 256\u00d7256 PyTorch logo in real time. The window has three panels:</p> Area Content Left GT panel \u2014 the target image (what you're fitting) Right Prediction panel \u2014 live network output, updated every frame Bottom Info panel \u2014 FPS, loss, iteration, progress bar, and a slider <p>Everything on screen, nothing buried in the terminal.</p>"},{"location":"03_training_test/#new-friends","title":"New friends","text":"<p>Chapters 01 and 02 were all static \u2014 <code>bind()</code> + <code>run()</code>, done. This time we bring three new toys:</p> New thing What it does How to use on_frame Per-frame callback \u2014 train and update here <code>@view.on_frame</code> Panel.on_frame Per-panel callback \u2014 widgets go here <code>@info_panel.on_frame</code> create_tensor GPU shared-memory tensor <code>vultorch.create_tensor(H, W, ...)</code> vultorch.imread Load image with zero dependencies <code>vultorch.imread(path, channels=3)</code> side=\"bottom\" Dock a panel to the bottom edge <code>view.panel(\"Info\", side=\"bottom\")</code> <p>Write any PyTorch code inside the view callback; put widgets inside the panel callback.  Vultorch handles the tensor-to-screen dance every frame.</p>"},{"location":"03_training_test/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# -- View + panels (high-level declarative API) -------------------------\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ninfo_panel = view.panel(\"Info\", side=\"bottom\", width=0.28)\ngt_panel = view.panel(\"GT\", side=\"left\", width=0.5)\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# 4 channels \u2014 zero-copy GPU display path\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n}\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n\n@info_panel.on_frame\ndef draw_info():\n    info_panel.text(f\"FPS: {view.fps:.1f}\")\n    info_panel.text(f\"Iteration: {state['iter']}\")\n    info_panel.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    info_panel.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = info_panel.slider_int(\n        \"Steps / Frame\", 1, 32, default=6\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    info_panel.progress(progress,\n                        overlay=f\"Training progress {progress * 100:.1f}%\")\n    info_panel.text_wrapped(\n        \"Left is GT, right is prediction. \"\n        \"Increase 'Steps / Frame' for faster fitting.\"\n    )\n\n\nview.run()\n</code></pre> <p>That's it. Run it and watch the grey blob on the right morph into the PyTorch logo in a few seconds.</p>"},{"location":"03_training_test/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 <code>vultorch.imread</code> loads the image straight into a float32    CUDA tensor (no PIL, no numpy). Pixel coordinates get <code>meshgrid</code>'d into    <code>(H*W, 2)</code>, normalized to <code>[-1, 1]</code>.</p> </li> <li> <p>Model \u2014 a two-hidden-layer MLP (64 wide). Takes <code>(x, y)</code>, outputs    <code>(r, g, b)</code>. Small enough to run inside a per-frame callback without    tanking your framerate.</p> </li> <li> <p>Layout \u2014 <code>side=\"bottom\"</code> docks Info at the bottom 28 % of the    window.  <code>side=\"left\"</code> puts GT on the left half of the remaining    space.  Prediction fills whatever is left.  No manual docking code.</p> </li> <li> <p>Two callbacks \u2014 <code>@view.on_frame</code> runs the training loop.    <code>@info_panel.on_frame</code> draws widgets (text, slider, progress bar)    inside the Info panel.  Vultorch opens / closes the ImGui window    for you.</p> </li> </ol>"},{"location":"03_training_test/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 you can run arbitrary PyTorch code in the    callback. At the end of each frame, Vultorch uploads every bound    tensor to the screen automatically.</p> </li> <li> <p><code>create_tensor</code> \u2014 looks and feels like <code>torch.zeros</code>, but the    underlying memory is Vulkan/CUDA shared. Display is zero-copy.</p> </li> <li> <p>Declarative layout \u2014 <code>side=\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> /    <code>\"top\"</code> splits the window without manual docking code.</p> </li> <li> <p>Panel widgets \u2014 <code>@panel.on_frame</code> runs inside the panel's ImGui    window.  Use <code>panel.text()</code>, <code>panel.slider_int()</code>, <code>panel.progress()</code>    instead of raw <code>ui.*</code> calls.</p> </li> <li> <p>No terminal spam \u2014 all live stats live in the Info panel.    Your console stays clean for warnings and tracebacks.</p> </li> </ol> <p>Tip</p> <p>Crank <code>Steps / Frame</code> up to 32 for blazing-fast convergence. But don't get too greedy \u2014 go too high and your framerate will drop because each frame spends more time training.</p> <p>Note</p> <p><code>create_tensor</code> is called once at init, not every frame. After that you just write into the tensor each frame \u2014 practically free.</p>"},{"location":"04_conway/","title":"04 \u2014 Conway's Game of Life","text":"<p>Example file: <code>examples/04_conway.py</code></p> <p>Loss curves, training visualizations \u2014 all very serious. Let's take a break and build something fun: Conway's Game of Life, running entirely on the GPU, displayed in zero-copy, with buttons and sliders to play god.</p> <p>More importantly, this chapter shows that <code>create_tensor</code> isn't only for neural networks. Any GPU computation can be displayed through Vultorch's shared memory \u2014 simulations, procedural generation, physics, anything that lives on CUDA.</p>"},{"location":"04_conway/#what-were-building","title":"What we're building","text":"<p>A 256\u00d7256 cellular automaton with a control panel:</p> Area Content Left Controls \u2014 play/pause, step, speed slider, pattern presets, color pickers Right Grid \u2014 the simulation, pixel-perfect (<code>filter=\"nearest\"</code>) <p>Everything runs on the GPU. The display tensor uses <code>create_tensor</code> for zero-copy \u2014 the grid never touches the CPU.</p>"},{"location":"04_conway/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>filter=\"nearest\"</code> Pixel-perfect display, no interpolation Cell boundaries stay sharp <code>side=\"left\"</code> sidebar Dock a panel to the left Clean control-panel layout <code>Panel.on_frame</code> Per-panel widget callback Widgets stay inside their panel <code>panel.button</code>, <code>panel.row()</code> Buttons on the same row Compact UI without raw <code>ui.*</code> calls <code>panel.color_picker</code> Color picker Customize alive/dead colors Circular padding + conv2d GPU-parallel neighbour count The whole simulation is one convolution"},{"location":"04_conway/#the-simulation-trick","title":"The simulation trick","text":"<p>Counting neighbours in Conway's Game of Life is just a 2D convolution with a 3\u00d73 kernel of all ones (center zero):</p> <pre><code>1 1 1\n1 0 1\n1 1 1\n</code></pre> <p>PyTorch's <code>F.conv2d</code> does this in one GPU kernel call \u2014 no loops, no per-cell logic. Circular padding wraps the edges so gliders fly off one side and reappear on the other.</p> <pre><code>kernel = torch.tensor([[1, 1, 1],\n                        [1, 0, 1],\n                        [1, 1, 1]], dtype=torch.float32, device=device)\npadded = F.pad(inp, (1, 1, 1, 1), mode='circular')\nneighbours = F.conv2d(padded, kernel.reshape(1, 1, 3, 3)).squeeze()\n</code></pre> <p>Then the rules are just two boolean masks:</p> <pre><code>survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\nbirth   = (grid == 0) &amp; (neighbours == 3)\ngrid[:] = (survive | birth).float()\n</code></pre>"},{"location":"04_conway/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 Grid parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGRID_H, GRID_W = 256, 256\n\n# \u2500\u2500 View + panels \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"04 - Conway's Game of Life\", 1024, 768)\ngrid_panel = view.panel(\"Grid\")\nctrl_panel = view.panel(\"Controls\", side=\"left\", width=0.22)\n\n# \u2500\u2500 Display tensor (RGBA, zero-copy) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndisplay = vultorch.create_tensor(GRID_H, GRID_W, channels=4,\n                                 device=device, name=\"grid\",\n                                 window=view.window)\ncanvas = grid_panel.canvas(\"grid\", filter=\"nearest\")\ncanvas.bind(display)\n\n# \u2500\u2500 Simulation state \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ngrid = torch.zeros(GRID_H, GRID_W, dtype=torch.float32, device=device)\n\nstate = {\n    \"running\": False,\n    \"generation\": 0,\n    \"speed\": 1,\n    \"prob\": 0.3,\n    \"alive_color\": (0.0, 1.0, 0.4),\n    \"dead_color\": (0.05, 0.05, 0.08),\n}\n\n\ndef randomize():\n    grid[:] = (torch.rand(GRID_H, GRID_W, device=device) &lt; state[\"prob\"]).float()\n    state[\"generation\"] = 0\n\ndef clear():\n    grid.zero_()\n    state[\"generation\"] = 0\n\ndef step_simulation():\n    kernel = torch.tensor([[1, 1, 1],\n                            [1, 0, 1],\n                            [1, 1, 1]], dtype=torch.float32, device=device)\n    inp = grid.unsqueeze(0).unsqueeze(0)\n    k = kernel.unsqueeze(0).unsqueeze(0)\n    padded = torch.nn.functional.pad(inp, (1, 1, 1, 1), mode='circular')\n    neighbours = torch.nn.functional.conv2d(padded, k).squeeze()\n\n    survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\n    birth = (grid == 0) &amp; (neighbours == 3)\n    grid[:] = (survive | birth).float()\n    state[\"generation\"] += 1\n\ndef grid_to_display():\n    alive_r, alive_g, alive_b = state[\"alive_color\"]\n    dead_r, dead_g, dead_b = state[\"dead_color\"]\n    display[:, :, 0] = dead_r + (alive_r - dead_r) * grid\n    display[:, :, 1] = dead_g + (alive_g - dead_g) * grid\n    display[:, :, 2] = dead_b + (alive_b - dead_b) * grid\n    display[:, :, 3] = 1.0\n\nrandomize()\n\n\n@view.on_frame\ndef update():\n    if state[\"running\"]:\n        for _ in range(state[\"speed\"]):\n            step_simulation()\n    grid_to_display()\n\n\n@ctrl_panel.on_frame\ndef draw_controls():\n    ctrl_panel.text(f\"Generation: {state['generation']}\")\n    ctrl_panel.text(f\"Alive cells: {int(grid.sum().item())}\")\n    ctrl_panel.text(f\"FPS: {view.fps:.1f}\")\n    ctrl_panel.separator()\n\n    with ctrl_panel.row():\n        label = \"Pause\" if state[\"running\"] else \"Play\"\n        if ctrl_panel.button(label, width=80):\n            state[\"running\"] = not state[\"running\"]\n        if ctrl_panel.button(\"Step\", width=80):\n            step_simulation()\n\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Randomize\", width=80):\n            randomize()\n        if ctrl_panel.button(\"Clear\", width=80):\n            clear()\n\n    ctrl_panel.separator()\n    state[\"speed\"] = ctrl_panel.slider_int(\"Speed\", 1, 20, default=1)\n    state[\"prob\"] = ctrl_panel.slider(\"Cell Probability\", 0.05, 0.8,\n                                       default=0.3)\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Colors\")\n    state[\"alive_color\"] = ctrl_panel.color_picker(\n        \"Alive\", default=(0.0, 1.0, 0.4))\n    state[\"dead_color\"] = ctrl_panel.color_picker(\n        \"Dead\", default=(0.05, 0.05, 0.08))\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Patterns\")\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Glider\", width=80):\n            clear()\n            grid[1, 2] = 1; grid[2, 3] = 1\n            grid[3, 1] = 1; grid[3, 2] = 1; grid[3, 3] = 1\n        if ctrl_panel.button(\"Pulsar\", width=80):\n            clear()\n            # ... place pulsar pattern ...\n        if ctrl_panel.button(\"Gosper Gun\", width=100):\n            clear()\n            # ... place Gosper glider gun ...\n\n    ctrl_panel.separator()\n    ctrl_panel.text_wrapped(\n        \"Click Play to start, or Step to advance one generation. \"\n        \"Use Randomize to reset with random cells.\"\n    )\n\n\nview.run()\n</code></pre> <p>(The full example file includes helper functions for all pattern placements.)</p>"},{"location":"04_conway/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Grid \u2014 a plain <code>(256, 256)</code> float32 CUDA tensor.  <code>1.0</code> = alive,    <code>0.0</code> = dead.  No classes, no fancy data structures \u2014 just a tensor.</p> </li> <li> <p>Simulation \u2014 <code>step_simulation()</code> uses <code>F.conv2d</code> with circular    padding to count neighbours, then applies the birth/survive rules    with boolean masks.  The entire generation runs in two GPU kernels.</p> </li> <li> <p>Display \u2014 <code>create_tensor</code> allocates shared Vulkan/CUDA memory.    <code>grid_to_display()</code> lerps between dead and alive colors and writes    into it.  Zero copy to screen.</p> </li> <li> <p>Controls \u2014 <code>@ctrl_panel.on_frame</code> draws all widgets inside the    Controls panel.  <code>panel.button()</code>, <code>panel.slider_int()</code>,    <code>panel.color_picker()</code>, and <code>with panel.row()</code> keep the layout    compact.  State lives in a plain Python dict.</p> </li> </ol>"},{"location":"04_conway/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>create_tensor</code> is for everything \u2014 not just neural networks.    Any GPU computation that produces an image-like tensor can be    displayed with zero copy.</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014 crucial for pixel-art / grid simulations.    Without it, bilinear interpolation blurs cell boundaries into mush.</p> </li> <li> <p>Convolution = neighbour counting \u2014 a cute trick that replaces    nested Python loops with a single GPU kernel.  The game runs at    hundreds of FPS even at large grid sizes.</p> </li> <li> <p>Panel widgets \u2014 <code>@panel.on_frame</code> + <code>panel.button()</code>,    <code>panel.slider_int()</code>, <code>panel.color_picker()</code> keep all UI code inside    the panel's window context.  No manual <code>ui.begin/end</code> needed.</p> </li> <li> <p><code>panel.row()</code> \u2014 context manager that places child widgets    side-by-side.  Cleaner than calling <code>ui.same_line()</code> by hand.</p> </li> <li> <p>Pattern presets \u2014 the Glider, Pulsar, and Gosper Gun buttons    demonstrate how to set initial conditions by writing directly into    the grid tensor.</p> </li> </ol> <p>Tip</p> <p>Crank the Speed slider to 20 and watch the grid evolve at 20 generations per frame.  On a modern GPU you'll still hold 60+ FPS.</p> <p>Note</p> <p>The grid wraps around thanks to <code>mode='circular'</code> padding. Gliders that fly off the right edge reappear on the left.</p>"},{"location":"05_image_viewer/","title":"05 \u2014 Image Viewer","text":"<p>Example file: <code>examples/05_image_viewer.py</code></p> <p>So far we've been manufacturing tensors from thin air \u2014 gradients, checkerboards, neural network outputs. Very impressive, but at some point you probably want to look at an actual image. You know, the kind that lives on your hard drive. In a <code>.png</code> file. Like a normal person.</p> <p>\"Just use PIL,\" you say. Sure \u2014 and then <code>torchvision.transforms</code>, and then <code>numpy</code>, and then <code>cv2.cvtColor</code> because someone mixed up RGB and BGR again, and then you're three Stack Overflow tabs deep wondering why everything is upside-down and slightly green.</p> <p>Vultorch has built-in image I/O. One function in, one function out. No PIL, no OpenCV, no existential dread.</p>"},{"location":"05_image_viewer/#new-friends","title":"New friends","text":"New thing What it does How to use imread Load a file into a CUDA tensor <code>vultorch.imread(\"photo.png\")</code> imwrite Save a tensor to a file <code>vultorch.imwrite(\"out.png\", t)</code> Canvas.save() Save the canvas's bound tensor <code>canvas.save(\"out.png\")</code> panel.combo() Drop-down selector <code>panel.combo(\"Pick\", [\"A\",\"B\"])</code> panel.input_text() Text input field <code>panel.input_text(\"Path\")</code> canvas.filter Sampling mode (<code>\"linear\"</code> / <code>\"nearest\"</code>) <code>canvas.filter = \"nearest\"</code>"},{"location":"05_image_viewer/#what-were-building","title":"What we're building","text":"<p>A mini image viewer: load a photo, pick a transform from a drop-down, tweak brightness / contrast with sliders, and save the result.</p> Left Right (two canvases) Controls \u2014 transform combo, brightness/contrast sliders, filter toggle, save Original (top) Transformed (bottom)"},{"location":"05_image_viewer/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 Load image \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\noriginal = vultorch.imread(img_path, channels=3, device=device)\nH, W, C = original.shape\n\n# Working copy for transforms\ntransformed = original.clone()\n\n# \u2500\u2500 View + panels \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"05 - Image Viewer\", 1024, 768)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nimg_panel = view.panel(\"Image\")\n\ncanvas_orig = img_panel.canvas(\"Original\")\ncanvas_orig.bind(original)\n\ncanvas_xform = img_panel.canvas(\"Transformed\")\ncanvas_xform.bind(transformed)\n\n# \u2500\u2500 State \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRANSFORMS = [\n    \"None\",\n    \"Horizontal Flip\",\n    \"Vertical Flip\",\n    \"Grayscale\",\n    \"Invert\",\n    \"Sepia\",\n]\n\nstate = {\n    \"brightness\": 0.0,\n    \"contrast\": 1.0,\n    \"last_transform\": -1,\n    \"last_brightness\": None,\n    \"last_contrast\": None,\n}\n\n\ndef apply_transform(img, idx):\n    if idx == 0:    return img.clone()\n    elif idx == 1:  return img.flip(1)               # horizontal flip\n    elif idx == 2:  return img.flip(0)               # vertical flip\n    elif idx == 3:                                    # grayscale\n        gray = img[:,:,0]*0.299 + img[:,:,1]*0.587 + img[:,:,2]*0.114\n        return gray.unsqueeze(-1).expand_as(img).contiguous()\n    elif idx == 4:  return 1.0 - img                 # invert\n    elif idx == 5:                                    # sepia\n        r = img[:,:,0]*0.393 + img[:,:,1]*0.769 + img[:,:,2]*0.189\n        g = img[:,:,0]*0.349 + img[:,:,1]*0.686 + img[:,:,2]*0.168\n        b = img[:,:,0]*0.272 + img[:,:,1]*0.534 + img[:,:,2]*0.131\n        return torch.stack([r, g, b], dim=-1).clamp(0, 1)\n    return img.clone()\n\n\ndef apply_brightness_contrast(img, brightness, contrast):\n    return ((img - 0.5) * contrast + 0.5 + brightness).clamp(0, 1)\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Image: {img_path.name}\")\n    ctrl.text(f\"Size: {W} \u00d7 {H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    # Transform selector\n    ctrl.text(\"Transform\")\n    xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n\n    ctrl.separator()\n\n    # Brightness / Contrast\n    ctrl.text(\"Adjustments\")\n    brightness = ctrl.slider(\"Brightness\", -1.0, 1.0, default=0.0)\n    contrast   = ctrl.slider(\"Contrast\",    0.0, 3.0, default=1.0)\n\n    changed = (xform_idx != state[\"last_transform\"]\n               or brightness != state[\"last_brightness\"]\n               or contrast   != state[\"last_contrast\"])\n\n    if changed:\n        result = apply_transform(original, xform_idx)\n        result = apply_brightness_contrast(result, brightness, contrast)\n        transformed[:] = result\n        state[\"last_transform\"]  = xform_idx\n        state[\"last_brightness\"] = brightness\n        state[\"last_contrast\"]   = contrast\n\n    ctrl.separator()\n\n    # Filter toggle\n    ctrl.text(\"Sampling Filter\")\n    filter_idx = ctrl.combo(\"##filter\", [\"Linear\", \"Nearest\"], default=0)\n    canvas_orig.filter  = \"nearest\" if filter_idx == 1 else \"linear\"\n    canvas_xform.filter = \"nearest\" if filter_idx == 1 else \"linear\"\n\n    ctrl.separator()\n\n    # Save\n    ctrl.text(\"Save Output\")\n    save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n\n    if ctrl.button(\"Save Image\", width=140):\n        try:\n            canvas_xform.save(save_path)\n            state[\"save_msg\"] = f\"Saved to {save_path}\"\n        except Exception as e:\n            state[\"save_msg\"] = f\"Error: {e}\"\n\n    if \"save_msg\" in state:\n        ctrl.text_wrapped(state[\"save_msg\"])\n\n\nview.run()\n</code></pre>"},{"location":"05_image_viewer/#what-just-happened","title":"What just happened?","text":""},{"location":"05_image_viewer/#imread-images-without-the-dependency-hell","title":"imread \u2014 images without the dependency hell","text":"<pre><code>original = vultorch.imread(img_path, channels=3, device=device)\n</code></pre> <p>One line. Returns a <code>(H, W, 3)</code> float32 CUDA tensor with values in <code>[0, 1]</code>. Supports PNG, JPEG, BMP, TGA, HDR, PSD, and GIF (first frame). Uses <code>stb_image</code> under the hood \u2014 no Python image library needed.</p> <p>Optional parameters:</p> <ul> <li><code>channels=4</code> \u2014 force RGBA output.</li> <li><code>size=(256, 256)</code> \u2014 resize after loading (bilinear interpolation).</li> <li><code>device=\"cpu\"</code> \u2014 keep it on CPU if you prefer.</li> <li><code>shared=True</code> \u2014 allocate via <code>create_tensor</code> for zero-copy display.</li> </ul>"},{"location":"05_image_viewer/#combo-the-drop-down-menu","title":"combo \u2014 the drop-down menu","text":"<pre><code>xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n</code></pre> <p>Shows a drop-down with the items in the list. Returns the index (int) of the selected item. The state is managed automatically by the panel \u2014 just pass <code>default=</code> for the initial selection.</p> <p>The <code>##</code> prefix hides the label in ImGui (the text after <code>##</code> is used as an internal ID only). Useful when you don't want a label next to your widget.</p>"},{"location":"05_image_viewer/#input_text-free-text-entry","title":"input_text \u2014 free text entry","text":"<pre><code>save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n</code></pre> <p>Returns the current string. Type a filename, hit Enter or click Save. <code>max_length=256</code> by default \u2014 plenty for a file path.</p>"},{"location":"05_image_viewer/#canvassave-one-line-export","title":"Canvas.save() \u2014 one-line export","text":"<pre><code>canvas_xform.save(save_path)\n</code></pre> <p>Saves whatever tensor is currently bound to the canvas. The file format is inferred from the extension (<code>.png</code>, <code>.jpg</code>, <code>.bmp</code>, <code>.tga</code>, <code>.hdr</code>). Under the hood it calls <code>vultorch.imwrite()</code>.</p>"},{"location":"05_image_viewer/#filter-nearest-vs-linear","title":"filter \u2014 nearest vs linear","text":"<pre><code>canvas_orig.filter = \"nearest\"   # pixel-perfect, blocky when zoomed\ncanvas_orig.filter = \"linear\"    # bilinear interpolation, smooth\n</code></pre> <p>Switch the sampling filter at any time. Try toggling it when the image is stretched \u2014 <code>\"nearest\"</code> shows you the raw pixels, <code>\"linear\"</code> blurs them into smooth gradients. For scientific visualization (segmentation masks, attention maps) you almost always want <code>\"nearest\"</code>.</p>"},{"location":"05_image_viewer/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>imread</code> / <code>imwrite</code> \u2014 zero-dependency image I/O. Reads straight    into a CUDA tensor, writes straight from one. No PIL, no numpy, no    <code>cv2.cvtColor</code> misadventures.</p> </li> <li> <p><code>combo</code> \u2014 drop-down selection. Returns an int index. Perfect for    mode switches, preset selectors, enum-style choices.</p> </li> <li> <p><code>input_text</code> \u2014 free-form string input. Useful for file paths,    model names, experiment tags.</p> </li> <li> <p><code>Canvas.save()</code> \u2014 save the bound tensor to disk in one call.    Extension determines the format.</p> </li> <li> <p>Lazy recomputation \u2014 we only re-run the transform when a slider    or combo value actually changes. Checking <code>changed</code> before doing    tensor ops avoids wasting GPU cycles every frame.</p> </li> </ol> <p>Tip</p> <p><code>imread</code> supports a <code>size=(H, W)</code> argument for resizing at load time. Useful when your image is 4K but you only need a 256\u00d7256 preview.</p> <p>Note</p> <p><code>imwrite</code> accepts float32 tensors in <code>[0, 1]</code> as well as uint8 tensors in <code>[0, 255]</code>. It handles the conversion automatically.</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for all public classes and functions in the <code>vultorch</code> package.</p>"},{"location":"api/#module-level-attributes","title":"Module-level Attributes","text":""},{"location":"api/#vultorch__version__","title":"<code>vultorch.__version__</code>","text":"<pre><code>__version__: str\n</code></pre> <p>Package version string (e.g. <code>\"0.5.0\"</code>).</p>"},{"location":"api/#vultorchhas_cuda","title":"<code>vultorch.HAS_CUDA</code>","text":"<pre><code>HAS_CUDA: bool\n</code></pre> <p><code>True</code> if the native extension was compiled with CUDA support. When <code>False</code>, all tensor display falls back to CPU staging (host-visible <code>memcpy</code>).</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#vultorchshow","title":"<code>vultorch.show()</code>","text":"<pre><code>def show(\n    tensor: torch.Tensor,\n    *,\n    name: str = \"tensor\",\n    width: float = 0,\n    height: float = 0,\n    filter: str = \"linear\",\n    window: Window | None = None,\n) -&gt; None\n</code></pre> <p>Display a tensor in the current ImGui context.</p> <p>Parameters:</p> Parameter Type Default Description <code>tensor</code> <code>torch.Tensor</code> (required) CUDA or CPU tensor. dtype: <code>float32</code>, <code>float16</code>, or <code>uint8</code>. Shape: <code>(H, W)</code> or <code>(H, W, C)</code> with C \u2208 {1, 3, 4}. <code>name</code> <code>str</code> <code>\"tensor\"</code> Unique label for caching when showing multiple tensors. <code>width</code> <code>float</code> <code>0</code> Display width in pixels. <code>0</code> = auto-fit to tensor. <code>height</code> <code>float</code> <code>0</code> Display height in pixels. <code>0</code> = auto-fit to tensor. <code>filter</code> <code>str</code> <code>\"linear\"</code> Sampling filter: <code>\"nearest\"</code> or <code>\"linear\"</code>. <code>window</code> <code>Window \\| None</code> <code>None</code> Target window. Defaults to <code>Window._current</code>. <p>Behavior:</p> <ul> <li>1-channel and 3-channel tensors are automatically expanded to RGBA.</li> <li>RGBA expansion buffers are cached per <code>name</code> to avoid per-frame allocation.</li> <li>On CUDA: uses zero-copy GPU\u2192GPU path. On CPU: uses host-visible staging buffer.</li> <li><code>uint8</code> tensors are divided by 255; <code>float16</code> tensors are converted to <code>float32</code>.</li> </ul> <p>Raises: <code>RuntimeError</code> if no active <code>Window</code> exists.</p>"},{"location":"api/#vultorchcreate_tensor","title":"<code>vultorch.create_tensor()</code>","text":"<pre><code>def create_tensor(\n    height: int,\n    width: int,\n    channels: int = 4,\n    device: str = \"cuda:0\",\n    *,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Allocate a Vulkan-shared CUDA tensor for true zero-copy display.</p> <p>Parameters:</p> Parameter Type Default Description <code>height</code> <code>int</code> (required) Tensor height in pixels. <code>width</code> <code>int</code> (required) Tensor width in pixels. <code>channels</code> <code>int</code> <code>4</code> Number of channels: 1, 3, or 4. <code>device</code> <code>str</code> <code>\"cuda:0\"</code> CUDA device string, or <code>\"cpu\"</code>. <code>name</code> <code>str</code> <code>\"tensor\"</code> Texture slot name (must match <code>show(..., name=...)</code>). <code>window</code> <code>Window \\| None</code> <code>None</code> Target window. Defaults to <code>Window._current</code>. <p>Returns: <code>torch.Tensor</code> of shape <code>(height, width, channels)</code>.</p> <p>Note</p> <p>Only <code>channels=4</code> gives true zero-copy via Vulkan external memory. For 1 or 3 channels, a regular CUDA tensor is returned and <code>show()</code> handles RGBA expansion with a GPU\u2192GPU copy.</p> <p>Raises: <code>RuntimeError</code> if no active <code>Window</code> exists.</p>"},{"location":"api/#vultorchimread","title":"<code>vultorch.imread()</code>","text":"<pre><code>def imread(\n    path: str,\n    *,\n    channels: int = 4,\n    size: tuple[int, int] | None = None,\n    device: str = \"cuda\",\n    shared: bool = False,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Load an image file into a <code>float32</code> tensor. Uses stb_image internally \u2014 no PIL or numpy needed.</p> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> (required) File path (PNG, JPG, BMP, TGA, HDR, \u2026). <code>channels</code> <code>int</code> <code>4</code> Desired channels: 1 (gray), 3 (RGB), or 4 (RGBA). <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> Optional <code>(height, width)</code> to resize with bilinear interpolation. <code>device</code> <code>str</code> <code>\"cuda\"</code> Target device (<code>\"cuda\"</code> or <code>\"cpu\"</code>). <code>shared</code> <code>bool</code> <code>False</code> If <code>True</code>, allocate via <code>create_tensor</code> for zero-copy display. <code>name</code> <code>str</code> <code>\"tensor\"</code> Texture slot name (only used when <code>shared=True</code>). <code>window</code> <code>Window \\| None</code> <code>None</code> Target window (only used when <code>shared=True</code>). <p>Returns: <code>torch.Tensor</code> of shape <code>(H, W, C)</code> with values in <code>[0, 1]</code>.</p> <p>Example:</p> <pre><code>import vultorch\ngt = vultorch.imread(\"photo.png\", channels=3, size=(256, 256), device=\"cuda\")\n</code></pre>"},{"location":"api/#vultorchimwrite","title":"<code>vultorch.imwrite()</code>","text":"<pre><code>def imwrite(\n    path: str,\n    tensor: torch.Tensor,\n    *,\n    channels: int = 0,\n    size: tuple[int, int] | None = None,\n    quality: int = 95,\n) -&gt; None\n</code></pre> <p>Save a tensor to an image file. Format is inferred from the extension.</p> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> (required) Output file path. Extension selects format: <code>.png</code>, <code>.jpg</code>, <code>.bmp</code>, <code>.tga</code>, <code>.hdr</code>. <code>tensor</code> <code>torch.Tensor</code> (required) <code>(H, W)</code>, <code>(H, W, 1)</code>, <code>(H, W, 3)</code>, or <code>(H, W, 4)</code> tensor. <code>channels</code> <code>int</code> <code>0</code> Override output channels. <code>0</code> = use tensor's channel count. <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> Optional <code>(height, width)</code> to resize before saving. <code>quality</code> <code>int</code> <code>95</code> JPEG quality (1\u2013100). Ignored for other formats. <p>Behavior:</p> <ul> <li><code>.hdr</code> writes 32-bit float data; all other formats quantize to 8-bit.</li> <li>If the tensor has more channels than <code>channels</code>, extras are dropped. If fewer, missing channels are filled (alpha \u2192 1.0).</li> <li>Tensor is moved to CPU and converted to <code>float32</code> before writing.</li> </ul> <p>Example:</p> <pre><code>vultorch.imwrite(\"output.png\", pred_tensor, channels=3)\nvultorch.imwrite(\"output.jpg\", pred_tensor, quality=90)\n</code></pre>"},{"location":"api/#classes","title":"Classes","text":""},{"location":"api/#vultorchwindow","title":"<code>vultorch.Window</code>","text":"<pre><code>class Window:\n    _current: Window | None   # singleton reference\n\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>High-level wrapper around the Vulkan + SDL3 + ImGui engine. Creating a <code>Window</code> automatically makes it the current target for <code>show()</code> and <code>create_tensor()</code>.</p>"},{"location":"api/#methods","title":"Methods","text":"Method Signature Description <code>poll()</code> <code>\u2192 bool</code> Process OS events. Returns <code>False</code> when the window should close. <code>begin_frame()</code> <code>\u2192 bool</code> Begin a new ImGui frame. Returns <code>False</code> if the frame was skipped (minimized). <code>end_frame()</code> <code>\u2192 None</code> Submit the frame to the GPU and present. <code>activate()</code> <code>\u2192 None</code> Make this window the current target for module-level helpers. <code>upload_tensor(tensor, *, name)</code> <code>\u2192 None</code> Upload a tensor for display (CUDA or CPU). <code>get_texture_id(name)</code> <code>\u2192 int</code> ImGui texture ID for a named tensor. <code>get_texture_size(name)</code> <code>\u2192 (int, int)</code> <code>(width, height)</code> for a named tensor. <code>destroy()</code> <code>\u2192 None</code> Release all GPU / window resources. Safe to call multiple times."},{"location":"api/#properties","title":"Properties","text":"Property Type Description <code>tensor_texture_id</code> <code>int</code> ImGui texture ID of the default <code>\"tensor\"</code> slot. <code>tensor_size</code> <code>(int, int)</code> <code>(width, height)</code> of the default <code>\"tensor\"</code> slot."},{"location":"api/#usage","title":"Usage","text":"<pre><code>import vultorch\nfrom vultorch import ui\n\nwin = vultorch.Window(\"Demo\", 1280, 720)\nwhile win.poll():\n    if not win.begin_frame():\n        continue\n    ui.begin(\"Panel\", True, 0)\n    vultorch.show(tensor)\n    ui.end()\n    win.end_frame()\nwin.destroy()\n</code></pre>"},{"location":"api/#vultorchcamera","title":"<code>vultorch.Camera</code>","text":"<pre><code>class Camera:\n    azimuth: float     # horizontal angle (radians), default 0.0\n    elevation: float   # vertical angle (radians), default 0.6\n    distance: float    # distance from target, default 3.0\n    target: tuple      # (x, y, z) look-at point, default (0, 0, 0)\n    fov: float         # field of view (degrees), default 45.0\n</code></pre> <p>Orbit camera parameters used by <code>SceneView</code>. Call <code>reset()</code> to restore defaults.</p>"},{"location":"api/#vultorchlight","title":"<code>vultorch.Light</code>","text":"<pre><code>class Light:\n    direction: tuple   # (x, y, z), default (0.3, -1.0, 0.5)\n    color: tuple       # (r, g, b), default (1, 1, 1)\n    intensity: float   # default 1.0\n    ambient: float     # ambient term, default 0.15\n    specular: float    # specular term, default 0.5\n    shininess: float   # Blinn-Phong exponent, default 32.0\n    enabled: bool      # default True\n</code></pre> <p>Blinn-Phong directional light parameters used by <code>SceneView</code>.</p>"},{"location":"api/#vultorchsceneview","title":"<code>vultorch.SceneView</code>","text":"<pre><code>class SceneView:\n    def __init__(self, name: str = \"SceneView\",\n                 width: int = 800, height: int = 600,\n                 msaa: int = 4) -&gt; None: ...\n</code></pre> <p>3D tensor viewer \u2014 renders a tensor on a lit plane with orbit camera and MSAA.</p>"},{"location":"api/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>name</code> <code>str</code> <code>\"SceneView\"</code> ImGui window label. <code>camera</code> <code>Camera</code> (auto) Orbit camera (drag to rotate). <code>light</code> <code>Light</code> (auto) Directional light. <code>background</code> <code>tuple</code> <code>(0.12, 0.12, 0.14)</code> Background color <code>(r, g, b)</code>. <code>msaa</code> <code>int</code> <code>4</code> Multi-sample anti-aliasing level (1/2/4/8)."},{"location":"api/#methods_1","title":"Methods","text":"Method Description <code>set_tensor(tensor)</code> Upload a tensor to the scene's texture. <code>render()</code> Process mouse interaction, render the scene, and display as an ImGui image."},{"location":"api/#usage_1","title":"Usage","text":"<pre><code>scene = vultorch.SceneView(\"3D View\", 800, 600, msaa=4)\n# inside frame loop:\nscene.set_tensor(tensor)\nscene.render()\n</code></pre>"},{"location":"api/#declarative-api","title":"Declarative API","text":"<p>The declarative API provides a higher-level abstraction for building multi-panel visualization apps.</p>"},{"location":"api/#vultorchview","title":"<code>vultorch.View</code>","text":"<pre><code>class View:\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>Top-level window with automatic docking layout.</p>"},{"location":"api/#methods_2","title":"Methods","text":"Method Signature Description <code>panel(name, *, side, width)</code> <code>\u2192 Panel</code> Create or retrieve a dockable panel. <code>side</code>: <code>\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code> / <code>None</code>. <code>on_frame(fn)</code> <code>\u2192 fn</code> Decorator \u2014 register a per-frame callback. <code>run()</code> <code>\u2192 None</code> Blocking event loop. <code>step()</code> <code>\u2192 bool</code> Non-blocking: process one frame. Returns <code>False</code> on close. <code>end_step()</code> <code>\u2192 None</code> Finish the frame started by <code>step()</code>. <code>close()</code> <code>\u2192 None</code> Destroy the window."},{"location":"api/#properties_1","title":"Properties","text":"Property Type Description <code>fps</code> <code>float</code> Current frames per second. <code>time</code> <code>float</code> Elapsed time in seconds. <code>window</code> <code>Window</code> Underlying <code>Window</code> instance."},{"location":"api/#usage-blocking","title":"Usage \u2014 Blocking","text":"<pre><code>view = vultorch.View(\"Demo\", 1280, 720)\nview.panel(\"Viewer\").canvas(\"img\").bind(tensor)\n\n@view.on_frame\ndef update():\n    speed = controls.slider(\"Speed\", 0, 10)\n    tensor[:,:,0] = (x + view.time * speed).sin()\n\nview.run()\n</code></pre>"},{"location":"api/#usage-training-loop","title":"Usage \u2014 Training Loop","text":"<pre><code>view = vultorch.View(\"Train\", 1024, 768)\noutput = view.panel(\"Output\").canvas(\"result\")\nfor epoch in range(100):\n    result = model(input)\n    output.bind(result)\n    if not view.step():\n        break\n    view.end_step()\nview.close()\n</code></pre>"},{"location":"api/#vultorchpanel","title":"<code>vultorch.Panel</code>","text":"<pre><code>class Panel:\n    # Created via View.panel() \u2014 not instantiated directly\n</code></pre> <p>A dockable panel containing canvases and widgets.</p>"},{"location":"api/#canvas-factory","title":"Canvas Factory","text":"Method Signature Description <code>canvas(name, *, filter, fit)</code> <code>\u2192 Canvas</code> Create a named canvas. <code>filter</code>: <code>\"linear\"</code> / <code>\"nearest\"</code>. <code>fit</code>: auto-fill panel space."},{"location":"api/#per-panel-callback","title":"Per-Panel Callback","text":"Method Signature Description <code>on_frame(fn)</code> <code>\u2192 fn</code> Decorator \u2014 register a per-frame callback that runs inside the panel's ImGui window."},{"location":"api/#layout","title":"Layout","text":"Method Description <code>row()</code> Context manager \u2014 place child widgets side-by-side."},{"location":"api/#widgets","title":"Widgets","text":"<p>All widget methods manage state automatically across frames.</p> Method Signature Description <code>text(text)</code> <code>\u2192 None</code> Static text. <code>text_colored(r, g, b, a, text)</code> <code>\u2192 None</code> Colored text. <code>text_wrapped(text)</code> <code>\u2192 None</code> Auto-wrapping text. <code>separator()</code> <code>\u2192 None</code> Horizontal separator line. <code>button(label, width, height)</code> <code>\u2192 bool</code> Button. Returns <code>True</code> when clicked. <code>width</code>/<code>height</code> default to <code>0</code> (auto-size). <code>checkbox(label, *, default)</code> <code>\u2192 bool</code> Checkbox with stateful toggle. <code>slider(label, min, max, *, default)</code> <code>\u2192 float</code> Float slider. <code>slider_int(label, min, max, *, default)</code> <code>\u2192 int</code> Integer slider. <code>color_picker(label, *, default)</code> <code>\u2192 (r, g, b)</code> Color picker (3-float tuple). <code>combo(label, items, *, default)</code> <code>\u2192 int</code> Dropdown combo box. Returns selected index. <code>input_text(label, *, default, max_length)</code> <code>\u2192 str</code> Text input field. <code>plot(values, *, label, overlay, width, height)</code> <code>\u2192 None</code> Line plot from a list of floats. <code>progress(fraction, *, overlay)</code> <code>\u2192 None</code> Progress bar (0.0 \u2013 1.0)."},{"location":"api/#vultorchcanvas","title":"<code>vultorch.Canvas</code>","text":"<pre><code>class Canvas:\n    # Created via Panel.canvas() \u2014 not instantiated directly\n</code></pre> <p>A display surface that renders a bound tensor as an ImGui image.</p>"},{"location":"api/#methods_3","title":"Methods","text":"Method Signature Description <code>bind(tensor)</code> <code>\u2192 Canvas</code> Bind a tensor for display. Returns <code>self</code> for chaining. <code>alloc(height, width, channels, device)</code> <code>\u2192 torch.Tensor</code> Allocate Vulkan-shared memory and auto-bind. Returns the tensor. <code>save(path, *, channels, size, quality)</code> <code>\u2192 None</code> Save the bound tensor to an image file via <code>imwrite()</code>."},{"location":"api/#properties_2","title":"Properties","text":"Property Type Default Description <code>filter</code> <code>str</code> <code>\"linear\"</code> <code>\"linear\"</code> or <code>\"nearest\"</code>. <code>fit</code> <code>bool</code> <code>True</code> Auto-fill available panel space."},{"location":"api/#imgui-bindings-vultorchui","title":"ImGui Bindings (<code>vultorch.ui</code>)","text":"<p>The <code>vultorch.ui</code> submodule exposes Dear ImGui functions (docking branch). All functions map directly to their ImGui C++ counterparts.</p>"},{"location":"api/#windows","title":"Windows","text":"<pre><code>ui.begin(name: str, opened: bool = True, flags: int = 0) -&gt; tuple[bool, bool]\nui.end() -&gt; None\nui.begin_child(id: str, width=0.0, height=0.0, child_flags=0, window_flags=0) -&gt; bool\nui.end_child() -&gt; None\n</code></pre>"},{"location":"api/#text","title":"Text","text":"<pre><code>ui.text(text: str) -&gt; None\nui.text_colored(r, g, b, a, text: str) -&gt; None\nui.text_disabled(text: str) -&gt; None\nui.text_wrapped(text: str) -&gt; None\nui.label_text(label: str, text: str) -&gt; None\nui.bullet_text(text: str) -&gt; None\n</code></pre>"},{"location":"api/#buttons","title":"Buttons","text":"<pre><code>ui.button(label: str, width=0.0, height=0.0) -&gt; bool\nui.small_button(label: str) -&gt; bool\nui.invisible_button(id: str, width, height) -&gt; bool\nui.arrow_button(id: str, direction: int) -&gt; bool\nui.radio_button(label: str, active: bool) -&gt; bool\n</code></pre>"},{"location":"api/#inputs","title":"Inputs","text":"<pre><code>ui.checkbox(label, value: bool) -&gt; bool\nui.slider_float(label, value, min=0.0, max=1.0, format=\"%.3f\") -&gt; float\nui.slider_float2(label, v1, v2, min, max) -&gt; tuple[float, float]\nui.slider_float3(label, v1, v2, v3, min, max) -&gt; tuple[float, float, float]\nui.slider_float4(label, v1, v2, v3, v4, min, max) -&gt; tuple\nui.slider_int(label, value, min=0, max=100) -&gt; int\nui.slider_angle(label, value, min=-360, max=360) -&gt; float\nui.drag_float(label, value, speed=1.0) -&gt; float\nui.drag_float2(label, v1, v2, speed=1.0) -&gt; tuple\nui.drag_float3(label, v1, v2, v3, speed=1.0) -&gt; tuple\nui.drag_int(label, value, speed=1.0) -&gt; int\nui.input_float(label, value) -&gt; float\nui.input_float2(label, v1, v2) -&gt; tuple\nui.input_float3(label, v1, v2, v3) -&gt; tuple\nui.input_float4(label, v1, v2, v3, v4) -&gt; tuple\nui.input_int(label, value) -&gt; int\nui.input_text(label, text, max_length=256) -&gt; str\nui.input_text_multiline(label, text, max_length=1024) -&gt; str\n</code></pre>"},{"location":"api/#colors","title":"Colors","text":"<pre><code>ui.color_edit3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_edit4(label, r, g, b, a, flags=0) -&gt; tuple[float, float, float, float]\nui.color_picker3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_picker4(label, r, g, b, a, flags=0) -&gt; tuple\n</code></pre>"},{"location":"api/#selection","title":"Selection","text":"<pre><code>ui.combo(label, current: int, items: list[str]) -&gt; int\nui.listbox(label, current: int, items: list[str], height_items=-1) -&gt; int\nui.tree_node(label: str) -&gt; bool\nui.tree_pop() -&gt; None\nui.collapsing_header(label: str) -&gt; bool\nui.selectable(label: str, selected: bool = False) -&gt; bool\n</code></pre>"},{"location":"api/#tabs","title":"Tabs","text":"<pre><code>ui.begin_tab_bar(id: str) -&gt; bool\nui.end_tab_bar() -&gt; None\nui.begin_tab_item(label: str) -&gt; bool\nui.end_tab_item() -&gt; None\n</code></pre>"},{"location":"api/#display","title":"Display","text":"<pre><code>ui.progress_bar(fraction, sx=-1.0, sy=0.0, overlay=\"\")\nui.image(texture_id: int, width, height, uv0x=0, uv0y=0, uv1x=1, uv1y=1)\nui.image_button(id: str, texture_id: int, width, height) -&gt; bool\nui.plot_lines(label, values: list[float], offset=0, overlay=\"\", ...)\nui.plot_histogram(label, values: list[float], offset=0, overlay=\"\", ...)\n</code></pre>"},{"location":"api/#layout_1","title":"Layout","text":"<pre><code>ui.separator()\nui.same_line(offset=0.0, spacing=-1.0)\nui.new_line()\nui.spacing()\nui.dummy(width, height)\nui.indent(width=0.0)\nui.unindent(width=0.0)\nui.begin_group()\nui.end_group()\nui.push_item_width(width)\nui.pop_item_width()\nui.columns(count=1, id=None, border=True)\nui.next_column()\n</code></pre>"},{"location":"api/#tables","title":"Tables","text":"<pre><code>ui.begin_table(id: str, columns: int, flags=0) -&gt; bool\nui.end_table()\nui.table_next_row(flags=0, min_row_height=0.0)\nui.table_next_column() -&gt; bool\nui.table_set_column_index(index: int) -&gt; bool\nui.table_setup_column(label: str, flags=0, init_width=0.0)\nui.table_headers_row()\n</code></pre>"},{"location":"api/#menus","title":"Menus","text":"<pre><code>ui.begin_main_menu_bar() -&gt; bool\nui.end_main_menu_bar()\nui.begin_menu_bar() -&gt; bool\nui.end_menu_bar()\nui.begin_menu(label: str, enabled=True) -&gt; bool\nui.end_menu()\nui.menu_item(label: str, shortcut=\"\", selected=False, enabled=True) -&gt; bool\n</code></pre>"},{"location":"api/#popups","title":"Popups","text":"<pre><code>ui.open_popup(id: str)\nui.begin_popup(id: str) -&gt; bool\nui.begin_popup_modal(name: str, flags=0) -&gt; bool\nui.end_popup()\nui.close_current_popup()\n</code></pre>"},{"location":"api/#tooltips","title":"Tooltips","text":"<pre><code>ui.begin_tooltip()\nui.end_tooltip()\nui.set_tooltip(text: str)\n</code></pre>"},{"location":"api/#id-stack","title":"ID Stack","text":"<pre><code>ui.push_id_str(id: str)\nui.push_id_int(id: int)\nui.pop_id()\nui.get_id(id: str) -&gt; int\n</code></pre>"},{"location":"api/#style","title":"Style","text":"<pre><code>ui.push_style_color(idx: int, r, g, b, a)\nui.pop_style_color(count=1)\nui.push_style_var_float(idx: int, value: float)\nui.push_style_var_vec2(idx: int, x: float, y: float)\nui.pop_style_var(count=1)\nui.style_colors_dark()\nui.style_colors_light()\nui.style_colors_classic()\n</code></pre>"},{"location":"api/#cursor-window-info","title":"Cursor &amp; Window Info","text":"<pre><code>ui.get_cursor_pos() -&gt; tuple[float, float]\nui.set_cursor_pos(x, y)\nui.get_content_region_avail() -&gt; tuple[float, float]\nui.get_window_size() -&gt; tuple[float, float]\nui.get_window_pos() -&gt; tuple[float, float]\nui.set_next_window_pos(x, y, cond=0)\nui.set_next_window_size(width, height, cond=0)\n</code></pre>"},{"location":"api/#docking","title":"Docking","text":"<pre><code>ui.dock_space_over_viewport(flags=0) -&gt; int\nui.dock_space(id: int, sx=0.0, sy=0.0, flags=0) -&gt; int\nui.set_next_window_dock_id(dock_id: int, cond=0)\nui.dock_builder_add_node(node_id=0, flags=0) -&gt; int\nui.dock_builder_remove_node(node_id: int)\nui.dock_builder_set_node_size(node_id, width, height)\nui.dock_builder_set_node_pos(node_id, x, y)\nui.dock_builder_split_node(node_id, split_dir, ratio) -&gt; tuple[int, int]\nui.dock_builder_dock_window(window_name: str, node_id: int)\nui.dock_builder_finish(node_id: int)\nui.dock_builder_get_node(node_id: int) -&gt; int\n</code></pre>"},{"location":"api/#drawing","title":"Drawing","text":"<pre><code>ui.draw_line(x1, y1, x2, y2, col=0xFFFFFFFF, thickness=1.0)\nui.draw_rect(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_rect_filled(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_circle(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_circle_filled(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_text(x, y, col: int, text: str)\nui.bg_draw_image(texture_id, x1, y1, x2, y2)\n</code></pre>"},{"location":"api/#input-state","title":"Input State","text":"<pre><code>ui.is_item_hovered() -&gt; bool\nui.is_item_active() -&gt; bool\nui.is_item_clicked() -&gt; bool\nui.is_item_focused() -&gt; bool\nui.is_item_edited() -&gt; bool\nui.is_item_deactivated_after_edit() -&gt; bool\nui.get_mouse_pos() -&gt; tuple[float, float]\nui.is_mouse_clicked(button: int) -&gt; bool\nui.is_mouse_double_clicked(button: int) -&gt; bool\nui.is_mouse_dragging(button: int, lock_threshold=-1.0) -&gt; bool\nui.get_mouse_drag_delta(button=0, lock_threshold=-1.0) -&gt; tuple[float, float]\nui.is_key_pressed(key: int) -&gt; bool\nui.is_key_down(key: int) -&gt; bool\n</code></pre>"},{"location":"api/#utility","title":"Utility","text":"<pre><code>ui.get_io_framerate() -&gt; float\nui.get_io_delta_time() -&gt; float\nui.get_time() -&gt; float\nui.get_frame_count() -&gt; int\nui.get_display_size() -&gt; tuple[float, float]\nui.col32(r: int, g: int, b: int, a: int = 255) -&gt; int\nui.show_demo_window()\nui.show_metrics_window()\n</code></pre>"},{"location":"api/#internal-helpers","title":"Internal Helpers","text":""},{"location":"api/#vultorch_normalize_tensor","title":"<code>vultorch._normalize_tensor()</code>","text":"<pre><code>def _normalize_tensor(tensor) -&gt; tuple[Tensor, int, int, int]\n</code></pre> <p>Normalize tensor dtype and shape for display. Returns <code>(tensor, height, width, channels)</code>.</p> <ul> <li>Converts <code>uint8</code> \u2192 <code>float32</code> (\u00f7 255), <code>float16</code> \u2192 <code>float32</code>.</li> <li>Accepts 2D <code>(H, W)</code> and 3D <code>(H, W, C)</code> with C \u2208 {1, 3, 4}.</li> <li>Raises <code>ValueError</code> for unsupported dtype, shape, or channel count.</li> </ul>"},{"location":"zh/","title":"Vultorch \u6559\u7a0b","text":"<p>\u9010\u6b65\u5b66\u4e60 Vultorch\uff0c\u6bcf\u4e2a\u7ae0\u8282\u5bf9\u5e94 <code>examples/</code> \u76ee\u5f55\u4e2d\u7684\u4e00\u4e2a\u53ef\u8fd0\u884c\u811a\u672c\u3002</p> \u7ae0\u8282 \u4e3b\u9898 \u6838\u5fc3\u6982\u5ff5 01 \u2014 Hello Tensor \u6700\u5c0f\u793a\u4f8b View, Panel, Canvas, bind, run 02 \u2014 \u591a\u9762\u677f \u591a\u9762\u677f\u4e0e\u591a\u753b\u5e03 \u5e03\u5c40, side, \u591a\u753b\u5e03 03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5 \u62df\u5408 GT \u56fe\u50cf \u81ea\u5b9a\u4e49\u505c\u9760\u5e03\u5c40, create_tensor, \u9010\u50cf\u7d20\u4f18\u5316 04 \u2014 \u5eb7\u5a01\u751f\u547d\u6e38\u620f GPU \u5143\u80de\u81ea\u52a8\u673a create_tensor \u7528\u4e8e\u6a21\u62df, filter=\"nearest\", \u4fa7\u8fb9\u680f, \u6309\u94ae, \u989c\u8272\u9009\u62e9\u5668 05 \u2014 \u56fe\u7247\u67e5\u770b\u5668 \u52a0\u8f7d\u3001\u53d8\u6362\u3001\u4fdd\u5b58\u56fe\u7247 imread, imwrite, Canvas.save, combo, input_text, \u6ee4\u6ce2\u5207\u6362 <p>\u66f4\u591a\u7ae0\u8282\u5373\u5c06\u63a8\u51fa\u3002</p>"},{"location":"zh/01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/01_hello_tensor.py</code></p> <p>\u4f60\u662f\u5426\u53d7\u591f\u4e86\u6bcf\u4e2a\u4ed3\u5e93\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u5b9e\u73b0\u7684\u53ef\u89c6\u5316\u65b9\u6848 \u2014\u2014 \u8fd9\u4e2a\u7528 matplotlib \u624b\u52a8\u5237\u65b0\uff0c \u90a3\u4e2a\u7528 <code>cv2.imshow</code> \u7136\u540e <code>waitKey(1)</code>\uff0c\u8fd8\u6709\u7684\u76f4\u63a5\u5b58 PNG \u8ba9\u4f60\u5f00\u4e2a\u56fe\u7247\u67e5\u770b\u5668\uff1f</p> <p>Vultorch \u53ea\u9700 \u56db\u884c\u4ee3\u7801 \u5c31\u628a CUDA tensor \u642c\u5230\u5c4f\u5e55\u4e0a\u3002\u4e0d\u5b58\u56fe\u3001\u4e0d\u8fc7 CPU\u3001 \u4e0d\u9700\u8981 <code>plt.pause(0.001)</code> \u8fd9\u79cd\u9ed1\u9b54\u6cd5\u3002</p>"},{"location":"zh/01_hello_tensor/#_1","title":"\u4f60\u9700\u8981\u8bb0\u4f4f\u7684\u4e1c\u897f","text":"<p>\u4e00\u5171\u5c31\u56db\u4e2a\u5bf9\u8c61\uff1a</p> \u5bf9\u8c61 \u662f\u4ec0\u4e48 \u5199\u6cd5 View \u64cd\u4f5c\u7cfb\u7edf\u7a97\u53e3 <code>vultorch.View(\"title\", w, h)</code> Panel View \u91cc\u53ef\u505c\u9760\u7684\u5b50\u7a97\u53e3 <code>view.panel(\"name\")</code> Canvas Panel \u91cc\u7684 GPU \u56fe\u50cf\u69fd\u4f4d <code>panel.canvas(\"name\")</code> bind() \u628a tensor \u8fde\u5230 Canvas \u4e0a <code>canvas.bind(t)</code> <p>\u94fe\u8d77\u6765\uff0c\u8c03 <code>run()</code>\uff0c\u6536\u5de5\u3002</p>"},{"location":"zh/01_hello_tensor/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\n# \u4e00\u5f20 256\u00d7256 \u7684 RGB \u6e10\u53d8 \u2014 \u4efb\u4f55 (H,W,C) float32 CUDA tensor \u90fd\u884c\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # \u963b\u585e\uff0c\u76f4\u5230\u4f60\u5173\u95ed\u7a97\u53e3\n</code></pre> <p>\u6ca1\u4e86\u3002\u4e0d\u9700\u8981\u624b\u5199\u4e8b\u4ef6\u5faa\u73af\uff0c\u4e0d\u9700\u8981 <code>begin_frame()</code> / <code>end_frame()</code>\u3002</p>"},{"location":"zh/01_hello_tensor/#_3","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 \u6211\u4eec\u5728 CUDA \u4e0a\u9020\u4e86\u4e00\u5f20 RGB \u6e10\u53d8\u3002Vultorch \u652f\u6301    <code>(H,W)</code> / <code>(H,W,1)</code> / <code>(H,W,3)</code> / <code>(H,W,4)</code>\uff0c    float32 / float16 / uint8 \u90fd\u884c\uff0cRGBA \u6269\u5c55\u5b83\u81ea\u5df1\u641e\u5b9a\u3002</p> </li> <li> <p>\u5bf9\u8c61\u6811 \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>\u3002    \u753b\u5e03\u9ed8\u8ba4\u94fa\u6ee1\u6574\u4e2a\u9762\u677f\uff08<code>fit=True</code>\uff09\u3002</p> </li> <li> <p>\u8fd0\u884c \u2014 <code>view.run()</code> \u8fdb\u5165\u963b\u585e\u4e8b\u4ef6\u5faa\u73af\uff0c\u6bcf\u5e27\u91cd\u65b0\u4e0a\u4f20 tensor \u5e76\u6e32\u67d3\u3002    \u5173\u7a97\u53e3\u5c31\u9000\u51fa\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u56db\u884c\u521d\u59cb\u5316\u53ef\u4ee5\u538b\u6210\u4e00\u884c\uff0c\u5982\u679c\u4f60\u559c\u6b22\u70ab\u6280\u7684\u8bdd\uff1a <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p>"},{"location":"zh/02_multi_panel/","title":"02 \u2014 \u591a\u9762\u677f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/02_imgui_controls.py</code></p> <p>\u4e00\u4e2a\u9762\u677f\u633a\u597d\u7684\u3002\u4f46\u5b9e\u9645\u505a\u7814\u7a76\u7684\u65f6\u5019\uff0c\u4f60\u60f3\u540c\u65f6\u770b loss map\u3001\u68af\u5ea6\u573a\u3001 \u6a21\u578b\u8f93\u51fa \u2014\u2014 \u800c\u4e14\u6700\u597d\u4e0d\u7528\u81ea\u5df1\u5199\u4efb\u4f55\u5e03\u5c40\u4ee3\u7801\u3002</p> <p>\u597d\u6d88\u606f\uff1aVultorch \u7684\u9762\u677f\u662f\u53ef\u505c\u9760\u7684\u3002\u4f60\u53ea\u7ba1\u521b\u5efa\uff0c\u5b83\u4eec\u81ea\u5df1\u6392\u597d\u3002 \u7528\u6237\u8fd0\u884c\u65f6\u8fd8\u80fd\u968f\u4fbf\u62d6\u3001\u968f\u4fbf\u62c9\u3002</p> <p>\u672c\u7ae0\u6f14\u793a\u4e24\u79cd\u6a21\u5f0f\uff1a</p> <ul> <li>\u6bcf\u4e2a\u9762\u677f\u4e00\u4e2a\u753b\u5e03 \u2014 \u4e09\u4e2a\u9762\u677f\u5782\u76f4\u5806\u53e0\uff0c\u5404\u81ea\u4e00\u5f20\u56fe\u3002</li> <li>\u4e00\u4e2a\u9762\u677f\u591a\u4e2a\u753b\u5e03 \u2014 \u4e00\u4e2a\u9762\u677f\u91cc\u585e\u4e09\u4e2a\u753b\u5e03\uff0c\u81ea\u52a8\u5747\u5206\u3002</li> </ul>"},{"location":"zh/02_multi_panel/#_1","title":"\u5e03\u5c40","text":"<p>\u7a97\u53e3\u957f\u8fd9\u6837\uff1a</p> \u5de6\u4fa7\uff08\u4e3b\u533a\u57df\uff09 \u53f3\u4fa7\uff08<code>side=\"right\"</code>\uff09 Red \u9762\u677f \u2014 <code>red_img</code> Combined \u9762\u677f Green \u9762\u677f \u2014 <code>green_img</code> <code>c_red</code> \u753b\u5e03 Blue \u9762\u677f \u2014 <code>blue_img</code> <code>c_green</code> \u753b\u5e03 <code>c_blue</code> \u753b\u5e03 <p>\u5de6\u8fb9\uff1a3 \u4e2a\u72ec\u7acb\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\u3002\u53f3\u8fb9\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\u5171\u4eab\u7a7a\u95f4\u3002</p>"},{"location":"zh/02_multi_panel/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# \u4e09\u5f20\u4e0d\u540c\u7684 tensor\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# \u5de6\u4fa7\uff1a3 \u4e2a\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# \u53f3\u4fa7\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\uff08\u81ea\u52a8\u5782\u76f4\u5747\u5206\uff09\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"zh/02_multi_panel/#_3","title":"\u8981\u70b9","text":"<ol> <li> <p>\u81ea\u52a8\u5e03\u5c40 \u2014 \u4e0d\u5199 <code>side=</code> \u7684\u9762\u677f\u81ea\u52a8\u5782\u76f4\u5806\u53e0\u3002    \u52a0 <code>side=\"right\"</code> + <code>width=0.5</code> \u5c31\u505c\u9760\u5230\u53f3\u8fb9\u5360\u4e00\u534a\u3002</p> </li> <li> <p>\u591a\u753b\u5e03 \u2014 \u5bf9\u540c\u4e00\u4e2a\u9762\u677f\u591a\u6b21\u8c03\u7528 <code>panel.canvas()</code>\u3002    \u591a\u4e2a <code>fit=True</code>\uff08\u9ed8\u8ba4\uff09\u7684\u753b\u5e03\u4f1a\u81ea\u52a8\u5747\u5206\u5782\u76f4\u7a7a\u95f4\uff0c\u4e0d\u7528\u624b\u7b97\u9ad8\u5ea6\u3002</p> </li> <li> <p>\u4f9d\u7136\u4e0d\u9700\u8981\u56de\u8c03 \u2014 \u9759\u6001\u6570\u636e\u53ea\u9700 <code>bind()</code> + <code>run()</code>\u3002    \u52a8\u6001\u66f4\u65b0\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8bb2\u3002</p> </li> <li> <p>\u968f\u4fbf\u62d6 \u2014 \u6240\u6709\u9762\u677f\u90fd\u652f\u6301\u505c\u9760\u3002\u7528\u6237\u53ef\u4ee5\u62d6\u6807\u9898\u680f\u91cd\u6392\u3001    \u62c9\u51fa\u6765\u53d8\u6d6e\u52a8\u7a97\u53e3\u3001\u6216\u8005\u62d6\u8fb9\u6846\u8c03\u5927\u5c0f\u3002</p> </li> </ol> <p>\u8bf4\u660e</p> <p>\u540c\u4e00\u4e2a tensor \u53ef\u4ee5\u540c\u65f6\u7ed1\u5b9a\u591a\u4e2a\u753b\u5e03 \u2014\u2014 <code>red</code> \u540c\u65f6\u51fa\u73b0\u5728\u5de6\u8fb9\u7684 Red \u9762\u677f\u548c\u53f3\u8fb9\u7684 Combined \u9762\u677f\u91cc\u3002</p>"},{"location":"zh/03_training_test/","title":"03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/03_training_test.py</code></p> <p>\u4f60\u6709\u6ca1\u6709\u7ecf\u5386\u8fc7\u8fd9\u79cd\u4e8b\uff1a\u8bad\u7ec3\u8dd1\u4e86\u534a\u5c0f\u65f6\uff0c\u7ec8\u7aef\u91cc loss \u6570\u5b57\u54d7\u54d7\u5f80\u4e0b\u6389\uff0c \u770b\u8d77\u6765\u633a\u6b63\u5e38 \u2014\u2014 \u7ed3\u679c\u4e00\u51fa\u56fe\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5168\u662f\u7070\u7684\uff1f</p> <p>\u76ef\u7740\u547d\u4ee4\u884c\u91cc\u7684\u6570\u5b57\u731c\u6a21\u578b\u72b6\u6001\uff0c\u8ddf\u770b\u80a1\u7968 K \u7ebf\u731c\u660e\u5929\u6da8\u8dcc\u4e00\u6837\u4e0d\u9760\u8c31\u3002 \u672c\u7ae0\u76f4\u63a5\u628a GT \u548c\u9884\u6d4b\u5e76\u6392\u653e\u5728\u5c4f\u5e55\u4e0a\u3002\u7f51\u7edc\u5230\u5e95\u6709\u6ca1\u6709\u5728\u5b66\uff0c\u4e00\u773c\u5c31\u77e5\u9053\u3002</p>"},{"location":"zh/03_training_test/#_1","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u5f88\u5c0f\u7684 MLP\uff082 \u2192 64 \u2192 64 \u2192 3\uff09\u53bb\u5b9e\u65f6\u62df\u5408\u4e00\u5f20 256\u00d7256 \u7684 PyTorch logo\u3002 \u7a97\u53e3\u5206\u4e09\u5757\uff1a</p> \u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 GT \u9762\u677f \u2014 \u76ee\u6807\u56fe\uff08\u4f60\u8981\u62df\u5408\u7684\u4e1c\u897f\uff09 \u53f3\u4fa7 Prediction \u9762\u677f \u2014 \u7f51\u7edc\u5b9e\u65f6\u8f93\u51fa\uff0c\u6bcf\u5e27\u5237\u65b0 \u4e0b\u65b9 Info \u9762\u677f \u2014 FPS\u3001loss\u3001\u8fed\u4ee3\u6b21\u6570\u3001\u8fdb\u5ea6\u6761\uff0c\u8fd8\u80fd\u62d6\u6ed1\u6761 <p>\u6240\u6709\u6570\u503c\u90fd\u5728\u753b\u9762\u91cc\uff0c\u4e0d\u7528\u518d\u5728\u7ec8\u7aef\u91cc\u7ffb\u6765\u7ffb\u53bb\u627e\u3002</p>"},{"location":"zh/03_training_test/#_2","title":"\u65b0\u670b\u53cb","text":"<p>\u524d\u4e24\u7ae0\u90fd\u662f\u9759\u6001\u6570\u636e \u2014\u2014 <code>bind()</code> + <code>run()</code>\uff0c\u5b8c\u4e8b\u3002 \u8fd9\u6b21\u6211\u4eec\u8981\u5f15\u5165\u4e09\u4e2a\u65b0\u4e1c\u897f\uff1a</p> \u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u5199\u6cd5 on_frame \u6bcf\u5e27\u56de\u8c03\uff0c\u8bad\u7ec3 + \u66f4\u65b0\u5728\u8fd9\u91cc\u641e <code>@view.on_frame</code> Panel.on_frame \u9762\u677f\u56de\u8c03\uff0c\u63a7\u4ef6\u653e\u8fd9\u91cc <code>@info_panel.on_frame</code> create_tensor \u521b\u5efa GPU \u5171\u4eab\u663e\u5b58 tensor <code>vultorch.create_tensor(H, W, ...)</code> vultorch.imread \u52a0\u8f7d\u56fe\u7247\uff0c\u96f6\u4f9d\u8d56 <code>vultorch.imread(path, channels=3)</code> side=\"bottom\" \u628a\u9762\u677f\u505c\u9760\u5230\u7a97\u53e3\u5e95\u90e8 <code>view.panel(\"Info\", side=\"bottom\")</code> <p>View \u56de\u8c03\u91cc\u5199 PyTorch \u4ee3\u7801\uff0cPanel \u56de\u8c03\u91cc\u753b\u63a7\u4ef6\uff0c Vultorch \u6bcf\u5e27\u5e2e\u4f60\u628a tensor \u642c\u4e0a\u5c4f\u5e55\u3002</p>"},{"location":"zh/03_training_test/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# -- \u89c6\u56fe + \u9762\u677f\uff08\u9ad8\u5c42\u58f0\u660e\u5f0f API\uff09------------------------------------\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ninfo_panel = view.panel(\"Info\", side=\"bottom\", width=0.28)\ngt_panel = view.panel(\"GT\", side=\"left\", width=0.5)\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# \u9884\u6d4b\u7528 4 \u901a\u9053 \u2014 GPU \u96f6\u62f7\u8d1d\u663e\u793a\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n}\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n\n@info_panel.on_frame\ndef draw_info():\n    info_panel.text(f\"FPS: {view.fps:.1f}\")\n    info_panel.text(f\"Iteration: {state['iter']}\")\n    info_panel.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    info_panel.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = info_panel.slider_int(\n        \"Steps / Frame\", 1, 32, default=6\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    info_panel.progress(progress,\n                        overlay=f\"Training progress {progress * 100:.1f}%\")\n    info_panel.text_wrapped(\n        \"\u5de6\u8fb9\u662f GT\uff0c\u53f3\u8fb9\u662f\u9884\u6d4b\u3002\u60f3\u66f4\u5feb\u6536\u655b\u5c31\u63d0\u9ad8 Steps / Frame\u3002\"\n    )\n\n\nview.run()\n</code></pre> <p>\u641e\u5b9a\u3002\u8dd1\u8d77\u6765\u4e4b\u540e\u4f60\u4f1a\u770b\u5230\u53f3\u8fb9\u90a3\u5768\u7070\u8272\u5728\u51e0\u79d2\u5185\u53d8\u6210 PyTorch logo\u3002</p>"},{"location":"zh/03_training_test/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 <code>vultorch.imread</code> \u76f4\u63a5\u628a\u56fe\u7247\u8bfb\u6210 float32 CUDA tensor\uff08\u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 numpy\uff09\u3002    \u5750\u6807\u7528 <code>meshgrid</code> \u5c55\u6210 <code>(H*W, 2)</code>\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684 <code>(x, y)</code> \u5f52\u4e00\u5316\u5230 <code>[-1, 1]</code>\u3002</p> </li> <li> <p>\u6a21\u578b \u2014 \u4e24\u5c42 64 \u5bbd\u7684 MLP\uff0c\u8f93\u5165 <code>(x, y)</code>\uff0c\u8f93\u51fa <code>(r, g, b)</code>\u3002    \u8fd9\u4e2a\u7f51\u7edc\u5c0f\u5230\u53ef\u4ee5\u8dd1\u5728\u56de\u8c03\u91cc\u4e0d\u6389\u5e27\u3002</p> </li> <li> <p>\u5e03\u5c40 \u2014 <code>side=\"bottom\"</code> \u628a Info \u505c\u9760\u5230\u5e95\u90e8 28%\u3002    <code>side=\"left\"</code> \u628a GT \u653e\u5230\u5269\u4f59\u7a7a\u95f4\u7684\u5de6\u534a\u8fb9\u3002    Prediction \u81ea\u52a8\u586b\u6ee1\u5269\u4e0b\u7684\u533a\u57df\u3002\u4e0d\u9700\u8981\u624b\u5199\u4efb\u4f55 docking \u4ee3\u7801\u3002</p> </li> <li> <p>\u4e24\u4e2a\u56de\u8c03 \u2014 <code>@view.on_frame</code> \u8dd1\u8bad\u7ec3\u3002    <code>@info_panel.on_frame</code> \u5728 Info \u9762\u677f\u91cc\u753b\u63a7\u4ef6\uff08\u6587\u5b57\u3001\u6ed1\u6761\u3001\u8fdb\u5ea6\u6761\uff09\u3002    Vultorch \u5e2e\u4f60\u7ba1 ImGui \u7684 begin / end\uff0c\u4e0d\u7528\u81ea\u5df1\u64cd\u5fc3\u3002</p> </li> </ol>"},{"location":"zh/03_training_test/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 \u56de\u8c03\u91cc\u53ef\u4ee5\u8dd1\u4efb\u610f PyTorch \u4ee3\u7801\u3002    \u6bcf\u5e27\u7ed3\u675f\u65f6 Vultorch \u81ea\u52a8\u628a\u7ed1\u5b9a\u7684 tensor \u642c\u5230\u5c4f\u5e55\uff0c\u4e0d\u7528\u4f60\u7ba1\u3002</p> </li> <li> <p><code>create_tensor</code> \u2014 \u8ddf\u666e\u901a <code>torch.zeros</code> \u4e00\u6837\u7528\uff0c    \u4f46\u5e95\u5c42\u662f Vulkan/CUDA \u5171\u4eab\u663e\u5b58\uff0c\u663e\u793a\u7684\u65f6\u5019\u96f6\u62f7\u8d1d\u3002</p> </li> <li> <p>\u58f0\u660e\u5f0f\u5e03\u5c40 \u2014 <code>side=\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code>    \u5207\u7a97\u53e3\uff0c\u4e0d\u9700\u8981\u624b\u52a8\u5199 dock_builder \u4ee3\u7801\u3002</p> </li> <li> <p>\u9762\u677f\u63a7\u4ef6 \u2014 <code>@panel.on_frame</code> \u5728\u9762\u677f\u7a97\u53e3\u5185\u90e8\u8fd0\u884c\u3002    \u7528 <code>panel.text()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.progress()</code> \u4ee3\u66ff\u539f\u59cb <code>ui.*</code> \u8c03\u7528\u3002</p> </li> <li> <p>\u4e0d\u5237\u7ec8\u7aef \u2014 \u6240\u6709\u72b6\u6001\u4fe1\u606f\u90fd\u5728 Info \u9762\u677f\u91cc\uff0c    \u4f60\u7684\u7ec8\u7aef\u53ef\u4ee5\u7559\u7740\u770b warning \u548c traceback\uff0c\u5e72\u51c0\u591a\u4e86\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p><code>Steps / Frame</code> \u6ed1\u6761\u62c9\u5230 32 \u6536\u655b\u98de\u5feb\u3002 \u4f46\u4e5f\u522b\u592a\u8d2a \u2014\u2014 \u62c9\u592a\u9ad8\u5e27\u7387\u4f1a\u6389\u4e0b\u6765\uff0c\u56e0\u4e3a\u6bcf\u5e27\u7684\u8bad\u7ec3\u65f6\u95f4\u53d8\u957f\u4e86\u3002</p> <p>\u8bf4\u660e</p> <p><code>create_tensor</code> \u53ea\u5728\u521d\u59cb\u5316\u65f6\u8c03\u7528\u4e00\u6b21\uff0c\u4e0d\u5728\u5e27\u5faa\u73af\u91cc\u3002 \u4e4b\u540e\u6bcf\u5e27\u53ea\u9700\u8981\u5f80\u8fd9\u4e2a tensor \u91cc\u5199\u6570\u636e\uff0c\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002</p>"},{"location":"zh/04_conway/","title":"04 \u2014 \u5eb7\u5a01\u751f\u547d\u6e38\u620f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/04_conway.py</code></p> <p>Loss \u66f2\u7ebf\uff0c\u8bad\u7ec3\u53ef\u89c6\u5316 \u2014\u2014 \u524d\u9762\u592a\u6b63\u7ecf\u4e86\u3002 \u6b47\u4e00\u6b47\uff0c\u6765\u641e\u4e2a\u597d\u73a9\u7684\uff1a\u5eb7\u5a01\u751f\u547d\u6e38\u620f\uff0c\u5168\u8dd1\u5728 GPU \u4e0a\uff0c\u96f6\u62f7\u8d1d\u663e\u793a\uff0c \u53f3\u8fb9\u4e00\u6392\u6309\u94ae\u548c\u6ed1\u6761\u8ba9\u4f60\u5f53\u4e0a\u5e1d\u3002</p> <p>\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e00\u7ae0\u8981\u8bf4\u660e\u4e00\u4ef6\u4e8b\uff1a<code>create_tensor</code> \u4e0d\u53ea\u662f\u7ed9\u8bad\u7ec3\u7528\u7684\u3002 \u4efb\u4f55 GPU \u8ba1\u7b97\u90fd\u53ef\u4ee5\u901a\u8fc7 Vultorch \u7684\u5171\u4eab\u663e\u5b58\u663e\u793a\u51fa\u6765 \u2014\u2014 \u6a21\u62df\u3001 \u7a0b\u5e8f\u5316\u751f\u6210\u3001\u7269\u7406\u4eff\u771f\uff0c\u53ea\u8981\u662f CUDA \u4e0a\u8dd1\u7684\uff0c\u90fd\u884c\u3002</p>"},{"location":"zh/04_conway/#_1","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a 256\u00d7256 \u7684\u5143\u80de\u81ea\u52a8\u673a\uff0c\u5e26\u63a7\u5236\u9762\u677f\uff1a</p> \u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 Controls \u2014 \u64ad\u653e/\u6682\u505c\u3001\u5355\u6b65\u3001\u901f\u5ea6\u6ed1\u6761\u3001\u7ecf\u5178\u56fe\u6848\u6309\u94ae\u3001\u989c\u8272\u9009\u62e9\u5668 \u53f3\u4fa7 Grid \u2014 \u6a21\u62df\u753b\u9762\uff0c\u50cf\u7d20\u7ea7\u7cbe\u786e\uff08<code>filter=\"nearest\"</code>\uff09 <p>\u6574\u4e2a\u6a21\u62df\u8dd1\u5728 GPU \u4e0a\u3002\u663e\u793a\u7528\u7684 tensor \u901a\u8fc7 <code>create_tensor</code> \u5206\u914d\uff0c \u96f6\u62f7\u8d1d \u2014\u2014 \u7f51\u683c\u6570\u636e\u4e0d\u7ecf\u8fc7 CPU\u3002</p>"},{"location":"zh/04_conway/#_2","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u4e3a\u4ec0\u4e48\u91cd\u8981 <code>filter=\"nearest\"</code> \u50cf\u7d20\u7ea7\u7cbe\u786e\u663e\u793a\uff0c\u65e0\u63d2\u503c \u683c\u5b50\u8fb9\u754c\u4e0d\u4f1a\u7cca\u6389 <code>side=\"left\"</code> \u4fa7\u8fb9\u680f \u628a\u9762\u677f\u505c\u9760\u5230\u5de6\u8fb9 \u5e72\u51c0\u7684\u63a7\u5236\u9762\u677f\u5e03\u5c40 <code>Panel.on_frame</code> \u9762\u677f\u7ea7\u63a7\u4ef6\u56de\u8c03 \u63a7\u4ef6\u81ea\u52a8\u5728\u9762\u677f\u7a97\u53e3\u5185\u7ed8\u5236 <code>panel.button</code>\u3001<code>panel.row()</code> \u540c\u4e00\u884c\u653e\u591a\u4e2a\u6309\u94ae \u7d27\u51d1\u5e03\u5c40\uff0c\u4e0d\u9700\u8981\u539f\u59cb <code>ui.*</code> \u8c03\u7528 <code>panel.color_picker</code> \u989c\u8272\u9009\u62e9\u5668 \u81ea\u5b9a\u4e49\u5b58\u6d3b/\u6b7b\u4ea1\u989c\u8272 \u5faa\u73af padding + conv2d GPU \u5e76\u884c\u6570\u90bb\u5c45 \u6574\u4e2a\u6a21\u62df\u5c31\u662f\u4e00\u6b21\u5377\u79ef"},{"location":"zh/04_conway/#_3","title":"\u6a21\u62df\u7684\u6838\u5fc3\u6280\u5de7","text":"<p>\u6570\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u91cc\u7684\u90bb\u5c45\u4e2a\u6570\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u6b21 2D \u5377\u79ef\uff0c\u7528\u4e00\u4e2a 3\u00d73 \u7684\u5377\u79ef\u6838\uff08\u4e2d\u5fc3\u4e3a 0\uff09\uff1a</p> <pre><code>1 1 1\n1 0 1\n1 1 1\n</code></pre> <p>PyTorch \u7684 <code>F.conv2d</code> \u4e00\u4e2a GPU kernel \u8c03\u7528\u5c31\u641e\u5b9a \u2014\u2014 \u4e0d\u9700\u8981\u5faa\u73af\uff0c \u4e0d\u9700\u8981\u9010\u50cf\u7d20\u7684\u903b\u8f91\u3002circular padding \u8ba9\u8fb9\u7f18\u73af\u7ed5\uff0c\u98de\u884c\u5668\u98de\u51fa\u53f3\u8fb9 \u4f1a\u4ece\u5de6\u8fb9\u5192\u51fa\u6765\u3002</p> <pre><code>kernel = torch.tensor([[1, 1, 1],\n                        [1, 0, 1],\n                        [1, 1, 1]], dtype=torch.float32, device=device)\npadded = F.pad(inp, (1, 1, 1, 1), mode='circular')\nneighbours = F.conv2d(padded, kernel.reshape(1, 1, 3, 3)).squeeze()\n</code></pre> <p>\u7136\u540e\u89c4\u5219\u5c31\u662f\u4e24\u4e2a\u5e03\u5c14 mask\uff1a</p> <pre><code>survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\nbirth   = (grid == 0) &amp; (neighbours == 3)\ngrid[:] = (survive | birth).float()\n</code></pre>"},{"location":"zh/04_conway/#_4","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 \u7f51\u683c\u53c2\u6570 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGRID_H, GRID_W = 256, 256\n\n# \u2500\u2500 \u89c6\u56fe + \u9762\u677f \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"04 - Conway's Game of Life\", 1024, 768)\ngrid_panel = view.panel(\"Grid\")\nctrl_panel = view.panel(\"Controls\", side=\"left\", width=0.22)\n\n# \u2500\u2500 \u663e\u793a tensor\uff08RGBA\uff0c\u96f6\u62f7\u8d1d\uff09\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndisplay = vultorch.create_tensor(GRID_H, GRID_W, channels=4,\n                                 device=device, name=\"grid\",\n                                 window=view.window)\ncanvas = grid_panel.canvas(\"grid\", filter=\"nearest\")\ncanvas.bind(display)\n\n# \u2500\u2500 \u6a21\u62df\u72b6\u6001 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ngrid = torch.zeros(GRID_H, GRID_W, dtype=torch.float32, device=device)\n\nstate = {\n    \"running\": False,\n    \"generation\": 0,\n    \"speed\": 1,\n    \"prob\": 0.3,\n    \"alive_color\": (0.0, 1.0, 0.4),\n    \"dead_color\": (0.05, 0.05, 0.08),\n}\n\n\ndef randomize():\n    grid[:] = (torch.rand(GRID_H, GRID_W, device=device) &lt; state[\"prob\"]).float()\n    state[\"generation\"] = 0\n\ndef clear():\n    grid.zero_()\n    state[\"generation\"] = 0\n\ndef step_simulation():\n    kernel = torch.tensor([[1, 1, 1],\n                            [1, 0, 1],\n                            [1, 1, 1]], dtype=torch.float32, device=device)\n    inp = grid.unsqueeze(0).unsqueeze(0)\n    k = kernel.unsqueeze(0).unsqueeze(0)\n    padded = torch.nn.functional.pad(inp, (1, 1, 1, 1), mode='circular')\n    neighbours = torch.nn.functional.conv2d(padded, k).squeeze()\n\n    survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\n    birth = (grid == 0) &amp; (neighbours == 3)\n    grid[:] = (survive | birth).float()\n    state[\"generation\"] += 1\n\ndef grid_to_display():\n    alive_r, alive_g, alive_b = state[\"alive_color\"]\n    dead_r, dead_g, dead_b = state[\"dead_color\"]\n    display[:, :, 0] = dead_r + (alive_r - dead_r) * grid\n    display[:, :, 1] = dead_g + (alive_g - dead_g) * grid\n    display[:, :, 2] = dead_b + (alive_b - dead_b) * grid\n    display[:, :, 3] = 1.0\n\nrandomize()\n\n\n@view.on_frame\ndef update():\n    if state[\"running\"]:\n        for _ in range(state[\"speed\"]):\n            step_simulation()\n    grid_to_display()\n\n\n@ctrl_panel.on_frame\ndef draw_controls():\n    ctrl_panel.text(f\"Generation: {state['generation']}\")\n    ctrl_panel.text(f\"Alive cells: {int(grid.sum().item())}\")\n    ctrl_panel.text(f\"FPS: {view.fps:.1f}\")\n    ctrl_panel.separator()\n\n    with ctrl_panel.row():\n        label = \"Pause\" if state[\"running\"] else \"Play\"\n        if ctrl_panel.button(label, width=80):\n            state[\"running\"] = not state[\"running\"]\n        if ctrl_panel.button(\"Step\", width=80):\n            step_simulation()\n\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Randomize\", width=80):\n            randomize()\n        if ctrl_panel.button(\"Clear\", width=80):\n            clear()\n\n    ctrl_panel.separator()\n    state[\"speed\"] = ctrl_panel.slider_int(\"Speed\", 1, 20, default=1)\n    state[\"prob\"] = ctrl_panel.slider(\"Cell Probability\", 0.05, 0.8,\n                                       default=0.3)\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Colors\")\n    state[\"alive_color\"] = ctrl_panel.color_picker(\n        \"Alive\", default=(0.0, 1.0, 0.4))\n    state[\"dead_color\"] = ctrl_panel.color_picker(\n        \"Dead\", default=(0.05, 0.05, 0.08))\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Patterns\")\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Glider\", width=80):\n            clear()\n            grid[1, 2] = 1; grid[2, 3] = 1\n            grid[3, 1] = 1; grid[3, 2] = 1; grid[3, 3] = 1\n        if ctrl_panel.button(\"Pulsar\", width=80):\n            clear()\n            # ... \u653e\u7f6e Pulsar \u56fe\u6848 ...\n        if ctrl_panel.button(\"Gosper Gun\", width=100):\n            clear()\n            # ... \u653e\u7f6e Gosper \u6ed1\u7fd4\u673a\u67aa ...\n\n    ctrl_panel.separator()\n    ctrl_panel.text_wrapped(\n        \"\u70b9 Play \u5f00\u59cb\uff0c\u6216\u8005 Step \u5355\u6b65\u3002\"\n        \"Randomize \u91cd\u65b0\u968f\u673a\u586b\u5145\u3002\"\n    )\n\n\nview.run()\n</code></pre> <p>\uff08\u5b8c\u6574\u7684\u793a\u4f8b\u6587\u4ef6\u4e2d\u5305\u542b\u6240\u6709\u56fe\u6848\u653e\u7f6e\u7684\u8f85\u52a9\u51fd\u6570\u3002\uff09</p>"},{"location":"zh/04_conway/#_5","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u7f51\u683c \u2014 \u4e00\u4e2a\u666e\u901a\u7684 <code>(256, 256)</code> float32 CUDA tensor\u3002    <code>1.0</code> \u5c31\u662f\u6d3b\u7684\uff0c<code>0.0</code> \u5c31\u662f\u6b7b\u7684\u3002\u4e0d\u9700\u8981\u7c7b\uff0c\u4e0d\u9700\u8981\u82b1\u54e8\u7684\u6570\u636e\u7ed3\u6784 \u2014\u2014 \u5c31\u4e00\u4e2a tensor\u3002</p> </li> <li> <p>\u6a21\u62df \u2014 <code>step_simulation()</code> \u7528 <code>F.conv2d</code> \u914d\u5408 circular padding \u6570\u90bb\u5c45\uff0c    \u7136\u540e\u7528\u5e03\u5c14 mask \u5e94\u7528\u51fa\u751f/\u5b58\u6d3b\u89c4\u5219\u3002\u6574\u4ee3\u53ea\u9700\u8981\u4e24\u4e2a GPU kernel\u3002</p> </li> <li> <p>\u663e\u793a \u2014 <code>create_tensor</code> \u5206\u914d Vulkan/CUDA \u5171\u4eab\u663e\u5b58\u3002    <code>grid_to_display()</code> \u5728\u5b58\u6d3b\u548c\u6b7b\u4ea1\u989c\u8272\u4e4b\u95f4\u63d2\u503c\uff0c\u5199\u8fdb\u53bb\u5c31\u884c\u3002\u96f6\u62f7\u8d1d\u4e0a\u5c4f\u3002</p> </li> <li> <p>\u63a7\u5236\u9762\u677f \u2014 <code>@ctrl_panel.on_frame</code> \u628a\u6240\u6709\u63a7\u4ef6\u753b\u5728 Controls \u9762\u677f\u5185\u90e8\u3002    <code>panel.button()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.color_picker()</code>\u3001    <code>with panel.row()</code> \u8ba9\u5e03\u5c40\u7d27\u51d1\u3002\u72b6\u6001\u5c31\u7528\u4e00\u4e2a\u666e\u901a Python dict \u7ba1\u7406\u3002</p> </li> </ol>"},{"location":"zh/04_conway/#_6","title":"\u8981\u70b9","text":"<ol> <li> <p><code>create_tensor</code> \u4e0d\u53ea\u662f\u7ed9\u8bad\u7ec3\u7528\u7684 \u2014 \u4efb\u4f55 GPU \u8ba1\u7b97\u53ea\u8981\u4ea7\u51fa    \u7c7b\u4f3c\u56fe\u50cf\u7684 tensor\uff0c\u90fd\u53ef\u4ee5\u96f6\u62f7\u8d1d\u663e\u793a\u3002</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014 \u50cf\u7d20\u753b / \u7f51\u683c\u6a21\u62df\u7684\u5fc5\u9009\u9879\u3002    \u6ca1\u6709\u5b83\u7684\u8bdd\u53cc\u7ebf\u6027\u63d2\u503c\u4f1a\u628a\u683c\u5b50\u8fb9\u754c\u7cca\u6210\u4e00\u5768\u3002</p> </li> <li> <p>\u5377\u79ef = \u6570\u90bb\u5c45 \u2014 \u4e00\u4e2a\u5c0f\u6280\u5de7\uff0c\u7528\u4e00\u4e2a GPU kernel \u66ff\u4ee3\u5d4c\u5957 Python \u5faa\u73af\u3002    \u5373\u4f7f\u7f51\u683c\u5f88\u5927\uff0c\u6e38\u620f\u4e5f\u80fd\u8dd1\u5230\u51e0\u767e FPS\u3002</p> </li> <li> <p>\u9762\u677f\u63a7\u4ef6 \u2014 <code>@panel.on_frame</code> + <code>panel.button()</code>\u3001    <code>panel.slider_int()</code>\u3001<code>panel.color_picker()</code> \u628a\u6240\u6709 UI \u4ee3\u7801\u653e\u5728    \u9762\u677f\u7a97\u53e3\u4e0a\u4e0b\u6587\u91cc\u3002\u4e0d\u7528\u624b\u52a8\u5199 <code>ui.begin/end</code>\u3002</p> </li> <li> <p><code>panel.row()</code> \u2014 \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668\uff0c\u628a\u5b50\u63a7\u4ef6\u5e76\u6392\u653e\u3002    \u6bd4\u624b\u52a8\u8c03 <code>ui.same_line()</code> \u66f4\u5e72\u51c0\u3002</p> </li> <li> <p>\u7ecf\u5178\u56fe\u6848 \u2014 Glider\u3001Pulsar\u3001Gosper Gun \u6309\u94ae\u5c55\u793a\u4e86\u600e\u4e48\u901a\u8fc7    \u76f4\u63a5\u5f80 grid tensor \u91cc\u5199\u503c\u6765\u8bbe\u521d\u59cb\u6761\u4ef6\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>Speed \u6ed1\u6761\u62c9\u5230 20\uff0c\u770b\u7f51\u683c\u4ee5\u6bcf\u5e27 20 \u4ee3\u7684\u901f\u5ea6\u6f14\u5316\u3002 \u73b0\u4ee3 GPU \u4e0a\u5e27\u7387\u4f9d\u7136\u7a33\u7a33 60+\u3002</p> <p>\u8bf4\u660e</p> <p>\u7f51\u683c\u73af\u7ed5\u662f\u56e0\u4e3a\u7528\u4e86 <code>mode='circular'</code> padding\u3002 \u6ed1\u7fd4\u673a\u4ece\u53f3\u8fb9\u98de\u51fa\u53bb\u4f1a\u4ece\u5de6\u8fb9\u5192\u51fa\u6765\u3002</p>"},{"location":"zh/05_image_viewer/","title":"05 \u2014 \u56fe\u7247\u67e5\u770b\u5668","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/05_image_viewer.py</code></p> <p>\u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u4e00\u76f4\u5728\u51ed\u7a7a\u9020 tensor \u2014\u2014 \u6e10\u53d8\u3001\u68cb\u76d8\u683c\u3001\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u3002 \u5f88\u9177\uff0c\u4f46\u603b\u6709\u90a3\u4e48\u4e00\u5929\uff0c\u4f60\u60f3\u770b\u4e00\u5f20\u771f\u6b63\u7684\u56fe\u7247\u3002\u5c31\u662f\u90a3\u79cd\u5b89\u5b89\u9759\u9759\u8eba\u5728 \u786c\u76d8\u4e0a\u7684 <code>.png</code> \u6587\u4ef6\u3002\u50cf\u4e2a\u6b63\u5e38\u4eba\u4e00\u6837\u3002</p> <p>\"\u7528 PIL \u4e0d\u5c31\u884c\u4e86\u3002\" \u597d\uff0c\u7136\u540e\u4f60\u8fd8\u9700\u8981 <code>torchvision.transforms</code>\uff0c \u7136\u540e\u9700\u8981 <code>numpy</code>\uff0c\u7136\u540e\u9700\u8981 <code>cv2.cvtColor</code> \u56e0\u4e3a\u4e0d\u77e5\u9053\u8c01\u628a RGB \u548c BGR \u641e\u53cd\u4e86\uff0c\u7136\u540e\u4f60\u6253\u5f00\u4e86\u4e09\u4e2a Stack Overflow \u6807\u7b7e\u9875\uff0c\u8bd5\u56fe\u641e\u660e\u767d\u4e3a\u4ec0\u4e48\u56fe\u7247 \u4e0a\u4e0b\u98a0\u5012\u800c\u4e14\u504f\u7eff\u3002</p> <p>Vultorch \u5185\u7f6e\u4e86\u56fe\u7247\u8bfb\u5199\u3002\u4e00\u4e2a\u51fd\u6570\u8fdb\uff0c\u4e00\u4e2a\u51fd\u6570\u51fa\u3002 \u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 OpenCV\uff0c\u4e0d\u9700\u8981\u6000\u7591\u4eba\u751f\u3002</p>"},{"location":"zh/05_image_viewer/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u5199\u6cd5 imread \u628a\u56fe\u7247\u6587\u4ef6\u8bfb\u6210 CUDA tensor <code>vultorch.imread(\"photo.png\")</code> imwrite \u628a tensor \u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6 <code>vultorch.imwrite(\"out.png\", t)</code> Canvas.save() \u4fdd\u5b58\u753b\u5e03\u7ed1\u5b9a\u7684 tensor <code>canvas.save(\"out.png\")</code> panel.combo() \u4e0b\u62c9\u9009\u62e9\u83dc\u5355 <code>panel.combo(\"\u9009\u9879\", [\"A\",\"B\"])</code> panel.input_text() \u6587\u672c\u8f93\u5165\u6846 <code>panel.input_text(\"\u8def\u5f84\")</code> canvas.filter \u91c7\u6837\u6a21\u5f0f\uff08<code>\"linear\"</code> / <code>\"nearest\"</code>\uff09 <code>canvas.filter = \"nearest\"</code>"},{"location":"zh/05_image_viewer/#_2","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u8ff7\u4f60\u56fe\u7247\u67e5\u770b\u5668\uff1a\u52a0\u8f7d\u4e00\u5f20\u56fe\uff0c\u4ece\u4e0b\u62c9\u83dc\u5355\u9009\u53d8\u6362\uff0c \u7528\u6ed1\u6761\u8c03\u4eae\u5ea6 / \u5bf9\u6bd4\u5ea6\uff0c\u7136\u540e\u628a\u7ed3\u679c\u5b58\u76d8\u3002</p> \u5de6\u4fa7 \u53f3\u4fa7\uff08\u4e24\u4e2a\u753b\u5e03\uff09 Controls \u2014 \u53d8\u6362\u9009\u62e9\u3001\u4eae\u5ea6/\u5bf9\u6bd4\u5ea6\u6ed1\u6761\u3001\u6ee4\u6ce2\u5207\u6362\u3001\u4fdd\u5b58 \u539f\u56fe\uff08\u4e0a\uff09 \u53d8\u6362\u540e\uff08\u4e0b\uff09"},{"location":"zh/05_image_viewer/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 \u52a0\u8f7d\u56fe\u7247 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\noriginal = vultorch.imread(img_path, channels=3, device=device)\nH, W, C = original.shape\n\n# \u7528\u4e8e\u53d8\u6362\u7684\u5de5\u4f5c\u526f\u672c\uff08\u539f\u56fe\u6c38\u8fdc\u4e0d\u52a8\uff09\ntransformed = original.clone()\n\n# \u2500\u2500 \u89c6\u56fe + \u9762\u677f \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"05 - Image Viewer\", 1024, 768)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nimg_panel = view.panel(\"Image\")\n\ncanvas_orig = img_panel.canvas(\"Original\")\ncanvas_orig.bind(original)\n\ncanvas_xform = img_panel.canvas(\"Transformed\")\ncanvas_xform.bind(transformed)\n\n# \u2500\u2500 \u72b6\u6001 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRANSFORMS = [\n    \"None\",\n    \"Horizontal Flip\",\n    \"Vertical Flip\",\n    \"Grayscale\",\n    \"Invert\",\n    \"Sepia\",\n]\n\nstate = {\n    \"brightness\": 0.0,\n    \"contrast\": 1.0,\n    \"last_transform\": -1,\n    \"last_brightness\": None,\n    \"last_contrast\": None,\n}\n\n\ndef apply_transform(img, idx):\n    if idx == 0:    return img.clone()\n    elif idx == 1:  return img.flip(1)               # \u6c34\u5e73\u7ffb\u8f6c\n    elif idx == 2:  return img.flip(0)               # \u5782\u76f4\u7ffb\u8f6c\n    elif idx == 3:                                    # \u7070\u5ea6\n        gray = img[:,:,0]*0.299 + img[:,:,1]*0.587 + img[:,:,2]*0.114\n        return gray.unsqueeze(-1).expand_as(img).contiguous()\n    elif idx == 4:  return 1.0 - img                 # \u53cd\u8272\n    elif idx == 5:                                    # \u590d\u53e4\u8272\u8c03\n        r = img[:,:,0]*0.393 + img[:,:,1]*0.769 + img[:,:,2]*0.189\n        g = img[:,:,0]*0.349 + img[:,:,1]*0.686 + img[:,:,2]*0.168\n        b = img[:,:,0]*0.272 + img[:,:,1]*0.534 + img[:,:,2]*0.131\n        return torch.stack([r, g, b], dim=-1).clamp(0, 1)\n    return img.clone()\n\n\ndef apply_brightness_contrast(img, brightness, contrast):\n    return ((img - 0.5) * contrast + 0.5 + brightness).clamp(0, 1)\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Image: {img_path.name}\")\n    ctrl.text(f\"Size: {W} \u00d7 {H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    # \u53d8\u6362\u9009\u62e9\u5668\n    ctrl.text(\"Transform\")\n    xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n\n    ctrl.separator()\n\n    # \u4eae\u5ea6 / \u5bf9\u6bd4\u5ea6\n    ctrl.text(\"Adjustments\")\n    brightness = ctrl.slider(\"Brightness\", -1.0, 1.0, default=0.0)\n    contrast   = ctrl.slider(\"Contrast\",    0.0, 3.0, default=1.0)\n\n    changed = (xform_idx != state[\"last_transform\"]\n               or brightness != state[\"last_brightness\"]\n               or contrast   != state[\"last_contrast\"])\n\n    if changed:\n        result = apply_transform(original, xform_idx)\n        result = apply_brightness_contrast(result, brightness, contrast)\n        transformed[:] = result\n        state[\"last_transform\"]  = xform_idx\n        state[\"last_brightness\"] = brightness\n        state[\"last_contrast\"]   = contrast\n\n    ctrl.separator()\n\n    # \u6ee4\u6ce2\u5207\u6362\n    ctrl.text(\"Sampling Filter\")\n    filter_idx = ctrl.combo(\"##filter\", [\"Linear\", \"Nearest\"], default=0)\n    canvas_orig.filter  = \"nearest\" if filter_idx == 1 else \"linear\"\n    canvas_xform.filter = \"nearest\" if filter_idx == 1 else \"linear\"\n\n    ctrl.separator()\n\n    # \u4fdd\u5b58\n    ctrl.text(\"Save Output\")\n    save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n\n    if ctrl.button(\"Save Image\", width=140):\n        try:\n            canvas_xform.save(save_path)\n            state[\"save_msg\"] = f\"Saved to {save_path}\"\n        except Exception as e:\n            state[\"save_msg\"] = f\"Error: {e}\"\n\n    if \"save_msg\" in state:\n        ctrl.text_wrapped(state[\"save_msg\"])\n\n\nview.run()\n</code></pre>"},{"location":"zh/05_image_viewer/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/05_image_viewer/#imread","title":"imread \u2014 \u544a\u522b\u4f9d\u8d56\u5730\u72f1","text":"<pre><code>original = vultorch.imread(img_path, channels=3, device=device)\n</code></pre> <p>\u4e00\u884c\u641e\u5b9a\u3002\u8fd4\u56de <code>(H, W, 3)</code> \u7684 float32 CUDA tensor\uff0c\u503c\u57df <code>[0, 1]</code>\u3002 \u652f\u6301 PNG\u3001JPEG\u3001BMP\u3001TGA\u3001HDR\u3001PSD\u3001GIF\uff08\u7b2c\u4e00\u5e27\uff09\u3002 \u5e95\u5c42\u7528\u7684\u662f <code>stb_image</code> \u2014\u2014 \u4e0d\u9700\u8981\u4efb\u4f55 Python \u56fe\u50cf\u5e93\u3002</p> <p>\u53ef\u9009\u53c2\u6570\uff1a</p> <ul> <li><code>channels=4</code> \u2014\u2014 \u5f3a\u5236\u8f93\u51fa RGBA\u3002</li> <li><code>size=(256, 256)</code> \u2014\u2014 \u52a0\u8f7d\u540e\u7f29\u653e\uff08\u53cc\u7ebf\u6027\u63d2\u503c\uff09\u3002</li> <li><code>device=\"cpu\"</code> \u2014\u2014 \u5982\u679c\u4f60\u60f3\u7559\u5728 CPU \u4e0a\u3002</li> <li><code>shared=True</code> \u2014\u2014 \u7528 <code>create_tensor</code> \u5206\u914d\uff0c\u96f6\u62f7\u8d1d\u663e\u793a\u3002</li> </ul>"},{"location":"zh/05_image_viewer/#combo","title":"combo \u2014 \u4e0b\u62c9\u83dc\u5355","text":"<pre><code>xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n</code></pre> <p>\u663e\u793a\u4e00\u4e2a\u4e0b\u62c9\u83dc\u5355\uff0c\u91cc\u9762\u662f\u5217\u8868\u4e2d\u7684\u9009\u9879\u3002\u8fd4\u56de\u9009\u4e2d\u9879\u7684\u7d22\u5f15\uff08int\uff09\u3002 \u72b6\u6001\u7531\u9762\u677f\u81ea\u52a8\u7ba1\u7406 \u2014\u2014 \u53ea\u9700\u8981\u4f20 <code>default=</code> \u8bbe\u521d\u59cb\u503c\u5c31\u884c\u3002</p> <p><code>##</code> \u524d\u7f00\u4f1a\u9690\u85cf ImGui \u7684\u6807\u7b7e\u6587\u5b57\uff08<code>##</code> \u540e\u9762\u7684\u5185\u5bb9\u53ea\u4f5c\u4e3a\u5185\u90e8 ID\uff09\u3002 \u5f53\u4f60\u4e0d\u60f3\u5728\u63a7\u4ef6\u65c1\u8fb9\u663e\u793a\u6587\u5b57\u7684\u65f6\u5019\u5f88\u6709\u7528\u3002</p>"},{"location":"zh/05_image_viewer/#input_text","title":"input_text \u2014 \u6587\u672c\u8f93\u5165","text":"<pre><code>save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u5b57\u7b26\u4e32\u3002\u8f93\u4e2a\u6587\u4ef6\u540d\uff0c\u70b9\u4fdd\u5b58\u5c31\u884c\u3002 \u9ed8\u8ba4 <code>max_length=256</code> \u2014\u2014 \u5199\u4e2a\u8def\u5f84\u7ef0\u7ef0\u6709\u4f59\u3002</p>"},{"location":"zh/05_image_viewer/#canvassave","title":"Canvas.save() \u2014 \u4e00\u884c\u5bfc\u51fa","text":"<pre><code>canvas_xform.save(save_path)\n</code></pre> <p>\u628a\u5f53\u524d\u753b\u5e03\u7ed1\u5b9a\u7684 tensor \u4fdd\u5b58\u5230\u6587\u4ef6\u3002\u683c\u5f0f\u6839\u636e\u6269\u5c55\u540d\u81ea\u52a8\u5224\u65ad \uff08<code>.png</code>\u3001<code>.jpg</code>\u3001<code>.bmp</code>\u3001<code>.tga</code>\u3001<code>.hdr</code>\uff09\u3002 \u5e95\u5c42\u8c03\u7528\u7684\u662f <code>vultorch.imwrite()</code>\u3002</p>"},{"location":"zh/05_image_viewer/#filter-nearest-vs-linear","title":"filter \u2014 nearest vs linear","text":"<pre><code>canvas_orig.filter = \"nearest\"   # \u50cf\u7d20\u7ea7\u7cbe\u786e\uff0c\u653e\u5927\u540e\u6709\u952f\u9f7f\ncanvas_orig.filter = \"linear\"    # \u53cc\u7ebf\u6027\u63d2\u503c\uff0c\u5e73\u6ed1\n</code></pre> <p>\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u91c7\u6837\u6ee4\u6ce2\u5668\u3002\u8bd5\u8bd5\u5728\u56fe\u7247\u88ab\u62c9\u4f38\u7684\u65f6\u5019\u5207\u6362 \u2014\u2014 <code>\"nearest\"</code> \u8ba9\u4f60\u770b\u5230\u539f\u59cb\u50cf\u7d20\uff0c<code>\"linear\"</code> \u628a\u5b83\u4eec\u7cca\u6210\u5e73\u6ed1\u6e10\u53d8\u3002 \u505a\u79d1\u7814\u53ef\u89c6\u5316\uff08\u5206\u5272 mask\u3001attention map\uff09\u7684\u65f6\u5019\uff0c\u57fa\u672c\u4e0a\u6c38\u8fdc\u9009 <code>\"nearest\"</code>\u3002</p>"},{"location":"zh/05_image_viewer/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p><code>imread</code> / <code>imwrite</code> \u2014 \u96f6\u4f9d\u8d56\u7684\u56fe\u7247\u8bfb\u5199\u3002\u76f4\u63a5\u8bfb\u5230 CUDA tensor\uff0c    \u76f4\u63a5\u4ece tensor \u5199\u6587\u4ef6\u3002\u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 numpy\uff0c    \u4e0d\u9700\u8981\u8ddf <code>cv2.cvtColor</code> \u6597\u667a\u6597\u52c7\u3002</p> </li> <li> <p><code>combo</code> \u2014 \u4e0b\u62c9\u9009\u62e9\u63a7\u4ef6\u3002\u8fd4\u56de int \u7d22\u5f15\u3002\u9002\u5408\u6a21\u5f0f\u5207\u6362\u3001    \u9884\u8bbe\u9009\u62e9\u3001\u679a\u4e3e\u7c7b\u578b\u7684\u9009\u9879\u3002</p> </li> <li> <p><code>input_text</code> \u2014 \u81ea\u7531\u6587\u672c\u8f93\u5165\u3002\u9002\u5408\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u540d\u3001    \u5b9e\u9a8c\u6807\u7b7e\u8fd9\u7c7b\u9700\u8981\u6253\u5b57\u7684\u573a\u666f\u3002</p> </li> <li> <p><code>Canvas.save()</code> \u2014 \u4e00\u884c\u628a\u7ed1\u5b9a\u7684 tensor \u5b58\u4e3a\u56fe\u7247\u3002    \u6269\u5c55\u540d\u51b3\u5b9a\u683c\u5f0f\u3002</p> </li> <li> <p>\u6309\u9700\u91cd\u7b97 \u2014 \u53ea\u5728\u6ed1\u6761\u6216\u4e0b\u62c9\u503c\u771f\u6b63\u53d8\u5316\u65f6\u624d\u91cd\u65b0\u8ba1\u7b97\u53d8\u6362\u3002    \u901a\u8fc7\u68c0\u67e5 <code>changed</code> \u6765\u907f\u514d\u6bcf\u5e27\u90fd\u767d\u767d\u70e7 GPU\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p><code>imread</code> \u652f\u6301 <code>size=(H, W)</code> \u53c2\u6570\uff0c\u53ef\u4ee5\u5728\u52a0\u8f7d\u65f6\u7f29\u653e\u3002 \u4f60\u7684\u56fe\u7247\u662f 4K \u7684\uff0c\u4f46\u53ea\u9700\u8981 256\u00d7256 \u9884\u89c8\uff1f\u4e00\u4e2a\u53c2\u6570\u641e\u5b9a\u3002</p> <p>\u8bf4\u660e</p> <p><code>imwrite</code> \u63a5\u53d7 <code>[0, 1]</code> \u7684 float32 tensor\uff0c\u4e5f\u63a5\u53d7 <code>[0, 255]</code> \u7684 uint8 tensor\u3002 \u683c\u5f0f\u8f6c\u6362\u5b83\u81ea\u5df1\u5904\u7406\u3002</p>"},{"location":"zh/api/","title":"API \u53c2\u8003\u6587\u6863","text":"<p><code>vultorch</code> \u5305\u6240\u6709\u516c\u5f00\u7c7b\u548c\u51fd\u6570\u7684\u5b8c\u6574\u53c2\u8003\u3002</p>"},{"location":"zh/api/#_1","title":"\u6a21\u5757\u7ea7\u5c5e\u6027","text":""},{"location":"zh/api/#vultorch__version__","title":"<code>vultorch.__version__</code>","text":"<pre><code>__version__: str\n</code></pre> <p>\u5305\u7248\u672c\u5b57\u7b26\u4e32\uff08\u4f8b\u5982 <code>\"0.5.0\"</code>\uff09\u3002</p>"},{"location":"zh/api/#vultorchhas_cuda","title":"<code>vultorch.HAS_CUDA</code>","text":"<pre><code>HAS_CUDA: bool\n</code></pre> <p>\u5982\u679c\u539f\u751f\u6269\u5c55\u6a21\u5757\u7f16\u8bd1\u65f6\u542f\u7528\u4e86 CUDA \u5219\u4e3a <code>True</code>\u3002\u4e3a <code>False</code> \u65f6\uff0c\u6240\u6709\u5f20\u91cf\u663e\u793a\u5c06\u56de\u9000\u5230 CPU \u6682\u5b58\u7f13\u51b2\u533a\uff08host-visible <code>memcpy</code>\uff09\u3002</p>"},{"location":"zh/api/#_2","title":"\u6838\u5fc3\u51fd\u6570","text":""},{"location":"zh/api/#vultorchshow","title":"<code>vultorch.show()</code>","text":"<pre><code>def show(\n    tensor: torch.Tensor,\n    *,\n    name: str = \"tensor\",\n    width: float = 0,\n    height: float = 0,\n    filter: str = \"linear\",\n    window: Window | None = None,\n) -&gt; None\n</code></pre> <p>\u5728\u5f53\u524d ImGui \u4e0a\u4e0b\u6587\u4e2d\u663e\u793a\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>tensor</code> <code>torch.Tensor</code> (\u5fc5\u9700) CUDA \u6216 CPU \u5f20\u91cf\u3002dtype\uff1a<code>float32</code>\u3001<code>float16</code> \u6216 <code>uint8</code>\u3002\u5f62\u72b6\uff1a<code>(H, W)</code> \u6216 <code>(H, W, C)</code>\uff0cC \u2208 {1, 3, 4}\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u552f\u4e00\u6807\u7b7e\uff0c\u7528\u4e8e\u540c\u65f6\u663e\u793a\u591a\u4e2a\u5f20\u91cf\u65f6\u7684\u7f13\u5b58\u3002 <code>width</code> <code>float</code> <code>0</code> \u663e\u793a\u5bbd\u5ea6\uff08\u50cf\u7d20\uff09\u3002<code>0</code> = \u81ea\u52a8\u9002\u5e94\u5f20\u91cf\u5927\u5c0f\u3002 <code>height</code> <code>float</code> <code>0</code> \u663e\u793a\u9ad8\u5ea6\uff08\u50cf\u7d20\uff09\u3002<code>0</code> = \u81ea\u52a8\u9002\u5e94\u5f20\u91cf\u5927\u5c0f\u3002 <code>filter</code> <code>str</code> <code>\"linear\"</code> \u91c7\u6837\u6ee4\u6ce2\u5668\uff1a<code>\"nearest\"</code> \u6216 <code>\"linear\"</code>\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\u3002\u9ed8\u8ba4\u4f7f\u7528 <code>Window._current</code>\u3002 <p>\u884c\u4e3a\uff1a</p> <ul> <li>1 \u901a\u9053\u548c 3 \u901a\u9053\u5f20\u91cf\u4f1a\u81ea\u52a8\u6269\u5c55\u4e3a RGBA\u3002</li> <li>RGBA \u6269\u5c55\u7f13\u51b2\u533a\u6309 <code>name</code> \u7f13\u5b58\uff0c\u907f\u514d\u6bcf\u5e27\u5206\u914d\u5185\u5b58\u3002</li> <li>CUDA\uff1a\u4f7f\u7528\u96f6\u62f7\u8d1d GPU\u2192GPU \u8def\u5f84\u3002CPU\uff1a\u4f7f\u7528 host-visible \u6682\u5b58\u7f13\u51b2\u533a\u3002</li> <li><code>uint8</code> \u5f20\u91cf\u9664\u4ee5 255\uff1b<code>float16</code> \u5f20\u91cf\u8f6c\u6362\u4e3a <code>float32</code>\u3002</li> </ul> <p>\u5f02\u5e38\uff1a \u5982\u679c\u6ca1\u6709\u6d3b\u8dc3\u7684 <code>Window</code> \u5219\u629b\u51fa <code>RuntimeError</code>\u3002</p>"},{"location":"zh/api/#vultorchcreate_tensor","title":"<code>vultorch.create_tensor()</code>","text":"<pre><code>def create_tensor(\n    height: int,\n    width: int,\n    channels: int = 4,\n    device: str = \"cuda:0\",\n    *,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>\u5206\u914d Vulkan \u5171\u4eab\u7684 CUDA \u5f20\u91cf\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u62f7\u8d1d\u663e\u793a\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>height</code> <code>int</code> (\u5fc5\u9700) \u5f20\u91cf\u9ad8\u5ea6\uff08\u50cf\u7d20\uff09\u3002 <code>width</code> <code>int</code> (\u5fc5\u9700) \u5f20\u91cf\u5bbd\u5ea6\uff08\u50cf\u7d20\uff09\u3002 <code>channels</code> <code>int</code> <code>4</code> \u901a\u9053\u6570\uff1a1\u30013 \u6216 4\u3002 <code>device</code> <code>str</code> <code>\"cuda:0\"</code> CUDA \u8bbe\u5907\u5b57\u7b26\u4e32\uff0c\u6216 <code>\"cpu\"</code>\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u7eb9\u7406\u69fd\u540d\u79f0\uff08\u5fc5\u987b\u4e0e <code>show(..., name=...)</code> \u5339\u914d\uff09\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\u3002\u9ed8\u8ba4\u4f7f\u7528 <code>Window._current</code>\u3002 <p>\u8fd4\u56de\uff1a \u5f62\u72b6\u4e3a <code>(height, width, channels)</code> \u7684 <code>torch.Tensor</code>\u3002</p> <p>Note</p> <p>\u53ea\u6709 <code>channels=4</code> \u624d\u80fd\u901a\u8fc7 Vulkan \u5916\u90e8\u5185\u5b58\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u62f7\u8d1d\u3002\u5bf9\u4e8e 1 \u6216 3 \u901a\u9053\uff0c\u8fd4\u56de\u666e\u901a CUDA \u5f20\u91cf\uff0c<code>show()</code> \u4f1a\u5904\u7406 RGBA \u6269\u5c55\u5e76\u8fdb\u884c GPU\u2192GPU \u62f7\u8d1d\u3002</p> <p>\u5f02\u5e38\uff1a \u5982\u679c\u6ca1\u6709\u6d3b\u8dc3\u7684 <code>Window</code> \u5219\u629b\u51fa <code>RuntimeError</code>\u3002</p>"},{"location":"zh/api/#vultorchimread","title":"<code>vultorch.imread()</code>","text":"<pre><code>def imread(\n    path: str,\n    *,\n    channels: int = 4,\n    size: tuple[int, int] | None = None,\n    device: str = \"cuda\",\n    shared: bool = False,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>\u5c06\u56fe\u7247\u6587\u4ef6\u52a0\u8f7d\u4e3a <code>float32</code> \u5f20\u91cf\u3002\u5185\u90e8\u4f7f\u7528 stb_image \u2014\u2014 \u4e0d\u9700\u8981 PIL \u6216 numpy\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>path</code> <code>str</code> \uff08\u5fc5\u9700\uff09 \u6587\u4ef6\u8def\u5f84\uff08PNG\u3001JPG\u3001BMP\u3001TGA\u3001HDR \u7b49\uff09\u3002 <code>channels</code> <code>int</code> <code>4</code> \u671f\u671b\u901a\u9053\u6570\uff1a1\uff08\u7070\u5ea6\uff09\u30013\uff08RGB\uff09\u62164\uff08RGBA\uff09\u3002 <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> \u53ef\u9009 <code>(\u9ad8\u5ea6, \u5bbd\u5ea6)</code>\uff0c\u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u7f29\u653e\u3002 <code>device</code> <code>str</code> <code>\"cuda\"</code> \u76ee\u6807\u8bbe\u5907\uff08<code>\"cuda\"</code> \u6216 <code>\"cpu\"</code>\uff09\u3002 <code>shared</code> <code>bool</code> <code>False</code> \u4e3a <code>True</code> \u65f6\u901a\u8fc7 <code>create_tensor</code> \u5206\u914d\uff0c\u5b9e\u73b0\u96f6\u62f7\u8d1d\u663e\u793a\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u7eb9\u7406\u69fd\u540d\u79f0\uff08\u4ec5\u5728 <code>shared=True</code> \u65f6\u4f7f\u7528\uff09\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\uff08\u4ec5\u5728 <code>shared=True</code> \u65f6\u4f7f\u7528\uff09\u3002 <p>\u8fd4\u56de\uff1a \u5f62\u72b6\u4e3a <code>(H, W, C)</code> \u7684 <code>torch.Tensor</code>\uff0c\u503c\u8303\u56f4 <code>[0, 1]</code>\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>import vultorch\ngt = vultorch.imread(\"photo.png\", channels=3, device=\"cuda\")\n</code></pre>"},{"location":"zh/api/#vultorchimwrite","title":"<code>vultorch.imwrite()</code>","text":"<pre><code>def imwrite(\n    path: str,\n    tensor: torch.Tensor,\n    *,\n    channels: int = 0,\n    size: tuple[int, int] | None = None,\n    quality: int = 95,\n) -&gt; None\n</code></pre> <p>\u5c06\u5f20\u91cf\u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6\u3002\u683c\u5f0f\u7531\u6269\u5c55\u540d\u63a8\u65ad\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>path</code> <code>str</code> \uff08\u5fc5\u9700\uff09 \u8f93\u51fa\u6587\u4ef6\u8def\u5f84\u3002\u6269\u5c55\u540d\u9009\u62e9\u683c\u5f0f\uff1a<code>.png</code>\u3001<code>.jpg</code>\u3001<code>.bmp</code>\u3001<code>.tga</code>\u3001<code>.hdr</code>\u3002 <code>tensor</code> <code>torch.Tensor</code> \uff08\u5fc5\u9700\uff09 <code>(H, W)</code>\u3001<code>(H, W, 1)</code>\u3001<code>(H, W, 3)</code> \u6216 <code>(H, W, 4)</code> \u5f20\u91cf\u3002 <code>channels</code> <code>int</code> <code>0</code> \u8986\u76d6\u8f93\u51fa\u901a\u9053\u6570\u3002<code>0</code> = \u4f7f\u7528\u5f20\u91cf\u81ea\u8eab\u7684\u901a\u9053\u6570\u3002 <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> \u53ef\u9009 <code>(\u9ad8\u5ea6, \u5bbd\u5ea6)</code>\uff0c\u4fdd\u5b58\u524d\u7f29\u653e\u3002 <code>quality</code> <code>int</code> <code>95</code> JPEG \u8d28\u91cf\uff081\u2013100\uff09\u3002\u5176\u4ed6\u683c\u5f0f\u5ffd\u7565\u6b64\u53c2\u6570\u3002 <p>\u884c\u4e3a\uff1a</p> <ul> <li><code>.hdr</code> \u5199\u5165 32 \u4f4d\u6d6e\u70b9\u6570\u636e\uff1b\u5176\u4ed6\u683c\u5f0f\u91cf\u5316\u4e3a 8 \u4f4d\u3002</li> <li>\u5982\u679c\u5f20\u91cf\u901a\u9053\u6570\u591a\u4e8e <code>channels</code>\uff0c\u591a\u4f59\u901a\u9053\u88ab\u4e22\u5f03\u3002\u5982\u679c\u5c11\u4e8e\uff0c\u7f3a\u5c11\u7684\u901a\u9053\u4f1a\u88ab\u586b\u5145\uff08alpha \u2192 1.0\uff09\u3002</li> <li>\u5f20\u91cf\u4f1a\u88ab\u79fb\u5230 CPU \u5e76\u8f6c\u6362\u4e3a <code>float32</code> \u540e\u518d\u5199\u5165\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>vultorch.imwrite(\"output.png\", pred_tensor, channels=3)\nvultorch.imwrite(\"output.jpg\", pred_tensor, quality=90)\n</code></pre>"},{"location":"zh/api/#_3","title":"\u7c7b","text":""},{"location":"zh/api/#vultorchwindow","title":"<code>vultorch.Window</code>","text":"<pre><code>class Window:\n    _current: Window | None   # \u5355\u4f8b\u5f15\u7528\n\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>Vulkan + SDL3 + ImGui \u5f15\u64ce\u7684\u9ad8\u5c42\u5c01\u88c5\u3002\u521b\u5efa <code>Window</code> \u4f1a\u81ea\u52a8\u5c06\u5176\u8bbe\u7f6e\u4e3a <code>show()</code> \u548c <code>create_tensor()</code> \u7684\u5f53\u524d\u76ee\u6807\u3002</p>"},{"location":"zh/api/#_4","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>poll()</code> <code>\u2192 bool</code> \u5904\u7406\u64cd\u4f5c\u7cfb\u7edf\u4e8b\u4ef6\u3002\u7a97\u53e3\u5e94\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>begin_frame()</code> <code>\u2192 bool</code> \u5f00\u59cb\u65b0\u7684 ImGui \u5e27\u3002\u5e27\u88ab\u8df3\u8fc7\uff08\u6700\u5c0f\u5316\uff09\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>end_frame()</code> <code>\u2192 None</code> \u63d0\u4ea4\u5e27\u5230 GPU \u5e76\u5448\u73b0\u3002 <code>activate()</code> <code>\u2192 None</code> \u5c06\u6b64\u7a97\u53e3\u8bbe\u4e3a\u6a21\u5757\u7ea7\u8f85\u52a9\u51fd\u6570\u7684\u5f53\u524d\u76ee\u6807\u3002 <code>upload_tensor(tensor, *, name)</code> <code>\u2192 None</code> \u4e0a\u4f20\u5f20\u91cf\u7528\u4e8e\u663e\u793a\uff08CUDA \u6216 CPU\uff09\u3002 <code>get_texture_id(name)</code> <code>\u2192 int</code> \u6307\u5b9a\u540d\u79f0\u5f20\u91cf\u7684 ImGui \u7eb9\u7406 ID\u3002 <code>get_texture_size(name)</code> <code>\u2192 (int, int)</code> \u6307\u5b9a\u540d\u79f0\u5f20\u91cf\u7684 <code>(\u5bbd\u5ea6, \u9ad8\u5ea6)</code>\u3002 <code>destroy()</code> <code>\u2192 None</code> \u91ca\u653e\u6240\u6709 GPU / \u7a97\u53e3\u8d44\u6e90\u3002\u53ef\u5b89\u5168\u591a\u6b21\u8c03\u7528\u3002"},{"location":"zh/api/#_5","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 <code>tensor_texture_id</code> <code>int</code> \u9ed8\u8ba4 <code>\"tensor\"</code> \u69fd\u7684 ImGui \u7eb9\u7406 ID\u3002 <code>tensor_size</code> <code>(int, int)</code> \u9ed8\u8ba4 <code>\"tensor\"</code> \u69fd\u7684 <code>(\u5bbd\u5ea6, \u9ad8\u5ea6)</code>\u3002"},{"location":"zh/api/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>import vultorch\nfrom vultorch import ui\n\nwin = vultorch.Window(\"Demo\", 1280, 720)\nwhile win.poll():\n    if not win.begin_frame():\n        continue\n    ui.begin(\"Panel\", True, 0)\n    vultorch.show(tensor)\n    ui.end()\n    win.end_frame()\nwin.destroy()\n</code></pre>"},{"location":"zh/api/#vultorchcamera","title":"<code>vultorch.Camera</code>","text":"<pre><code>class Camera:\n    azimuth: float     # \u6c34\u5e73\u89d2\u5ea6\uff08\u5f27\u5ea6\uff09\uff0c\u9ed8\u8ba4 0.0\n    elevation: float   # \u5782\u76f4\u89d2\u5ea6\uff08\u5f27\u5ea6\uff09\uff0c\u9ed8\u8ba4 0.6\n    distance: float    # \u5230\u76ee\u6807\u7684\u8ddd\u79bb\uff0c\u9ed8\u8ba4 3.0\n    target: tuple      # (x, y, z) \u6ce8\u89c6\u70b9\uff0c\u9ed8\u8ba4 (0, 0, 0)\n    fov: float         # \u89c6\u573a\u89d2\uff08\u5ea6\uff09\uff0c\u9ed8\u8ba4 45.0\n</code></pre> <p><code>SceneView</code> \u4f7f\u7528\u7684\u8f68\u9053\u76f8\u673a\u53c2\u6570\u3002\u8c03\u7528 <code>reset()</code> \u6062\u590d\u9ed8\u8ba4\u503c\u3002</p>"},{"location":"zh/api/#vultorchlight","title":"<code>vultorch.Light</code>","text":"<pre><code>class Light:\n    direction: tuple   # (x, y, z)\uff0c\u9ed8\u8ba4 (0.3, -1.0, 0.5)\n    color: tuple       # (r, g, b)\uff0c\u9ed8\u8ba4 (1, 1, 1)\n    intensity: float   # \u9ed8\u8ba4 1.0\n    ambient: float     # \u73af\u5883\u5149\u9879\uff0c\u9ed8\u8ba4 0.15\n    specular: float    # \u9ad8\u5149\u9879\uff0c\u9ed8\u8ba4 0.5\n    shininess: float   # Blinn-Phong \u6307\u6570\uff0c\u9ed8\u8ba4 32.0\n    enabled: bool      # \u9ed8\u8ba4 True\n</code></pre> <p><code>SceneView</code> \u4f7f\u7528\u7684 Blinn-Phong \u65b9\u5411\u5149\u53c2\u6570\u3002</p>"},{"location":"zh/api/#vultorchsceneview","title":"<code>vultorch.SceneView</code>","text":"<pre><code>class SceneView:\n    def __init__(self, name: str = \"SceneView\",\n                 width: int = 800, height: int = 600,\n                 msaa: int = 4) -&gt; None: ...\n</code></pre> <p>3D \u5f20\u91cf\u67e5\u770b\u5668 \u2014 \u5728\u5e26\u5149\u7167\u7684\u5e73\u9762\u4e0a\u6e32\u67d3\u5f20\u91cf\uff0c\u652f\u6301\u8f68\u9053\u76f8\u673a\u548c MSAA\u3002</p>"},{"location":"zh/api/#_7","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>name</code> <code>str</code> <code>\"SceneView\"</code> ImGui \u7a97\u53e3\u6807\u7b7e\u3002 <code>camera</code> <code>Camera</code> (\u81ea\u52a8) \u8f68\u9053\u76f8\u673a\uff08\u62d6\u62fd\u65cb\u8f6c\uff09\u3002 <code>light</code> <code>Light</code> (\u81ea\u52a8) \u65b9\u5411\u5149\u6e90\u3002 <code>background</code> <code>tuple</code> <code>(0.12, 0.12, 0.14)</code> \u80cc\u666f\u989c\u8272 <code>(r, g, b)</code>\u3002 <code>msaa</code> <code>int</code> <code>4</code> \u591a\u91cd\u91c7\u6837\u6297\u952f\u9f7f\u7ea7\u522b\uff081/2/4/8\uff09\u3002"},{"location":"zh/api/#_8","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u63cf\u8ff0 <code>set_tensor(tensor)</code> \u4e0a\u4f20\u5f20\u91cf\u5230\u573a\u666f\u7eb9\u7406\u3002 <code>render()</code> \u5904\u7406\u9f20\u6807\u4ea4\u4e92\uff0c\u6e32\u67d3\u573a\u666f\uff0c\u5e76\u4f5c\u4e3a ImGui \u56fe\u50cf\u663e\u793a\u3002"},{"location":"zh/api/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>scene = vultorch.SceneView(\"3D \u89c6\u56fe\", 800, 600, msaa=4)\n# \u5728\u5e27\u5faa\u73af\u4e2d\uff1a\nscene.set_tensor(tensor)\nscene.render()\n</code></pre>"},{"location":"zh/api/#api_1","title":"\u58f0\u660e\u5f0f API","text":"<p>\u58f0\u660e\u5f0f API \u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u7684\u62bd\u8c61\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u9762\u677f\u53ef\u89c6\u5316\u5e94\u7528\u3002</p>"},{"location":"zh/api/#vultorchview","title":"<code>vultorch.View</code>","text":"<pre><code>class View:\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>\u652f\u6301\u81ea\u52a8\u505c\u9760\u5e03\u5c40\u7684\u9876\u5c42\u7a97\u53e3\u3002</p>"},{"location":"zh/api/#_10","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>panel(name, *, side, width)</code> <code>\u2192 Panel</code> \u521b\u5efa\u6216\u83b7\u53d6\u53ef\u505c\u9760\u9762\u677f\u3002<code>side</code>\uff1a<code>\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code> / <code>None</code>\u3002 <code>on_frame(fn)</code> <code>\u2192 fn</code> \u88c5\u9970\u5668 \u2014 \u6ce8\u518c\u6bcf\u5e27\u56de\u8c03\u51fd\u6570\u3002 <code>run()</code> <code>\u2192 None</code> \u963b\u585e\u5f0f\u4e8b\u4ef6\u5faa\u73af\u3002 <code>step()</code> <code>\u2192 bool</code> \u975e\u963b\u585e\uff1a\u5904\u7406\u4e00\u5e27\u3002\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>end_step()</code> <code>\u2192 None</code> \u7ed3\u675f\u7531 <code>step()</code> \u5f00\u59cb\u7684\u5e27\u3002 <code>close()</code> <code>\u2192 None</code> \u9500\u6bc1\u7a97\u53e3\u3002"},{"location":"zh/api/#_11","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 <code>fps</code> <code>float</code> \u5f53\u524d\u6bcf\u79d2\u5e27\u6570\u3002 <code>time</code> <code>float</code> \u5df2\u8fc7\u79d2\u6570\u3002 <code>window</code> <code>Window</code> \u5e95\u5c42 <code>Window</code> \u5b9e\u4f8b\u3002"},{"location":"zh/api/#_12","title":"\u4f7f\u7528\u793a\u4f8b \u2014 \u963b\u585e\u6a21\u5f0f","text":"<pre><code>view = vultorch.View(\"Demo\", 1280, 720)\nview.panel(\"Viewer\").canvas(\"img\").bind(tensor)\n\n@view.on_frame\ndef update():\n    speed = controls.slider(\"Speed\", 0, 10)\n    tensor[:,:,0] = (x + view.time * speed).sin()\n\nview.run()\n</code></pre>"},{"location":"zh/api/#_13","title":"\u4f7f\u7528\u793a\u4f8b \u2014 \u8bad\u7ec3\u5faa\u73af","text":"<pre><code>view = vultorch.View(\"Train\", 1024, 768)\noutput = view.panel(\"Output\").canvas(\"result\")\nfor epoch in range(100):\n    result = model(input)\n    output.bind(result)\n    if not view.step():\n        break\n    view.end_step()\nview.close()\n</code></pre>"},{"location":"zh/api/#vultorchpanel","title":"<code>vultorch.Panel</code>","text":"<pre><code>class Panel:\n    # \u901a\u8fc7 View.panel() \u521b\u5efa \u2014 \u4e0d\u76f4\u63a5\u5b9e\u4f8b\u5316\n</code></pre> <p>\u5305\u542b\u753b\u5e03\u548c\u63a7\u4ef6\u7684\u53ef\u505c\u9760\u9762\u677f\u3002</p>"},{"location":"zh/api/#_14","title":"\u753b\u5e03\u5de5\u5382","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>canvas(name, *, filter, fit)</code> <code>\u2192 Canvas</code> \u521b\u5efa\u547d\u540d\u753b\u5e03\u3002<code>filter</code>\uff1a<code>\"linear\"</code> / <code>\"nearest\"</code>\u3002<code>fit</code>\uff1a\u81ea\u52a8\u586b\u5145\u9762\u677f\u7a7a\u95f4\u3002"},{"location":"zh/api/#_15","title":"\u9762\u677f\u56de\u8c03","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>on_frame(fn)</code> <code>\u2192 fn</code> \u88c5\u9970\u5668 \u2014 \u6ce8\u518c\u5728\u9762\u677f ImGui \u7a97\u53e3\u5185\u8fd0\u884c\u7684\u6bcf\u5e27\u56de\u8c03\u3002"},{"location":"zh/api/#_16","title":"\u5e03\u5c40","text":"\u65b9\u6cd5 \u63cf\u8ff0 <code>row()</code> \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 \u2014 \u5c06\u5b50\u63a7\u4ef6\u5e76\u6392\u653e\u7f6e\u3002"},{"location":"zh/api/#_17","title":"\u63a7\u4ef6","text":"<p>\u6240\u6709\u63a7\u4ef6\u65b9\u6cd5\u81ea\u52a8\u7ba1\u7406\u8de8\u5e27\u72b6\u6001\u3002</p> \u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>text(text)</code> <code>\u2192 None</code> \u9759\u6001\u6587\u672c\u3002 <code>text_colored(r, g, b, a, text)</code> <code>\u2192 None</code> \u6709\u989c\u8272\u7684\u6587\u672c\u3002 <code>text_wrapped(text)</code> <code>\u2192 None</code> \u81ea\u52a8\u6362\u884c\u6587\u672c\u3002 <code>separator()</code> <code>\u2192 None</code> \u6c34\u5e73\u5206\u9694\u7ebf\u3002 <code>button(label, width, height)</code> <code>\u2192 bool</code> \u6309\u94ae\u3002\u70b9\u51fb\u65f6\u8fd4\u56de <code>True</code>\u3002<code>width</code>/<code>height</code> \u9ed8\u8ba4 <code>0</code>\uff08\u81ea\u52a8\u5927\u5c0f\uff09\u3002 <code>checkbox(label, *, default)</code> <code>\u2192 bool</code> \u5e26\u72b6\u6001\u5207\u6362\u7684\u590d\u9009\u6846\u3002 <code>slider(label, min, max, *, default)</code> <code>\u2192 float</code> \u6d6e\u70b9\u6ed1\u5757\u3002 <code>slider_int(label, min, max, *, default)</code> <code>\u2192 int</code> \u6574\u6570\u6ed1\u5757\u3002 <code>color_picker(label, *, default)</code> <code>\u2192 (r, g, b)</code> \u989c\u8272\u9009\u62e9\u5668\uff083 \u6d6e\u70b9\u5143\u7ec4\uff09\u3002 <code>combo(label, items, *, default)</code> <code>\u2192 int</code> \u4e0b\u62c9\u7ec4\u5408\u6846\u3002\u8fd4\u56de\u9009\u4e2d\u7d22\u5f15\u3002 <code>input_text(label, *, default, max_length)</code> <code>\u2192 str</code> \u6587\u672c\u8f93\u5165\u6846\u3002 <code>plot(values, *, label, overlay, width, height)</code> <code>\u2192 None</code> \u6d6e\u70b9\u5217\u8868\u7684\u6298\u7ebf\u56fe\u3002 <code>progress(fraction, *, overlay)</code> <code>\u2192 None</code> \u8fdb\u5ea6\u6761\uff080.0 \u2013 1.0\uff09\u3002"},{"location":"zh/api/#vultorchcanvas","title":"<code>vultorch.Canvas</code>","text":"<pre><code>class Canvas:\n    # \u901a\u8fc7 Panel.canvas() \u521b\u5efa \u2014 \u4e0d\u76f4\u63a5\u5b9e\u4f8b\u5316\n</code></pre> <p>\u5c06\u7ed1\u5b9a\u5f20\u91cf\u6e32\u67d3\u4e3a ImGui \u56fe\u50cf\u7684\u663e\u793a\u8868\u9762\u3002</p>"},{"location":"zh/api/#_18","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>bind(tensor)</code> <code>\u2192 Canvas</code> \u7ed1\u5b9a\u5f20\u91cf\u7528\u4e8e\u663e\u793a\u3002\u8fd4\u56de <code>self</code> \u4ee5\u652f\u6301\u94fe\u5f0f\u8c03\u7528\u3002 <code>alloc(height, width, channels, device)</code> <code>\u2192 torch.Tensor</code> \u5206\u914d Vulkan \u5171\u4eab\u5185\u5b58\u5e76\u81ea\u52a8\u7ed1\u5b9a\u3002\u8fd4\u56de\u5f20\u91cf\u3002 <code>save(path, *, channels, size, quality)</code> <code>\u2192 None</code> \u901a\u8fc7 <code>imwrite()</code> \u5c06\u7ed1\u5b9a\u7684\u5f20\u91cf\u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6\u3002"},{"location":"zh/api/#_19","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>filter</code> <code>str</code> <code>\"linear\"</code> <code>\"linear\"</code> \u6216 <code>\"nearest\"</code>\u3002 <code>fit</code> <code>bool</code> <code>True</code> \u81ea\u52a8\u586b\u5145\u53ef\u7528\u9762\u677f\u7a7a\u95f4\u3002"},{"location":"zh/api/#imgui-vultorchui","title":"ImGui \u7ed1\u5b9a (<code>vultorch.ui</code>)","text":"<p><code>vultorch.ui</code> \u5b50\u6a21\u5757\u66b4\u9732\u4e86 Dear ImGui \u51fd\u6570\uff08docking \u5206\u652f\uff09\u3002\u6240\u6709\u51fd\u6570\u76f4\u63a5\u6620\u5c04\u5230\u5bf9\u5e94\u7684 ImGui C++ \u63a5\u53e3\u3002</p>"},{"location":"zh/api/#_20","title":"\u7a97\u53e3","text":"<pre><code>ui.begin(name: str, opened: bool = True, flags: int = 0) -&gt; tuple[bool, bool]\nui.end() -&gt; None\nui.begin_child(id: str, width=0.0, height=0.0, child_flags=0, window_flags=0) -&gt; bool\nui.end_child() -&gt; None\n</code></pre>"},{"location":"zh/api/#_21","title":"\u6587\u672c","text":"<pre><code>ui.text(text: str) -&gt; None\nui.text_colored(r, g, b, a, text: str) -&gt; None\nui.text_disabled(text: str) -&gt; None\nui.text_wrapped(text: str) -&gt; None\nui.label_text(label: str, text: str) -&gt; None\nui.bullet_text(text: str) -&gt; None\n</code></pre>"},{"location":"zh/api/#_22","title":"\u6309\u94ae","text":"<pre><code>ui.button(label: str, width=0.0, height=0.0) -&gt; bool\nui.small_button(label: str) -&gt; bool\nui.invisible_button(id: str, width, height) -&gt; bool\nui.arrow_button(id: str, direction: int) -&gt; bool\nui.radio_button(label: str, active: bool) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_23","title":"\u8f93\u5165","text":"<pre><code>ui.checkbox(label, value: bool) -&gt; bool\nui.slider_float(label, value, min=0.0, max=1.0, format=\"%.3f\") -&gt; float\nui.slider_float2(label, v1, v2, min, max) -&gt; tuple[float, float]\nui.slider_float3(label, v1, v2, v3, min, max) -&gt; tuple[float, float, float]\nui.slider_float4(label, v1, v2, v3, v4, min, max) -&gt; tuple\nui.slider_int(label, value, min=0, max=100) -&gt; int\nui.slider_angle(label, value, min=-360, max=360) -&gt; float\nui.drag_float(label, value, speed=1.0) -&gt; float\nui.drag_float2(label, v1, v2, speed=1.0) -&gt; tuple\nui.drag_float3(label, v1, v2, v3, speed=1.0) -&gt; tuple\nui.drag_int(label, value, speed=1.0) -&gt; int\nui.input_float(label, value) -&gt; float\nui.input_float2(label, v1, v2) -&gt; tuple\nui.input_float3(label, v1, v2, v3) -&gt; tuple\nui.input_float4(label, v1, v2, v3, v4) -&gt; tuple\nui.input_int(label, value) -&gt; int\nui.input_text(label, text, max_length=256) -&gt; str\nui.input_text_multiline(label, text, max_length=1024) -&gt; str\n</code></pre>"},{"location":"zh/api/#_24","title":"\u989c\u8272","text":"<pre><code>ui.color_edit3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_edit4(label, r, g, b, a, flags=0) -&gt; tuple[float, float, float, float]\nui.color_picker3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_picker4(label, r, g, b, a, flags=0) -&gt; tuple\n</code></pre>"},{"location":"zh/api/#_25","title":"\u9009\u62e9","text":"<pre><code>ui.combo(label, current: int, items: list[str]) -&gt; int\nui.listbox(label, current: int, items: list[str], height_items=-1) -&gt; int\nui.tree_node(label: str) -&gt; bool\nui.tree_pop() -&gt; None\nui.collapsing_header(label: str) -&gt; bool\nui.selectable(label: str, selected: bool = False) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_26","title":"\u6807\u7b7e\u9875","text":"<pre><code>ui.begin_tab_bar(id: str) -&gt; bool\nui.end_tab_bar() -&gt; None\nui.begin_tab_item(label: str) -&gt; bool\nui.end_tab_item() -&gt; None\n</code></pre>"},{"location":"zh/api/#_27","title":"\u663e\u793a","text":"<pre><code>ui.progress_bar(fraction, sx=-1.0, sy=0.0, overlay=\"\")\nui.image(texture_id: int, width, height, uv0x=0, uv0y=0, uv1x=1, uv1y=1)\nui.image_button(id: str, texture_id: int, width, height) -&gt; bool\nui.plot_lines(label, values: list[float], offset=0, overlay=\"\", ...)\nui.plot_histogram(label, values: list[float], offset=0, overlay=\"\", ...)\n</code></pre>"},{"location":"zh/api/#_28","title":"\u5e03\u5c40","text":"<pre><code>ui.separator()\nui.same_line(offset=0.0, spacing=-1.0)\nui.new_line()\nui.spacing()\nui.dummy(width, height)\nui.indent(width=0.0)\nui.unindent(width=0.0)\nui.begin_group()\nui.end_group()\nui.push_item_width(width)\nui.pop_item_width()\nui.columns(count=1, id=None, border=True)\nui.next_column()\n</code></pre>"},{"location":"zh/api/#_29","title":"\u8868\u683c","text":"<pre><code>ui.begin_table(id: str, columns: int, flags=0) -&gt; bool\nui.end_table()\nui.table_next_row(flags=0, min_row_height=0.0)\nui.table_next_column() -&gt; bool\nui.table_set_column_index(index: int) -&gt; bool\nui.table_setup_column(label: str, flags=0, init_width=0.0)\nui.table_headers_row()\n</code></pre>"},{"location":"zh/api/#_30","title":"\u83dc\u5355","text":"<pre><code>ui.begin_main_menu_bar() -&gt; bool\nui.end_main_menu_bar()\nui.begin_menu_bar() -&gt; bool\nui.end_menu_bar()\nui.begin_menu(label: str, enabled=True) -&gt; bool\nui.end_menu()\nui.menu_item(label: str, shortcut=\"\", selected=False, enabled=True) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_31","title":"\u5f39\u51fa\u7a97\u53e3","text":"<pre><code>ui.open_popup(id: str)\nui.begin_popup(id: str) -&gt; bool\nui.begin_popup_modal(name: str, flags=0) -&gt; bool\nui.end_popup()\nui.close_current_popup()\n</code></pre>"},{"location":"zh/api/#_32","title":"\u63d0\u793a\u6846","text":"<pre><code>ui.begin_tooltip()\nui.end_tooltip()\nui.set_tooltip(text: str)\n</code></pre>"},{"location":"zh/api/#id","title":"ID \u6808","text":"<pre><code>ui.push_id_str(id: str)\nui.push_id_int(id: int)\nui.pop_id()\nui.get_id(id: str) -&gt; int\n</code></pre>"},{"location":"zh/api/#_33","title":"\u6837\u5f0f","text":"<pre><code>ui.push_style_color(idx: int, r, g, b, a)\nui.pop_style_color(count=1)\nui.push_style_var_float(idx: int, value: float)\nui.push_style_var_vec2(idx: int, x: float, y: float)\nui.pop_style_var(count=1)\nui.style_colors_dark()\nui.style_colors_light()\nui.style_colors_classic()\n</code></pre>"},{"location":"zh/api/#_34","title":"\u5149\u6807\u4e0e\u7a97\u53e3\u4fe1\u606f","text":"<pre><code>ui.get_cursor_pos() -&gt; tuple[float, float]\nui.set_cursor_pos(x, y)\nui.get_content_region_avail() -&gt; tuple[float, float]\nui.get_window_size() -&gt; tuple[float, float]\nui.get_window_pos() -&gt; tuple[float, float]\nui.set_next_window_pos(x, y, cond=0)\nui.set_next_window_size(width, height, cond=0)\n</code></pre>"},{"location":"zh/api/#_35","title":"\u505c\u9760","text":"<pre><code>ui.dock_space_over_viewport(flags=0) -&gt; int\nui.dock_space(id: int, sx=0.0, sy=0.0, flags=0) -&gt; int\nui.set_next_window_dock_id(dock_id: int, cond=0)\nui.dock_builder_add_node(node_id=0, flags=0) -&gt; int\nui.dock_builder_remove_node(node_id: int)\nui.dock_builder_set_node_size(node_id, width, height)\nui.dock_builder_set_node_pos(node_id, x, y)\nui.dock_builder_split_node(node_id, split_dir, ratio) -&gt; tuple[int, int]\nui.dock_builder_dock_window(window_name: str, node_id: int)\nui.dock_builder_finish(node_id: int)\nui.dock_builder_get_node(node_id: int) -&gt; int\n</code></pre>"},{"location":"zh/api/#_36","title":"\u7ed8\u56fe","text":"<pre><code>ui.draw_line(x1, y1, x2, y2, col=0xFFFFFFFF, thickness=1.0)\nui.draw_rect(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_rect_filled(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_circle(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_circle_filled(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_text(x, y, col: int, text: str)\nui.bg_draw_image(texture_id, x1, y1, x2, y2)\n</code></pre>"},{"location":"zh/api/#_37","title":"\u8f93\u5165\u72b6\u6001","text":"<pre><code>ui.is_item_hovered() -&gt; bool\nui.is_item_active() -&gt; bool\nui.is_item_clicked() -&gt; bool\nui.is_item_focused() -&gt; bool\nui.is_item_edited() -&gt; bool\nui.is_item_deactivated_after_edit() -&gt; bool\nui.get_mouse_pos() -&gt; tuple[float, float]\nui.is_mouse_clicked(button: int) -&gt; bool\nui.is_mouse_double_clicked(button: int) -&gt; bool\nui.is_mouse_dragging(button: int, lock_threshold=-1.0) -&gt; bool\nui.get_mouse_drag_delta(button=0, lock_threshold=-1.0) -&gt; tuple[float, float]\nui.is_key_pressed(key: int) -&gt; bool\nui.is_key_down(key: int) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_38","title":"\u5de5\u5177\u51fd\u6570","text":"<pre><code>ui.get_io_framerate() -&gt; float\nui.get_io_delta_time() -&gt; float\nui.get_time() -&gt; float\nui.get_frame_count() -&gt; int\nui.get_display_size() -&gt; tuple[float, float]\nui.col32(r: int, g: int, b: int, a: int = 255) -&gt; int\nui.show_demo_window()\nui.show_metrics_window()\n</code></pre>"},{"location":"zh/api/#_39","title":"\u5185\u90e8\u8f85\u52a9\u51fd\u6570","text":""},{"location":"zh/api/#vultorch_normalize_tensor","title":"<code>vultorch._normalize_tensor()</code>","text":"<pre><code>def _normalize_tensor(tensor) -&gt; tuple[Tensor, int, int, int]\n</code></pre> <p>\u89c4\u8303\u5316\u5f20\u91cf\u7684 dtype \u548c\u5f62\u72b6\u4ee5\u7528\u4e8e\u663e\u793a\u3002\u8fd4\u56de <code>(tensor, height, width, channels)</code>\u3002</p> <ul> <li><code>uint8</code> \u2192 <code>float32</code>\uff08\u00f7 255\uff09\uff0c<code>float16</code> \u2192 <code>float32</code>\u3002</li> <li>\u63a5\u53d7 2D <code>(H, W)</code> \u548c 3D <code>(H, W, C)</code>\uff0cC \u2208 {1, 3, 4}\u3002</li> <li>\u4e0d\u652f\u6301\u7684 dtype\u3001\u5f62\u72b6\u6216\u901a\u9053\u6570\u4f1a\u629b\u51fa <code>ValueError</code>\u3002</li> </ul>"}]}