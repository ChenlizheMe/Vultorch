{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vultorch Tutorial","text":"<p>A step-by-step guide through Vultorch, one example at a time. Each chapter maps to a runnable script in the <code>examples/</code> folder.</p> Chapter Topic Key concepts 01 \u2014 Hello Tensor Minimal display View, Panel, Canvas, bind, run 02 \u2014 Multi-Panel Multiple panels &amp; canvases Layout, side, multi-canvas 03 \u2014 Training Test Fit a tiny network to a GT image custom dock layout, create_tensor, per-pixel optimization 04 \u2014 Conway's Game of Life GPU cellular automaton create_tensor for simulation, filter=\"nearest\", sidebar, buttons, color pickers 05 \u2014 Image Viewer Load, transform &amp; save images imread, imwrite, Canvas.save, combo, input_text, filter toggle 06 \u2014 Pixel Canvas Interactive drawing on a GPU tensor mouse interaction, screen\u2192pixel mapping, backing store pattern 07 \u2014 Multi-Channel Viewer RGB + depth + normal + alpha in one window multiple zero-copy tensors, turbo colormap, ray-sphere intersection 08 \u2014 GT vs Prediction Live training comparison with error heatmap error heatmap, PSNR, loss curves, error mode switching 09 \u2014 Live Hyperparameter Tuning Change LR, optimizer, loss at runtime step()/end_step(), log-scale LR, optimizer hot-swap 10 \u2014 2D Gaussian Splatting Differentiable 2D Gaussian rendering nn.Parameter, alpha compositing, cumprod transmittance 11 \u2014 3D Surface Inspector Orbit camera with Blinn-Phong lighting SceneView, Camera, Light, MSAA, procedural textures 12 \u2014 Neural Rendering Workstation Capstone: 6-panel workstation with dual-head MLP Two-head MLP, six panels, pause/resume, snapshot, optimizer hot-swap 13 \u2014 Snake RL DQN learns to play Snake RL visualization, DQN, \u03b5-greedy, Q-value heatmap, manual mode"},{"location":"01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>Example file: <code>examples/01_hello_tensor.py</code></p> <p>Tired of every research repo inventing its own janky tensor viewer with matplotlib hacks and <code>cv2.imshow</code> spaghetti?  Yeah, us too.</p> <p>Vultorch gets your CUDA tensor on screen in four lines \u2014 no saving PNGs, no CPU round-trips, no <code>plt.pause(0.001)</code> nonsense.</p>"},{"location":"01_hello_tensor/#the-mental-model","title":"The mental model","text":"<p>If you've ever used matplotlib, you know this pattern: figure \u2192 axes \u2192 plot. Vultorch is the same idea, but for GPU tensors:</p> <pre><code>View          \u2190 the OS window (like plt.figure)\n \u2514\u2500 Panel     \u2190 a named region inside it (like plt.subplot)\n     \u2514\u2500 Canvas  \u2190 a display slot that shows a tensor (like ax.imshow)\n         \u2514\u2500 bind(tensor)  \u2190 connects data to the slot\n</code></pre> <p>Four objects, that's it:</p> Object What it is One-liner View The OS window <code>vultorch.View(\"title\", w, h)</code> Panel A dockable sub-window inside the View <code>view.panel(\"name\")</code> Canvas A GPU image slot inside a Panel <code>panel.canvas(\"name\")</code> bind() Connects a tensor to a Canvas <code>canvas.bind(t)</code> <p>Chain them together, call <code>run()</code>, done.</p> <p>What's a Panel, exactly?</p> <p>A Panel is a movable, resizable sub-window inside your main window. Think of it like a floating sticky note that you can drag around, snap to the edges, or stack with other panels.  You don't need to manage any of this \u2014 Vultorch auto-arranges them for you.  You'll see more of this in the next chapter.</p>"},{"location":"01_hello_tensor/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\n# A 256\u00d7256 RGB gradient \u2014 any (H,W,C) float32 CUDA tensor works\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # blocks until you close the window\n</code></pre> <p>That's it. No event loop boilerplate, no <code>begin_frame()</code> / <code>end_frame()</code>.</p>"},{"location":"01_hello_tensor/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 we made an RGB gradient on CUDA. Vultorch accepts <code>(H,W)</code>,    <code>(H,W,1)</code>, <code>(H,W,3)</code>, or <code>(H,W,4)</code>, in float32 / float16 / uint8.    It handles RGBA expansion for you.</p> </li> <li> <p>Object tree \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>.    The canvas auto-fills its panel by default (<code>fit=True</code>), meaning    it stretches to use all available space \u2014 like a single <code>imshow</code>    that fills the whole figure.</p> </li> <li> <p>Run \u2014 <code>view.run()</code> is a blocking loop (like <code>plt.show()</code>).    It keeps the window open, re-draws the tensor every frame (~60 times    per second), and handles OS events (resize, close, etc.) for you.    Close the window to exit \u2014 your Python script resumes after <code>run()</code>    returns.</p> </li> </ol> <p>Tip</p> <p>The four setup lines collapse into a one-liner if you're feeling fancy: <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p> <p>Why not just use <code>plt.imshow</code>?</p> <p>matplotlib copies your tensor to CPU, converts it to a numpy array, renders it on the CPU via Agg, then blits it to a window.  Vultorch keeps everything on the GPU \u2014 the tensor goes straight from CUDA memory to your screen via Vulkan.  That's why it can refresh at 60 FPS even for large images.</p>"},{"location":"02_multi_panel/","title":"02 \u2014 Multi-Panel","text":"<p>Example file: <code>examples/02_imgui_controls.py</code></p> <p>One panel is nice. But in practice you want to see your loss map, gradient field, and output side-by-side \u2014 without writing a single line of layout code.</p> <p>Good news: Vultorch panels auto-arrange themselves. You just create them, and they stack up inside the window like slides in a presentation. Want a panel on the right? Pass <code>side=\"right\"</code> and Vultorch splits the window for you \u2014 no CSS, no grid coordinates, no fighting with subplot indices.</p> <p>Even better: at runtime you can drag any panel's title bar to rearrange, resize by dragging edges, or pull a panel out into its own floating window. It's like a tiling window manager that you didn't have to configure.</p> <p>This chapter shows two patterns:</p> <ul> <li>One canvas per panel \u2014 three panels stacked, each with its own tensor.</li> <li>Multiple canvases in one panel \u2014 one panel, three canvases, auto-split.</li> </ul>"},{"location":"02_multi_panel/#layout","title":"Layout","text":"<p>Here's what the window looks like:</p> Left side (main area) Right side (<code>side=\"right\"</code>) Red panel \u2014 <code>red_img</code> Combined panel Green panel \u2014 <code>green_img</code> <code>c_red</code> canvas Blue panel \u2014 <code>blue_img</code> <code>c_green</code> canvas <code>c_blue</code> canvas <p>Left: 3 separate panels, each one canvas. Right: 1 panel, 3 canvases sharing space.</p>"},{"location":"02_multi_panel/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# Three different tensors\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# Left: 3 panels, each with one canvas\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# Right: 1 panel with 3 canvases (auto-split vertically)\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"02_multi_panel/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Auto-layout \u2014 panels you create without <code>side=</code> stack vertically    in the main area.  Think of it as <code>plt.subplot(3, 1, ...)</code> but    without counting rows and columns.</p> </li> <li> <p><code>side=\"right\"</code> + <code>width=0.5</code> \u2014 this tells Vultorch: \"split the    window and give this panel the right 50%.\"  The value <code>0.5</code> is a    ratio, not pixels.  <code>width=0.3</code> means 30% of the window.    You can also use <code>\"left\"</code>, <code>\"top\"</code>, or <code>\"bottom\"</code>.</p> </li> <li> <p>Multi-canvas \u2014 call <code>panel.canvas()</code> multiple times.    When several canvases have <code>fit=True</code> (the default), they evenly    split the panel's height.  <code>fit=True</code> just means \"stretch to fill    available space\" \u2014 like how a single <code>imshow</code> fills the whole axes.    No manual height math.</p> </li> <li> <p>Still no callback \u2014 static data only needs <code>bind()</code> + <code>run()</code>.    Dynamic updates come in a later chapter.</p> </li> <li> <p>Drag &amp; drop \u2014 try it: grab a panel's title bar and drag it to    another edge. Pull it out into a floating window.  Everything is    rearrangeable at runtime with your mouse.</p> </li> </ol> <p>Note</p> <p>The same tensor can be bound to multiple canvases at once \u2014 <code>red</code> appears in both the left panel and the right combined panel.</p>"},{"location":"03_training_test/","title":"03 \u2014 Training Test","text":"<p>Example file: <code>examples/03_training_test.py</code></p> <p>Ever stared at a wall of decreasing loss numbers in your terminal for ten minutes, feeling confident, only to discover the model's output is a solid grey rectangle? Yeah, us too.</p> <p>Reading loss values off a scrolling console is about as reliable as reading tea leaves. This chapter puts GT and prediction side by side on screen so you can see whether the network is actually learning.</p>"},{"location":"03_training_test/#what-were-building","title":"What we're building","text":"<p>A tiny MLP (2 \u2192 64 \u2192 64 \u2192 3) fitting a 256\u00d7256 PyTorch logo in real time. The window has three panels:</p> Area Content Left GT panel \u2014 the target image (what you're fitting) Right Prediction panel \u2014 live network output, updated every frame Bottom Info panel \u2014 FPS, loss, iteration, progress bar, and a slider <p>Everything on screen, nothing buried in the terminal.</p>"},{"location":"03_training_test/#new-friends","title":"New friends","text":"<p>Chapters 01 and 02 were all static \u2014 <code>bind()</code> + <code>run()</code>, done. This time we make things move:</p> New thing What it does How to use @view.on_frame A function that runs once per frame \u2014 put your training step here <code>@view.on_frame</code> @panel.on_frame A function that runs inside a specific panel \u2014 put interactive controls here <code>@info_panel.on_frame</code> create_tensor Allocates a CUDA tensor that shares memory with the display, so updates appear on screen instantly <code>vultorch.create_tensor(H, W, ...)</code> vultorch.imread Load an image file into a CUDA tensor (no PIL needed) <code>vultorch.imread(path)</code> side=\"bottom\" Place a panel at the bottom of the window <code>view.panel(\"Info\", side=\"bottom\")</code> <p>What's a widget?</p> <p>In UI terminology, a widget is any interactive element \u2014 a button, a slider, a text label, a progress bar.  Things you can see and (sometimes) click on.  In Vultorch, you create widgets by calling methods like <code>panel.text(\"hello\")</code>, <code>panel.slider(\"x\", 0, 1)</code>, etc. inside a <code>@panel.on_frame</code> callback.  No HTML, no CSS, no Qt \u2014 just Python method calls.</p> <p>Write PyTorch code inside the view callback; put widgets inside the panel callback.  Vultorch handles the tensor-to-screen dance every frame.</p>"},{"location":"03_training_test/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# -- View + panels (high-level declarative API) -------------------------\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ninfo_panel = view.panel(\"Info\", side=\"bottom\", width=0.28)\ngt_panel = view.panel(\"GT\", side=\"left\", width=0.5)\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# 4 channels \u2014 zero-copy GPU display path\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n}\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n\n@info_panel.on_frame\ndef draw_info():\n    info_panel.text(f\"FPS: {view.fps:.1f}\")\n    info_panel.text(f\"Iteration: {state['iter']}\")\n    info_panel.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    info_panel.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = info_panel.slider_int(\n        \"Steps / Frame\", 1, 32, default=6\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    info_panel.progress(progress,\n                        overlay=f\"Training progress {progress * 100:.1f}%\")\n    info_panel.text_wrapped(\n        \"Left is GT, right is prediction. \"\n        \"Increase 'Steps / Frame' for faster fitting.\"\n    )\n\n\nview.run()\n</code></pre> <p>That's it. Run it and watch the grey blob on the right morph into the PyTorch logo in a few seconds.</p>"},{"location":"03_training_test/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Data \u2014 <code>vultorch.imread</code> loads the image straight into a float32    CUDA tensor (no PIL, no numpy). Pixel coordinates get <code>meshgrid</code>'d into    <code>(H*W, 2)</code>, normalized to <code>[-1, 1]</code>.</p> </li> <li> <p>Model \u2014 a two-hidden-layer MLP (64 wide). Takes <code>(x, y)</code>, outputs    <code>(r, g, b)</code>. Small enough to run inside a per-frame callback without    tanking your framerate.</p> </li> <li> <p>Layout \u2014 <code>side=\"bottom\", width=0.28</code> puts the Info panel at the    bottom and gives it 28% of the window height.  (Yes, <code>width=</code> controls    height when the panel is at the bottom \u2014 it's the size along the    split direction.)  <code>side=\"left\", width=0.5</code> puts GT on the left    half of the remaining space.  Prediction fills whatever is left.</p> </li> <li> <p>Two callbacks:</p> <ul> <li> <p><code>@view.on_frame</code> \u2014 runs once per frame before panels are drawn.   This is where you put your training loop, data mutation,   model updates \u2014 any computation.</p> </li> <li> <p><code>@info_panel.on_frame</code> \u2014 runs inside the Info panel's drawing   context.  Every <code>panel.text()</code>, <code>panel.slider_int()</code>,   <code>panel.progress()</code> call you make here creates a widget (text   label, slider, progress bar) inside that specific panel.   You don't need to worry about positioning \u2014 widgets just   stack top-to-bottom, like <code>print()</code> statements.</p> </li> </ul> </li> </ol>"},{"location":"03_training_test/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 a plain Python function that runs once per    displayed frame (~60 times/second).  Put any PyTorch code in here.    At the end of each frame, Vultorch uploads every bound tensor to    the screen automatically.</p> </li> <li> <p><code>create_tensor</code> \u2014 looks and feels like <code>torch.zeros</code>, but the    underlying memory is Vulkan/CUDA shared.  When you write into it,    the changes appear on screen the next frame with zero copy \u2014 no    <code>.cpu()</code>, no <code>upload()</code>, nothing.</p> </li> <li> <p>Layout shorthand \u2014 <code>side=\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> /    <code>\"top\"</code> splits the window, and <code>width=</code> controls how big that    split is (as a 0\u20131 ratio).  That's it.  No coordinates, no grids.</p> </li> <li> <p>Panel widgets \u2014 <code>@panel.on_frame</code> runs inside a panel.    Call <code>panel.text()</code>, <code>panel.slider_int()</code>, <code>panel.progress()</code> \u2014    each call creates one interactive element, stacked top-to-bottom    like lines of <code>print()</code> output.</p> </li> <li> <p>No terminal spam \u2014 all live stats live in the Info panel.    Your console stays clean for warnings and tracebacks.</p> </li> </ol> <p>Tip</p> <p>Crank <code>Steps / Frame</code> up to 32 for blazing-fast convergence. But don't get too greedy \u2014 go too high and your framerate will drop because each frame spends more time training.</p> <p>Note</p> <p><code>create_tensor</code> is called once at init, not every frame. After that you just write into the tensor each frame \u2014 practically free.</p>"},{"location":"04_conway/","title":"04 \u2014 Conway's Game of Life","text":"<p>Example file: <code>examples/04_conway.py</code></p> <p>Loss curves, training visualizations \u2014 all very serious. Let's take a break and build something fun: Conway's Game of Life, running entirely on the GPU, displayed in zero-copy, with buttons and sliders to play god.</p> <p>More importantly, this chapter shows that <code>create_tensor</code> isn't only for neural networks. Any GPU computation can be displayed through Vultorch's shared memory \u2014 simulations, procedural generation, physics, anything that lives on CUDA.</p>"},{"location":"04_conway/#what-were-building","title":"What we're building","text":"<p>A 256\u00d7256 cellular automaton with a control panel:</p> Area Content Left Controls \u2014 play/pause, step, speed slider, pattern presets, color pickers Right Grid \u2014 the simulation, pixel-perfect (<code>filter=\"nearest\"</code>) <p>Everything runs on the GPU. The display tensor uses <code>create_tensor</code> for zero-copy \u2014 the grid never touches the CPU.</p>"},{"location":"04_conway/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>filter=\"nearest\"</code> Shows each pixel as a sharp square, no blurring Without it, bilinear interpolation smudges cell boundaries <code>side=\"left\"</code> sidebar Places a panel on the left, taking 22% of the window Gives you a permanent control strip next to your visualization <code>@panel.on_frame</code> Per-panel widget callback Widget calls (<code>button</code>, <code>slider</code>, <code>color_picker</code>) go inside here <code>panel.button(label)</code> A clickable button Returns <code>True</code> on the frame it was clicked <code>with panel.row():</code> Puts the next widgets on the same line Without it, widgets stack vertically (one per line).  Use <code>row()</code> to put two buttons side-by-side. It's a Python <code>with</code> block \u2014 everything inside the block goes on one line <code>panel.color_picker</code> An RGB color picker widget Click the colored square to open a palette Circular padding + conv2d GPU-parallel neighbour count The whole simulation is one convolution"},{"location":"04_conway/#the-simulation-trick","title":"The simulation trick","text":"<p>Counting neighbours in Conway's Game of Life is just a 2D convolution with a 3\u00d73 kernel of all ones (center zero):</p> <pre><code>1 1 1\n1 0 1\n1 1 1\n</code></pre> <p>PyTorch's <code>F.conv2d</code> does this in one GPU kernel call \u2014 no loops, no per-cell logic. Circular padding wraps the edges so gliders fly off one side and reappear on the other.</p> <pre><code>kernel = torch.tensor([[1, 1, 1],\n                        [1, 0, 1],\n                        [1, 1, 1]], dtype=torch.float32, device=device)\npadded = F.pad(inp, (1, 1, 1, 1), mode='circular')\nneighbours = F.conv2d(padded, kernel.reshape(1, 1, 3, 3)).squeeze()\n</code></pre> <p>Then the rules are just two boolean masks:</p> <pre><code>survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\nbirth   = (grid == 0) &amp; (neighbours == 3)\ngrid[:] = (survive | birth).float()\n</code></pre>"},{"location":"04_conway/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 Grid parameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGRID_H, GRID_W = 256, 256\n\n# \u2500\u2500 View + panels \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"04 - Conway's Game of Life\", 1024, 768)\ngrid_panel = view.panel(\"Grid\")\nctrl_panel = view.panel(\"Controls\", side=\"left\", width=0.22)\n\n# \u2500\u2500 Display tensor (RGBA, zero-copy) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndisplay = vultorch.create_tensor(GRID_H, GRID_W, channels=4,\n                                 device=device, name=\"grid\",\n                                 window=view.window)\ncanvas = grid_panel.canvas(\"grid\", filter=\"nearest\")\ncanvas.bind(display)\n\n# \u2500\u2500 Simulation state \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ngrid = torch.zeros(GRID_H, GRID_W, dtype=torch.float32, device=device)\n\nstate = {\n    \"running\": False,\n    \"generation\": 0,\n    \"speed\": 1,\n    \"prob\": 0.3,\n    \"alive_color\": (0.0, 1.0, 0.4),\n    \"dead_color\": (0.05, 0.05, 0.08),\n}\n\n\ndef randomize():\n    grid[:] = (torch.rand(GRID_H, GRID_W, device=device) &lt; state[\"prob\"]).float()\n    state[\"generation\"] = 0\n\ndef clear():\n    grid.zero_()\n    state[\"generation\"] = 0\n\ndef step_simulation():\n    kernel = torch.tensor([[1, 1, 1],\n                            [1, 0, 1],\n                            [1, 1, 1]], dtype=torch.float32, device=device)\n    inp = grid.unsqueeze(0).unsqueeze(0)\n    k = kernel.unsqueeze(0).unsqueeze(0)\n    padded = torch.nn.functional.pad(inp, (1, 1, 1, 1), mode='circular')\n    neighbours = torch.nn.functional.conv2d(padded, k).squeeze()\n\n    survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\n    birth = (grid == 0) &amp; (neighbours == 3)\n    grid[:] = (survive | birth).float()\n    state[\"generation\"] += 1\n\ndef grid_to_display():\n    alive_r, alive_g, alive_b = state[\"alive_color\"]\n    dead_r, dead_g, dead_b = state[\"dead_color\"]\n    display[:, :, 0] = dead_r + (alive_r - dead_r) * grid\n    display[:, :, 1] = dead_g + (alive_g - dead_g) * grid\n    display[:, :, 2] = dead_b + (alive_b - dead_b) * grid\n    display[:, :, 3] = 1.0\n\nrandomize()\n\n\n@view.on_frame\ndef update():\n    if state[\"running\"]:\n        for _ in range(state[\"speed\"]):\n            step_simulation()\n    grid_to_display()\n\n\n@ctrl_panel.on_frame\ndef draw_controls():\n    ctrl_panel.text(f\"Generation: {state['generation']}\")\n    ctrl_panel.text(f\"Alive cells: {int(grid.sum().item())}\")\n    ctrl_panel.text(f\"FPS: {view.fps:.1f}\")\n    ctrl_panel.separator()\n\n    with ctrl_panel.row():\n        label = \"Pause\" if state[\"running\"] else \"Play\"\n        if ctrl_panel.button(label, width=80):\n            state[\"running\"] = not state[\"running\"]\n        if ctrl_panel.button(\"Step\", width=80):\n            step_simulation()\n\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Randomize\", width=80):\n            randomize()\n        if ctrl_panel.button(\"Clear\", width=80):\n            clear()\n\n    ctrl_panel.separator()\n    state[\"speed\"] = ctrl_panel.slider_int(\"Speed\", 1, 20, default=1)\n    state[\"prob\"] = ctrl_panel.slider(\"Cell Probability\", 0.05, 0.8,\n                                       default=0.3)\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Colors\")\n    state[\"alive_color\"] = ctrl_panel.color_picker(\n        \"Alive\", default=(0.0, 1.0, 0.4))\n    state[\"dead_color\"] = ctrl_panel.color_picker(\n        \"Dead\", default=(0.05, 0.05, 0.08))\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Patterns\")\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Glider\", width=80):\n            clear()\n            grid[1, 2] = 1; grid[2, 3] = 1\n            grid[3, 1] = 1; grid[3, 2] = 1; grid[3, 3] = 1\n        if ctrl_panel.button(\"Pulsar\", width=80):\n            clear()\n            # ... place pulsar pattern ...\n        if ctrl_panel.button(\"Gosper Gun\", width=100):\n            clear()\n            # ... place Gosper glider gun ...\n\n    ctrl_panel.separator()\n    ctrl_panel.text_wrapped(\n        \"Click Play to start, or Step to advance one generation. \"\n        \"Use Randomize to reset with random cells.\"\n    )\n\n\nview.run()\n</code></pre> <p>(The full example file includes helper functions for all pattern placements.)</p>"},{"location":"04_conway/#what-just-happened","title":"What just happened?","text":"<ol> <li> <p>Grid \u2014 a plain <code>(256, 256)</code> float32 CUDA tensor.  <code>1.0</code> = alive,    <code>0.0</code> = dead.  No classes, no fancy data structures \u2014 just a tensor.</p> </li> <li> <p>Simulation \u2014 <code>step_simulation()</code> uses <code>F.conv2d</code> with circular    padding to count neighbours, then applies the birth/survive rules    with boolean masks.  The entire generation runs in two GPU kernels.</p> </li> <li> <p>Display \u2014 <code>create_tensor</code> allocates shared Vulkan/CUDA memory.    <code>grid_to_display()</code> lerps between dead and alive colors and writes    into it.  Zero copy to screen.</p> </li> <li> <p>Controls \u2014 <code>@ctrl_panel.on_frame</code> draws all widgets inside the    Controls panel.  <code>panel.button()</code>, <code>panel.slider_int()</code>,    <code>panel.color_picker()</code>, and <code>with panel.row()</code> keep the layout    compact.  State lives in a plain Python dict.</p> </li> </ol>"},{"location":"04_conway/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>create_tensor</code> is for everything \u2014 not just neural networks.    Any GPU computation that produces an image-like tensor can be    displayed with zero copy.</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014 crucial for pixel-art / grid simulations.    Without it, bilinear interpolation blurs cell boundaries.  Think of    it like <code>plt.imshow(data, interpolation='nearest')</code> \u2014 same idea,    you want to see the actual pixels.</p> </li> <li> <p>Convolution = neighbour counting \u2014 a cute trick that replaces    nested Python loops with a single GPU kernel.  The game runs at    hundreds of FPS even at large grid sizes.</p> </li> <li> <p>Panel widgets \u2014 inside <code>@panel.on_frame</code> you call    <code>panel.button()</code>, <code>panel.slider_int()</code>, <code>panel.color_picker()</code>.    Each call creates one interactive element.  They stack top-to-bottom    automatically, like lines of <code>print()</code> output.  No positioning    code needed.</p> </li> <li> <p><code>with panel.row():</code> \u2014 by default widgets go one-per-line.    Wrap several widget calls in <code>with panel.row():</code> to put them on    the same line instead.  It's just a Python <code>with</code> block \u2014    nothing exotic.</p> </li> <li> <p>Pattern presets \u2014 the Glider, Pulsar, and Gosper Gun buttons    demonstrate how to set initial conditions by writing directly into    the grid tensor.</p> </li> </ol> <p>Tip</p> <p>Crank the Speed slider to 20 and watch the grid evolve at 20 generations per frame.  On a modern GPU you'll still hold 60+ FPS.</p> <p>Note</p> <p>The grid wraps around thanks to <code>mode='circular'</code> padding. Gliders that fly off the right edge reappear on the left.</p>"},{"location":"05_image_viewer/","title":"05 \u2014 Image Viewer","text":"<p>Example file: <code>examples/05_image_viewer.py</code></p> <p>So far we've been manufacturing tensors from thin air \u2014 gradients, checkerboards, neural network outputs. Very impressive, but at some point you probably want to look at an actual image. You know, the kind that lives on your hard drive. In a <code>.png</code> file. Like a normal person.</p> <p>\"Just use PIL,\" you say. Sure \u2014 and then <code>torchvision.transforms</code>, and then <code>numpy</code>, and then <code>cv2.cvtColor</code> because someone mixed up RGB and BGR again, and then you're three Stack Overflow tabs deep wondering why everything is upside-down and slightly green.</p> <p>Vultorch has built-in image I/O. One function in, one function out. No PIL, no OpenCV, no existential dread.</p>"},{"location":"05_image_viewer/#new-friends","title":"New friends","text":"New thing What it does How to use imread Load a file into a CUDA tensor <code>vultorch.imread(\"photo.png\")</code> imwrite Save a tensor to a file <code>vultorch.imwrite(\"out.png\", t)</code> Canvas.save() Save the canvas's bound tensor <code>canvas.save(\"out.png\")</code> panel.combo() Drop-down selector <code>panel.combo(\"Pick\", [\"A\",\"B\"])</code> panel.input_text() Text input field <code>panel.input_text(\"Path\")</code> canvas.filter Sampling mode (<code>\"linear\"</code> / <code>\"nearest\"</code>) <code>canvas.filter = \"nearest\"</code>"},{"location":"05_image_viewer/#what-were-building","title":"What we're building","text":"<p>A mini image viewer: load a photo, pick a transform from a drop-down, tweak brightness / contrast with sliders, and save the result.</p> Left Right (two canvases) Controls \u2014 transform combo, brightness/contrast sliders, filter toggle, save Original (top) Transformed (bottom)"},{"location":"05_image_viewer/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 Load image \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\noriginal = vultorch.imread(img_path, channels=3, device=device)\nH, W, C = original.shape\n\n# Working copy for transforms\ntransformed = original.clone()\n\n# \u2500\u2500 View + panels \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"05 - Image Viewer\", 1024, 768)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nimg_panel = view.panel(\"Image\")\n\ncanvas_orig = img_panel.canvas(\"Original\")\ncanvas_orig.bind(original)\n\ncanvas_xform = img_panel.canvas(\"Transformed\")\ncanvas_xform.bind(transformed)\n\n# \u2500\u2500 State \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRANSFORMS = [\n    \"None\",\n    \"Horizontal Flip\",\n    \"Vertical Flip\",\n    \"Grayscale\",\n    \"Invert\",\n    \"Sepia\",\n]\n\nstate = {\n    \"brightness\": 0.0,\n    \"contrast\": 1.0,\n    \"last_transform\": -1,\n    \"last_brightness\": None,\n    \"last_contrast\": None,\n}\n\n\ndef apply_transform(img, idx):\n    if idx == 0:    return img.clone()\n    elif idx == 1:  return img.flip(1)               # horizontal flip\n    elif idx == 2:  return img.flip(0)               # vertical flip\n    elif idx == 3:                                    # grayscale\n        gray = img[:,:,0]*0.299 + img[:,:,1]*0.587 + img[:,:,2]*0.114\n        return gray.unsqueeze(-1).expand_as(img).contiguous()\n    elif idx == 4:  return 1.0 - img                 # invert\n    elif idx == 5:                                    # sepia\n        r = img[:,:,0]*0.393 + img[:,:,1]*0.769 + img[:,:,2]*0.189\n        g = img[:,:,0]*0.349 + img[:,:,1]*0.686 + img[:,:,2]*0.168\n        b = img[:,:,0]*0.272 + img[:,:,1]*0.534 + img[:,:,2]*0.131\n        return torch.stack([r, g, b], dim=-1).clamp(0, 1)\n    return img.clone()\n\n\ndef apply_brightness_contrast(img, brightness, contrast):\n    return ((img - 0.5) * contrast + 0.5 + brightness).clamp(0, 1)\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Image: {img_path.name}\")\n    ctrl.text(f\"Size: {W} \u00d7 {H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    # Transform selector\n    ctrl.text(\"Transform\")\n    xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n\n    ctrl.separator()\n\n    # Brightness / Contrast\n    ctrl.text(\"Adjustments\")\n    brightness = ctrl.slider(\"Brightness\", -1.0, 1.0, default=0.0)\n    contrast   = ctrl.slider(\"Contrast\",    0.0, 3.0, default=1.0)\n\n    changed = (xform_idx != state[\"last_transform\"]\n               or brightness != state[\"last_brightness\"]\n               or contrast   != state[\"last_contrast\"])\n\n    if changed:\n        result = apply_transform(original, xform_idx)\n        result = apply_brightness_contrast(result, brightness, contrast)\n        transformed[:] = result\n        state[\"last_transform\"]  = xform_idx\n        state[\"last_brightness\"] = brightness\n        state[\"last_contrast\"]   = contrast\n\n    ctrl.separator()\n\n    # Filter toggle\n    ctrl.text(\"Sampling Filter\")\n    filter_idx = ctrl.combo(\"##filter\", [\"Linear\", \"Nearest\"], default=0)\n    canvas_orig.filter  = \"nearest\" if filter_idx == 1 else \"linear\"\n    canvas_xform.filter = \"nearest\" if filter_idx == 1 else \"linear\"\n\n    ctrl.separator()\n\n    # Save\n    ctrl.text(\"Save Output\")\n    save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n\n    if ctrl.button(\"Save Image\", width=140):\n        try:\n            canvas_xform.save(save_path)\n            state[\"save_msg\"] = f\"Saved to {save_path}\"\n        except Exception as e:\n            state[\"save_msg\"] = f\"Error: {e}\"\n\n    if \"save_msg\" in state:\n        ctrl.text_wrapped(state[\"save_msg\"])\n\n\nview.run()\n</code></pre>"},{"location":"05_image_viewer/#what-just-happened","title":"What just happened?","text":""},{"location":"05_image_viewer/#imread-images-without-the-dependency-hell","title":"imread \u2014 images without the dependency hell","text":"<pre><code>original = vultorch.imread(img_path, channels=3, device=device)\n</code></pre> <p>One line. Returns a <code>(H, W, 3)</code> float32 CUDA tensor with values in <code>[0, 1]</code>. Supports PNG, JPEG, BMP, TGA, HDR, PSD, and GIF (first frame). Uses <code>stb_image</code> under the hood \u2014 no Python image library needed.</p> <p>Optional parameters:</p> <ul> <li><code>channels=4</code> \u2014 force RGBA output.</li> <li><code>size=(256, 256)</code> \u2014 resize after loading (bilinear interpolation).</li> <li><code>device=\"cpu\"</code> \u2014 keep it on CPU if you prefer.</li> <li><code>shared=True</code> \u2014 allocate via <code>create_tensor</code> for zero-copy display.</li> </ul>"},{"location":"05_image_viewer/#combo-the-drop-down-menu","title":"combo \u2014 the drop-down menu","text":"<pre><code>xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n</code></pre> <p>Shows a drop-down with the items in the list. Returns the index (int) of the selected item. The state is managed automatically by the panel \u2014 just pass <code>default=</code> for the initial selection.</p> <p>The <code>##</code> prefix hides the label in ImGui (the text after <code>##</code> is used as an internal ID only). Useful when you don't want a label next to your widget.</p>"},{"location":"05_image_viewer/#input_text-free-text-entry","title":"input_text \u2014 free text entry","text":"<pre><code>save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n</code></pre> <p>Returns the current string. Type a filename, hit Enter or click Save. <code>max_length=256</code> by default \u2014 plenty for a file path.</p>"},{"location":"05_image_viewer/#canvassave-one-line-export","title":"Canvas.save() \u2014 one-line export","text":"<pre><code>canvas_xform.save(save_path)\n</code></pre> <p>Saves whatever tensor is currently bound to the canvas. The file format is inferred from the extension (<code>.png</code>, <code>.jpg</code>, <code>.bmp</code>, <code>.tga</code>, <code>.hdr</code>). Under the hood it calls <code>vultorch.imwrite()</code>.</p>"},{"location":"05_image_viewer/#filter-nearest-vs-linear","title":"filter \u2014 nearest vs linear","text":"<pre><code>canvas_orig.filter = \"nearest\"   # pixel-perfect, blocky when zoomed\ncanvas_orig.filter = \"linear\"    # bilinear interpolation, smooth\n</code></pre> <p>Switch the sampling filter at any time. Try toggling it when the image is stretched \u2014 <code>\"nearest\"</code> shows you the raw pixels, <code>\"linear\"</code> blurs them into smooth gradients. For scientific visualization (segmentation masks, attention maps) you almost always want <code>\"nearest\"</code>.</p>"},{"location":"05_image_viewer/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>imread</code> / <code>imwrite</code> \u2014 zero-dependency image I/O. Reads straight    into a CUDA tensor, writes straight from one. No PIL, no numpy, no    <code>cv2.cvtColor</code> misadventures.</p> </li> <li> <p><code>combo</code> \u2014 drop-down selection. Returns an int index. Perfect for    mode switches, preset selectors, enum-style choices.</p> </li> <li> <p><code>input_text</code> \u2014 free-form string input. Useful for file paths,    model names, experiment tags.</p> </li> <li> <p><code>Canvas.save()</code> \u2014 save the bound tensor to disk in one call.    Extension determines the format.</p> </li> <li> <p>Lazy recomputation \u2014 we only re-run the transform when a slider    or combo value actually changes. Checking <code>changed</code> before doing    tensor ops avoids wasting GPU cycles every frame.</p> </li> </ol> <p>Tip</p> <p><code>imread</code> supports a <code>size=(H, W)</code> argument for resizing at load time. Useful when your image is 4K but you only need a 256\u00d7256 preview.</p> <p>Note</p> <p><code>imwrite</code> accepts float32 tensors in <code>[0, 1]</code> as well as uint8 tensors in <code>[0, 255]</code>. It handles the conversion automatically.</p>"},{"location":"06_pixel_canvas/","title":"06 \u2014 Pixel Canvas","text":"<p>Example file: <code>examples/06_pixel_canvas.py</code></p> <p>So far every tensor we've displayed was read-only from the viewer's perspective \u2014 the GPU computes something, Vultorch shows it, end of story. But what if the user wants to paint into the tensor? With the mouse? In real time?</p> <p>That's what this chapter is about: turning a zero-copy GPU tensor into an interactive drawing surface. Left-click to paint, right-click to erase, pick your color, resize your brush \u2014 done.</p> <p>The magic is that there is no magic. <code>create_tensor</code> gives you a normal <code>torch.Tensor</code> on CUDA. You write pixels into it with standard indexing (<code>tensor[y, x, :3] = color</code>). Because it's zero-copy, the display updates without any upload call.</p>"},{"location":"06_pixel_canvas/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>ui.get_mouse_pos()</code> Returns <code>(x, y)</code> of the mouse cursor in screen pixels You need this to know where on the canvas the user is pointing <code>ui.is_item_hovered()</code> <code>True</code> if the mouse is over the last-drawn widget (the canvas image) So you only paint when the cursor is on the canvas, not the controls <code>ui.is_mouse_clicked(0)</code> <code>True</code> on the frame the left button is pressed Detect a single click <code>ui.is_mouse_dragging(0)</code> <code>True</code> while the left button is held down and the mouse moves Continuous painting \u2014 fires every frame while you drag Screen \u2192 pixel mapping Convert screen coordinates to tensor <code>[y, x]</code> index The canvas image is stretched to fill the panel; you need to undo that stretch to find which pixel the mouse is over"},{"location":"06_pixel_canvas/#what-were-building","title":"What we're building","text":"<p>A 128\u00d7128 pixel canvas: left-click draws, right-click erases, with a sidebar for brush size, colors, clear button, and a grid overlay toggle.</p>"},{"location":"06_pixel_canvas/#full-code","title":"Full code","text":"<pre><code>import torch\nimport vultorch\nfrom vultorch import ui\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nH, W = 128, 128\n\nview = vultorch.View(\"06 - Pixel Canvas\", 900, 700)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.24)\ndraw_panel = view.panel(\"Canvas\")\n\n# Zero-copy RGBA tensor\ncanvas_tensor = vultorch.create_tensor(H, W, channels=4, device=device,\n                                        name=\"canvas\", window=view.window)\ncanvas_tensor[:, :, :3] = 0.1\ncanvas_tensor[:, :, 3] = 1.0\ncanvas = draw_panel.canvas(\"canvas\", filter=\"nearest\", fit=True)\ncanvas.bind(canvas_tensor)\n\n# Persistent backing store (so grid overlay doesn't accumulate)\nbacking = torch.zeros(H, W, 3, device=device)\nbacking[:] = 0.1\n\nstate = {\n    \"brush_size\": 1,\n    \"brush_color\": (1.0, 0.3, 0.1),\n    \"show_grid\": False,\n    \"bg_color\": (0.1, 0.1, 0.1),\n}\n\n\ndef draw_brush(cy, cx, size, r, g, b):\n    half = size // 2\n    y0, y1 = max(0, cy - half), min(H, cy + half + 1)\n    x0, x1 = max(0, cx - half), min(W, cx + half + 1)\n    backing[y0:y1, x0:x1, 0] = r\n    backing[y0:y1, x0:x1, 1] = g\n    backing[y0:y1, x0:x1, 2] = b\n\n\ndef clear_canvas():\n    r, g, b = state[\"bg_color\"]\n    backing[:, :, 0] = r\n    backing[:, :, 1] = g\n    backing[:, :, 2] = b\n\n\ndef refresh_display():\n    canvas_tensor[:, :, :3] = backing\n    if state[\"show_grid\"]:\n        for i in range(0, H, 8):\n            canvas_tensor[i, :, :3] = canvas_tensor[i, :, :3].clamp(0, 0.85) + 0.15\n        for j in range(0, W, 8):\n            canvas_tensor[:, j, :3] = canvas_tensor[:, j, :3].clamp(0, 0.85) + 0.15\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Canvas: {W}\u00d7{H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    state[\"brush_size\"] = ctrl.slider_int(\"Brush Size\", 1, 16, default=1)\n    state[\"brush_color\"] = ctrl.color_picker(\"Brush Color\",\n                                              default=(1.0, 0.3, 0.1))\n    state[\"bg_color\"] = ctrl.color_picker(\"Background\",\n                                           default=(0.1, 0.1, 0.1))\n    ctrl.separator()\n\n    if ctrl.button(\"Clear\", width=120):\n        clear_canvas()\n\n    state[\"show_grid\"] = ctrl.checkbox(\"Show Grid (8px)\", default=False)\n\n    ctrl.separator()\n    ctrl.text_wrapped(\n        \"Left-click to draw. Right-click to erase. \"\n        \"Adjust brush size and color above.\"\n    )\n\n\n@draw_panel.on_frame\ndef handle_drawing():\n    if not ui.is_item_hovered():\n        refresh_display()\n        return\n\n    mx, my = ui.get_mouse_pos()\n\n    # Map screen position to tensor pixel coordinates\n    wp_x, wp_y = ui.get_window_pos()\n    win_w, win_h = ui.get_window_size()\n    content_x = wp_x + 8\n    content_y = wp_y + 26\n    content_w = win_w - 16\n    content_h = win_h - 34\n\n    u = max(0.0, min(1.0, (mx - content_x) / max(content_w, 1)))\n    v = max(0.0, min(1.0, (my - content_y) / max(content_h, 1)))\n    px = int(u * (W - 1))\n    py = int(v * (H - 1))\n\n    painting = ui.is_mouse_clicked(0) or ui.is_mouse_dragging(0, 0.0)\n    erasing  = ui.is_mouse_clicked(1) or ui.is_mouse_dragging(1, 0.0)\n\n    if painting:\n        r, g, b = state[\"brush_color\"]\n        draw_brush(py, px, state[\"brush_size\"], r, g, b)\n    elif erasing:\n        r, g, b = state[\"bg_color\"]\n        draw_brush(py, px, state[\"brush_size\"], r, g, b)\n\n    refresh_display()\n\n\nview.run()\n</code></pre>"},{"location":"06_pixel_canvas/#what-just-happened","title":"What just happened?","text":""},{"location":"06_pixel_canvas/#screen-coordinates-tensor-pixels","title":"Screen coordinates \u2192 tensor pixels","text":"<p>This is the core trick of the example. The canvas image is stretched to fill the panel, so a 128\u00d7128 tensor might be displayed at 600\u00d7500 screen pixels. When the mouse is at screen position <code>(mx, my)</code>, you need to figure out which tensor pixel that corresponds to:</p> <pre><code># Panel window position and size\nwp_x, wp_y = ui.get_window_pos()\nwin_w, win_h = ui.get_window_size()\n\n# Content area (subtract ImGui title bar + padding)\ncontent_x = wp_x + 8\ncontent_y = wp_y + 26\n\n# Normalize to [0, 1]\nu = (mx - content_x) / content_w\nv = (my - content_y) / content_h\n\n# Scale to pixel indices\npx = int(u * (W - 1))\npy = int(v * (H - 1))\n</code></pre> <p>This is the same mental model as converting normalized device coordinates to pixel coordinates in a rasterizer \u2014 just in reverse.</p>"},{"location":"06_pixel_canvas/#is_item_hovered-is_mouse_dragging","title":"is_item_hovered + is_mouse_dragging","text":"<p>ImGui tracks which widget the mouse is over. After the canvas image is drawn (which happens automatically inside the panel), <code>is_item_hovered()</code> tells you whether the cursor is on the image.</p> <p><code>is_mouse_dragging(button, threshold)</code> returns <code>True</code> every frame while the button is held and the mouse has moved at least <code>threshold</code> pixels. With <code>threshold=0.0</code> it fires immediately \u2014 effectively \"is button held.\"</p> <p>Combined, this gives you continuous painting:</p> <pre><code>if ui.is_item_hovered():\n    if ui.is_mouse_dragging(0, 0.0):\n        draw_brush(py, px, ...)\n</code></pre>"},{"location":"06_pixel_canvas/#backing-store-display-refresh","title":"Backing store + display refresh","text":"<p>We keep a separate <code>backing</code> tensor (RGB, no alpha) that stores the actual pixel art. Each frame, we copy it to the display tensor and optionally overlay a grid. This prevents the grid lines from \"baking into\" the artwork over time.</p> <pre><code>canvas_tensor[:, :, :3] = backing          # copy artwork\nif state[\"show_grid\"]:\n    canvas_tensor[::8, :, :3] += 0.15      # lighten grid rows\n    canvas_tensor[:, ::8, :3] += 0.15      # lighten grid columns\n</code></pre>"},{"location":"06_pixel_canvas/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>create_tensor</code> is a two-way street \u2014 the GPU can write to it    (simulation) and Python can write to it (user interaction). Vultorch    displays whatever is in the tensor, no questions asked.</p> </li> <li> <p>Screen \u2192 tensor mapping \u2014 <code>get_mouse_pos()</code> gives screen pixels;    you subtract the panel origin, divide by the panel size, and multiply    by tensor dimensions. Same math as UV coordinates in graphics.</p> </li> <li> <p><code>is_item_hovered</code> + <code>is_mouse_dragging</code> \u2014 the standard pattern    for interactive widgets. Check hover first, then check button state.</p> </li> <li> <p>Backing store pattern \u2014 if you overlay decorations (grids, markers,    crosshairs) on top of user data, keep the raw data in a separate tensor    and composite every frame. Otherwise decorations accumulate.</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014 essential for pixel art. Without it the    128\u00d7128 grid would look like a blurry watercolor instead of crisp squares.</p> </li> </ol> <p>Tip</p> <p>This screen-to-pixel mapping technique is the same one you'd use to build annotation tools \u2014 segmentation masks, bounding boxes, keypoint labeling. The tensor is the label map.</p>"},{"location":"07_multichannel/","title":"07 \u2014 Multi-Channel Viewer","text":"<p>Example file: <code>examples/07_multichannel.py</code></p> <p>If you're doing neural rendering \u2014 NeRF, 3D Gaussian Splatting, whatever the next acronym is \u2014 your model doesn't just produce a pretty picture. It produces RGB and depth and normals and alpha. Every. Single. Pixel.</p> <p>During development you need to see all of these at once. The standard workflow: save four PNGs, open them in four matplotlib windows, squint at them side by side, realize the depth map is upside-down, save again, reopen, repeat until you question your career choices.</p> <p>This chapter replaces all of that with four zero-copy panels updating at 60 fps.</p>"},{"location":"07_multichannel/#new-friends","title":"New friends","text":"New thing What it does Why it matters Four <code>create_tensor</code> calls Four independent GPU-shared textures in one window Each output channel gets its own live display Turbo colormap Maps a scalar <code>[0, 1]</code> tensor to a colored <code>(H, W, 3)</code> image Depth and other scalar fields are invisible in grayscale; turbo makes structure obvious Normal \u2192 RGB mapping <code>normal * 0.5 + 0.5</code> converts <code>[-1, 1]</code> normals to <code>[0, 1]</code> colors The standard convention: X\u2192red, Y\u2192green, Z\u2192blue Ray-sphere intersection All-GPU procedural rendering in ~30 lines of PyTorch Demonstrates that any GPU computation can feed into Vultorch"},{"location":"07_multichannel/#what-were-building","title":"What we're building","text":"<p>A procedural ray-sphere renderer with four live outputs and a control sidebar. The entire computation \u2014 rays, intersections, shading, colormap \u2014 runs on the GPU. All four display tensors are zero-copy, so nothing is ever copied to CPU.</p>"},{"location":"07_multichannel/#full-code","title":"Full code","text":"<pre><code>import math\n\nimport torch\nimport torch.nn.functional as F\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nH, W = 256, 256\n\nview = vultorch.View(\"07 - Multi-Channel Viewer\", 512, 1024)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.20)\nrgb_panel = view.panel(\"RGB\")\ndepth_panel = view.panel(\"Depth\")\nnormal_panel = view.panel(\"Normal\")\nalpha_panel = view.panel(\"Alpha\")\n\n# Four zero-copy display tensors\nrgb_tensor = vultorch.create_tensor(H, W, 4, device, name=\"rgb\",\n                                     window=view.window)\ndepth_tensor = vultorch.create_tensor(H, W, 4, device, name=\"depth\",\n                                       window=view.window)\nnormal_tensor = vultorch.create_tensor(H, W, 4, device, name=\"normal\",\n                                        window=view.window)\nalpha_tensor = vultorch.create_tensor(H, W, 4, device, name=\"alpha\",\n                                       window=view.window)\n\nrgb_panel.canvas(\"rgb\").bind(rgb_tensor)\ndepth_panel.canvas(\"depth\").bind(depth_tensor)\nnormal_panel.canvas(\"normal\").bind(normal_tensor)\nalpha_panel.canvas(\"alpha\").bind(alpha_tensor)\n\n# --- Turbo colormap LUT (256 entries, built once) ---\n_turbo_data = [\n    (0.18995, 0.07176, 0.23217), (0.22500, 0.16354, 0.45096),\n    # ... (32 key colors, interpolated to 256)\n]\nTURBO_LUT = ...  # see full source for the complete LUT build\n\ndef apply_turbo(values):\n    \"\"\"Map [0,1] float tensor (H,W) \u2192 (H,W,3) turbo colors.\"\"\"\n    idx = (values.clamp(0, 1) * 255).long()\n    return TURBO_LUT[idx]\n\n# Precompute ray directions\nys = torch.linspace(1, -1, H, device=device)\nxs = torch.linspace(-1, 1, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n\nstate = {\"sphere_r\": 0.6, \"light_az\": 0.5, \"light_el\": 0.8,\n         \"ambient\": 0.1, \"bg_r\": 0.12, \"bg_g\": 0.12, \"bg_b\": 0.14}\n\n\ndef render_sphere():\n    r = state[\"sphere_r\"]\n    ray_o = torch.tensor([0.0, 0.0, -2.0], device=device)\n    ray_d = torch.stack([xx, yy, torch.ones_like(xx)], dim=-1)\n    ray_d = ray_d / ray_d.norm(dim=-1, keepdim=True)\n\n    # Quadratic formula for ray-sphere intersection\n    b = 2.0 * (ray_d * ray_o).sum(-1)\n    c_val = (ray_o * ray_o).sum() - r * r\n    disc = b * b - 4.0 * c_val\n    hit = disc &gt; 0\n\n    t = (-b - torch.sqrt(disc.clamp(min=0))) / 2.0\n    t = t.clamp(min=0)\n    point = ray_o + t.unsqueeze(-1) * ray_d\n    normal = point / (point.norm(dim=-1, keepdim=True) + 1e-8)\n\n    # Lambertian shading\n    az, el = state[\"light_az\"], state[\"light_el\"]\n    light_dir = torch.tensor([math.cos(el)*math.sin(az),\n                               math.sin(el),\n                               math.cos(el)*math.cos(az)], device=device)\n    light_dir = light_dir / light_dir.norm()\n    shade = state[\"ambient\"] + (1 - state[\"ambient\"]) * \\\n            (normal * light_dir).sum(-1).clamp(min=0)\n\n    bg = torch.tensor([state[\"bg_r\"], state[\"bg_g\"], state[\"bg_b\"]],\n                      device=device)\n\n    # RGB\n    rgb = torch.where(hit.unsqueeze(-1),\n                      shade.unsqueeze(-1) * torch.ones(1,1,3, device=device), bg)\n    rgb_tensor[:,:,:3] = rgb;  rgb_tensor[:,:,3] = 1.0\n\n    # Depth (turbo colormap)\n    depth_raw = t * hit.float()\n    d_min = depth_raw[hit].min() if hit.any() else torch.tensor(0.0)\n    d_max = depth_raw[hit].max() if hit.any() else torch.tensor(1.0)\n    depth_norm = ((depth_raw - d_min) / (d_max - d_min + 1e-8)).clamp(0,1)\n    depth_color = torch.where(hit.unsqueeze(-1), apply_turbo(depth_norm), bg)\n    depth_tensor[:,:,:3] = depth_color;  depth_tensor[:,:,3] = 1.0\n\n    # Normals ([-1,1] \u2192 [0,1])\n    nc = torch.where(hit.unsqueeze(-1), normal * 0.5 + 0.5, bg)\n    normal_tensor[:,:,:3] = nc;  normal_tensor[:,:,3] = 1.0\n\n    # Alpha\n    a = hit.float()\n    alpha_tensor[:,:,0] = a; alpha_tensor[:,:,1] = a\n    alpha_tensor[:,:,2] = a; alpha_tensor[:,:,3] = 1.0\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n    state[\"sphere_r\"] = ctrl.slider(\"Radius\", 0.1, 1.5, default=0.6)\n    ctrl.separator()\n    state[\"light_az\"] = ctrl.slider(\"Light Az\", -3.14, 3.14, default=0.5)\n    state[\"light_el\"] = ctrl.slider(\"Light El\", -1.5, 1.5, default=0.8)\n    state[\"ambient\"]  = ctrl.slider(\"Ambient\", 0.0, 1.0, default=0.1)\n    ctrl.separator()\n    bg = ctrl.color_picker(\"Background\", default=(0.12, 0.12, 0.14))\n    state[\"bg_r\"], state[\"bg_g\"], state[\"bg_b\"] = bg\n\n\n@view.on_frame\ndef update():\n    render_sphere()\n\n\nview.run()\n</code></pre> <p>(The listing above is abridged \u2014 see <code>examples/07_multichannel.py</code> for the complete turbo colormap LUT.)</p>"},{"location":"07_multichannel/#what-just-happened","title":"What just happened?","text":""},{"location":"07_multichannel/#four-panels-four-tensors-one-window","title":"Four panels, four tensors, one window","text":"<pre><code>rgb_tensor    = vultorch.create_tensor(H, W, 4, device, name=\"rgb\", ...)\ndepth_tensor  = vultorch.create_tensor(H, W, 4, device, name=\"depth\", ...)\nnormal_tensor = vultorch.create_tensor(H, W, 4, device, name=\"normal\", ...)\nalpha_tensor  = vultorch.create_tensor(H, W, 4, device, name=\"alpha\", ...)\n</code></pre> <p>Each call allocates a separate Vulkan-shared tensor. Each panel binds one of them. All four update every frame with no CPU involvement \u2014 the data path is CUDA \u2192 Vulkan \u2192 screen.</p> <p>This is the workflow for neural rendering: your model forward-pass fills four tensors, and the viewer shows all of them simultaneously.</p>"},{"location":"07_multichannel/#turbo-colormap-making-depth-visible","title":"Turbo colormap \u2014 making depth visible","text":"<p>Raw depth values are floats in some arbitrary range. Displaying them directly gives you a nearly-black image with invisible gradients. The turbo colormap maps <code>[0, 1]</code> scalars to a perceptually uniform rainbow so you can actually see the depth structure:</p> <pre><code>def apply_turbo(values):\n    idx = (values.clamp(0, 1) * 255).long()   # quantize to 256 bins\n    return TURBO_LUT[idx]                       # lookup (H, W, 3)\n</code></pre> <p>This runs entirely on the GPU \u2014 no numpy, no matplotlib.</p>"},{"location":"07_multichannel/#normal-rgb-convention","title":"Normal \u2192 RGB convention","text":"<p>The standard way to visualize surface normals: map each component from <code>[-1, 1]</code> to <code>[0, 1]</code> and assign it to a color channel:</p> <pre><code>normal_color = normal * 0.5 + 0.5   # X\u2192R, Y\u2192G, Z\u2192B\n</code></pre> <p>A surface pointing right is red, up is green, towards the camera is blue. Every neural rendering paper uses this convention, so you'll recognize the visual immediately.</p>"},{"location":"07_multichannel/#ray-sphere-intersection","title":"Ray-sphere intersection","text":"<p>The entire renderer is ~30 lines of PyTorch. The key is the quadratic formula for ray-sphere intersection:</p> <p>$$t = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}$$</p> <p>where $a = |d|^2$, $b = 2 \\langle o, d \\rangle$, $c = |o|^2 - r^2$. This runs in parallel for all $H \\times W$ rays in one GPU kernel call. Replace this with your neural network's forward pass and you've got a NeRF viewer.</p>"},{"location":"07_multichannel/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Multiple zero-copy tensors \u2014 call <code>create_tensor</code> once per output    channel, bind each to its own panel. All updates happen on the GPU.</p> </li> <li> <p>Turbo colormap \u2014 a GPU LUT indexed by <code>(values * 255).long()</code>.    Essential for depth, disparity, attention weights, loss heatmaps \u2014    any scalar field that would be invisible in grayscale.</p> </li> <li> <p>Normal \u2192 RGB \u2014 <code>n * 0.5 + 0.5</code>. Three characters of code,    universally understood in the graphics/vision community.</p> </li> <li> <p>Procedural \u2192 neural \u2014 this example uses ray-sphere intersection    as a stand-in. Replace <code>render_sphere()</code> with your model's forward    pass and you have a live multi-channel neural rendering viewer.</p> </li> <li> <p>Zero-copy scalability \u2014 four 256\u00d7256\u00d74 textures updating every    frame. The bottleneck is your computation, not the display pipeline.</p> </li> </ol> <p>Tip</p> <p>Drag the panel borders to rearrange the four views \u2014 put depth next to RGB for comparison, or stack normals on top of alpha. The layout is fully user-configurable at runtime.</p>"},{"location":"08_gt_vs_pred/","title":"08 \u2014 GT vs Prediction","text":"<p>Example file: <code>examples/08_gt_vs_pred.py</code></p> <p>Every neural rendering researcher does the same thing a hundred times a day: train a model, look at the result, compare it to the ground truth, squint at the diff, wonder where the error is hiding.</p> <p>The usual flow: save a PNG, open it in matplotlib next to the GT, compute PSNR in a separate cell, plot the loss in TensorBoard, alt-tab between three windows while your GPU sits idle. By the time you've assembled your comparison, you've forgotten what hyperparameter you changed.</p> <p>This chapter puts it all in one window: GT, prediction, error heatmap, loss curve, and PSNR \u2014 all live, all 60 fps, all zero-copy.</p>"},{"location":"08_gt_vs_pred/#new-friends","title":"New friends","text":"New thing What it does Why it matters Error heatmap <code>|GT - pred|</code> amplified and turbo-colormapped Sees errors that are invisible in raw pixel comparison <code>panel.plot()</code> Draws a line chart from a Python list Live loss and PSNR curves without TensorBoard <code>panel.progress()</code> A progress bar Quick visual for training completion PSNR $-10 \\log_{10}(\\text{MSE})$ computed from the live loss The standard metric for image reconstruction quality Error mode combo Switch between L1, L2, per-channel max Different error norms reveal different problems Error gain slider Amplify subtle errors for visibility Low error regions become visible when multiplied by 5\u201320\u00d7"},{"location":"08_gt_vs_pred/#what-were-building","title":"What we're building","text":"<p>A coordinate MLP fitting a target image (same as example 03, but upgraded). Three image panels \u2014 GT, prediction, error heatmap \u2014 plus a metrics sidebar with live loss/PSNR curves and controls.</p>"},{"location":"08_gt_vs_pred/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# Load target image\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\nH, W = gt.shape[0], gt.shape[1]\n\n# Coordinate grid for the MLP\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\n# Simple coordinate MLP\nclass CoordMLP(nn.Module):\n    def __init__(self, hidden=64, layers=3):\n        super().__init__()\n        net = [nn.Linear(2, hidden), nn.ReLU(inplace=True)]\n        for _ in range(layers - 1):\n            net += [nn.Linear(hidden, hidden), nn.ReLU(inplace=True)]\n        net += [nn.Linear(hidden, 3), nn.Sigmoid()]\n        self.net = nn.Sequential(*net)\n    def forward(self, x):\n        return self.net(x)\n\nmodel = CoordMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# Turbo colormap LUT (see example 07 for details)\nTURBO_LUT = ...  # 256\u00d73 on GPU\n\ndef apply_turbo(values):\n    idx = (values.clamp(0, 1) * 255).long()\n    return TURBO_LUT[idx]\n\n# View + panels\nview = vultorch.View(\"08 - GT vs Prediction\", 1280, 1000)\nmetrics_panel = view.panel(\"Metrics\", side=\"right\", width=0.28)\ngt_panel = view.panel(\"Ground Truth\")\npred_panel = view.panel(\"Prediction\")\nerror_panel = view.panel(\"Error Map\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\npred_rgba = vultorch.create_tensor(H, W, 4, device, name=\"pred\",\n                                    window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nerror_rgba = vultorch.create_tensor(H, W, 4, device, name=\"error\",\n                                     window=view.window)\nerror_rgba[:, :, 3] = 1.0\nerror_panel.canvas(\"error\").bind(error_rgba)\n\nERROR_MODES = [\"L1\", \"L2\", \"Per-Channel Max\"]\nstate = {\"iter\": 0, \"loss\": 1.0, \"psnr\": 0.0, \"steps_per_frame\": 6,\n         \"error_mode\": 0, \"error_gain\": 5.0,\n         \"loss_history\": [], \"psnr_history\": []}\n\n\ndef compute_error_map(gt_img, pred_img, mode, gain):\n    if mode == 0:    err = (gt_img - pred_img).abs().mean(dim=-1)\n    elif mode == 1:  err = ((gt_img - pred_img)**2).mean(dim=-1).sqrt()\n    else:            err = (gt_img - pred_img).abs().max(dim=-1).values\n    return apply_turbo((err * gain).clamp(0, 1))\n\n\ndef compute_psnr(mse):\n    return -10.0 * math.log10(mse) if mse &gt; 0 else 50.0\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n        error_color = compute_error_map(gt, pred, state[\"error_mode\"],\n                                         state[\"error_gain\"])\n        error_rgba[:, :, :3] = error_color\n        state[\"psnr\"] = compute_psnr(state[\"loss\"])\n\n    state[\"loss_history\"].append(state[\"loss\"])\n    state[\"psnr_history\"].append(state[\"psnr\"])\n    if len(state[\"loss_history\"]) &gt; 300:\n        state[\"loss_history\"] = state[\"loss_history\"][-300:]\n        state[\"psnr_history\"] = state[\"psnr_history\"][-300:]\n\n\n@metrics_panel.on_frame\ndef draw_metrics():\n    metrics_panel.text(f\"FPS: {view.fps:.1f}\")\n    metrics_panel.text(f\"Iteration: {state['iter']}\")\n    metrics_panel.separator()\n    metrics_panel.text(f\"MSE Loss: {state['loss']:.6f}\")\n    metrics_panel.text(f\"PSNR: {state['psnr']:.2f} dB\")\n    metrics_panel.separator()\n\n    metrics_panel.text(\"Loss Curve\")\n    if state[\"loss_history\"]:\n        metrics_panel.plot(state[\"loss_history\"], label=\"##loss\",\n                           overlay=f\"{state['loss']:.5f}\", height=80)\n\n    metrics_panel.text(\"PSNR Curve\")\n    if state[\"psnr_history\"]:\n        metrics_panel.plot(state[\"psnr_history\"], label=\"##psnr\",\n                           overlay=f\"{state['psnr']:.1f} dB\", height=80)\n\n    metrics_panel.separator()\n    state[\"steps_per_frame\"] = metrics_panel.slider_int(\n        \"Steps/Frame\", 1, 32, default=6)\n    state[\"error_mode\"] = metrics_panel.combo(\"Error Mode\", ERROR_MODES)\n    state[\"error_gain\"] = metrics_panel.slider(\"Error Gain\", 1.0, 20.0,\n                                                default=5.0)\n\n    progress = min(1.0, state[\"iter\"] / 5000.0)\n    metrics_panel.progress(progress, overlay=f\"{progress*100:.0f}%\")\n\n\nview.run()\n</code></pre> <p>(Abridged \u2014 see <code>examples/08_gt_vs_pred.py</code> for the complete code including turbo LUT.)</p>"},{"location":"08_gt_vs_pred/#what-just-happened","title":"What just happened?","text":""},{"location":"08_gt_vs_pred/#error-heatmap-seeing-the-invisible","title":"Error heatmap \u2014 seeing the invisible","text":"<p>The raw <code>|GT - pred|</code> difference is usually tiny floats near zero. If you display it directly, you see a nearly-black image and conclude everything is fine. Bad idea.</p> <pre><code>err = (gt_img - pred_img).abs().mean(dim=-1)   # L1 error per pixel\nerr = (err * gain).clamp(0, 1)                  # amplify by 5\u201320\u00d7\nheatmap = apply_turbo(err)                       # turbo colormap\n</code></pre> <p>The gain slider lets you amplify subtle errors until they become visible. At 5\u00d7 gain, you'll see exactly where the network struggles \u2014 edges, fine textures, high-frequency regions. This is the kind of insight you never get from a single PSNR number.</p>"},{"location":"08_gt_vs_pred/#psnr-the-neural-rendering-metric","title":"PSNR \u2014 the neural rendering metric","text":"<p>$$\\text{PSNR} = -10 \\log_{10}(\\text{MSE})$$</p> <p>Updated every frame from the live loss. In one number, you know whether your model is at 20 dB (blurry mess), 30 dB (decent), or 40 dB (sharp). The live curve tells you when training plateaus.</p> <pre><code>def compute_psnr(mse):\n    return -10.0 * math.log10(mse) if mse &gt; 0 else 50.0\n</code></pre>"},{"location":"08_gt_vs_pred/#panelplot-loss-curves-without-tensorboard","title":"panel.plot() \u2014 loss curves without TensorBoard","text":"<pre><code>metrics_panel.plot(state[\"loss_history\"],\n                   label=\"##loss\",\n                   overlay=f\"{state['loss']:.5f}\",\n                   height=80)\n</code></pre> <p>Takes a Python list of floats, draws a sparkline. The <code>overlay</code> text shows on top of the chart. We keep the last 300 values for a rolling window. No external logging library needed.</p>"},{"location":"08_gt_vs_pred/#error-modes-different-norms-for-different-bugs","title":"Error modes \u2014 different norms for different bugs","text":"<ul> <li>L1 (<code>abs().mean(dim=-1)</code>) \u2014 average absolute error. Shows where   the prediction is generally wrong.</li> <li>L2 (<code>square().mean(dim=-1).sqrt()</code>) \u2014 root mean square error.   Amplifies outliers more than L1.</li> <li>Per-channel max (<code>abs().max(dim=-1)</code>) \u2014 worst-case channel.   Reveals color channel mismatches (e.g. the blue channel is wrong   but R and G are fine).</li> </ul> <p>Switch between them at runtime with the combo. Different error norms make different problems visible.</p>"},{"location":"08_gt_vs_pred/#upgrading-from-example-03","title":"Upgrading from example 03","text":"<p>Example 03 had GT and prediction side by side. This example adds:</p> <ul> <li>Error heatmap \u2014 see where the model is wrong, not just how much.</li> <li>PSNR + loss curves \u2014 real-time metrics, not just a text counter.</li> <li>Error gain \u2014 amplify subtle errors for visibility.</li> <li>Error mode switching \u2014 L1/L2/per-channel at runtime.</li> </ul> <p>This is the difference between \"my model is training\" and \"I can debug my model while it trains.\"</p>"},{"location":"08_gt_vs_pred/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>Three-panel comparison \u2014 GT, prediction, error heatmap. The    fundamental visual debugging layout for any reconstruction task.</p> </li> <li> <p>Error heatmap = amplified diff + colormap \u2014 the gain slider is    the key. Low errors are invisible without amplification.</p> </li> <li> <p>Live PSNR \u2014 one number that summarizes reconstruction quality.    $-10 \\log_{10}(\\text{MSE})$, computed from the loss you already have.</p> </li> <li> <p>panel.plot() \u2014 instant loss/metric curves inside the same window.    No TensorBoard, no wandb, no alt-tab.</p> </li> <li> <p>Error modes at runtime \u2014 different norms reveal different problems.    L1 for general errors, L2 for outliers, per-channel max for color bugs.</p> </li> </ol> <p>Tip</p> <p>Increase \"Error Gain\" to 15\u201320\u00d7 after training converges. You'll see the residual error pattern \u2014 it usually concentrates on edges and high-frequency textures, which tells you whether you need positional encoding or a deeper network.</p> <p>Note</p> <p>This example uses a tiny 3-layer MLP for speed. Replace it with your actual model and training loop \u2014 the visualization code stays exactly the same.</p>"},{"location":"09_live_tuning/","title":"09 \u2014 Live Hyperparameter Tuning","text":"<p>Example file: <code>examples/09_live_tuning.py</code></p> <p>You're training a model. The loss is stuck. You want to try a different learning rate. So you kill the process, edit the script, restart, wait for initialization, wait for the first 500 iterations to re-cover lost ground, and then see if your new LR helps.</p> <p>Or \u2014 you could just drag a slider.</p> <p>This example changes the training recipe on the fly: learning rate, optimizer, loss function, weight decay. No restart. No checkpoint. The model keeps training with the new settings applied instantly.</p> <p>The secret weapon is <code>step()</code> / <code>end_step()</code> \u2014 they give your training loop ownership of the main loop, while Vultorch handles rendering on each step.</p>"},{"location":"09_live_tuning/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>view.step()</code> Process one frame, return <code>False</code> on close Your training <code>while</code> loop owns the outer iteration <code>view.end_step()</code> Finish the current frame Pair with <code>step()</code> \u2014 replaces <code>run()</code> <code>view.close()</code> Explicitly destroy the window Called in <code>finally</code> for clean shutdown Log-scale LR slider <code>slider</code> from -5 to -1, then <code>10 ** value</code> Linear sliders are useless for learning rates \u2014 you need log scale Optimizer hot-swap Detect combo change, rebuild optimizer Switch Adam \u2194 SGD \u2194 AdamW without restarting <code>compute_loss()</code> MSE / L1 / Huber selected by combo Different losses for different problems, switchable at runtime"},{"location":"09_live_tuning/#what-were-building","title":"What we're building","text":"<p>The same coordinate MLP from example 08, but now with a control sidebar that lets you:</p> <ul> <li>Drag the LR on a log-scale slider (1e-5 to 0.1)</li> <li>Switch optimizer between Adam, SGD (with momentum), AdamW</li> <li>Change loss function between MSE, L1, Huber</li> <li>Adjust weight decay in real time</li> <li>Reset the model to random weights with one button</li> <li>See the effect instantly in the prediction image and loss curve</li> </ul>"},{"location":"09_live_tuning/#full-code","title":"Full code","text":"<pre><code>from pathlib import Path\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\ndevice = \"cuda\"\n\n# Load target image\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\nH, W = gt.shape[0], gt.shape[1]\n\n# Coordinate grid\ncoords = ...  # (H*W, 2) in [-1, 1]\ntarget = gt.reshape(-1, 3)\n\n\nclass CoordMLP(nn.Module):\n    def __init__(self, hidden=64, layers=3):\n        super().__init__()\n        net = [nn.Linear(2, hidden), nn.ReLU(inplace=True)]\n        for _ in range(layers - 1):\n            net += [nn.Linear(hidden, hidden), nn.ReLU(inplace=True)]\n        net += [nn.Linear(hidden, 3), nn.Sigmoid()]\n        self.net = nn.Sequential(*net)\n    def forward(self, x):\n        return self.net(x)\n\n\nOPTIMIZER_NAMES = [\"Adam\", \"SGD\", \"AdamW\"]\nLOSS_NAMES = [\"MSE\", \"L1\", \"Huber\"]\n\n\ndef make_optimizer(model, name, lr, weight_decay):\n    if name == \"Adam\":\n        return torch.optim.Adam(model.parameters(), lr=lr,\n                                weight_decay=weight_decay)\n    elif name == \"SGD\":\n        return torch.optim.SGD(model.parameters(), lr=lr,\n                               momentum=0.9, weight_decay=weight_decay)\n    elif name == \"AdamW\":\n        return torch.optim.AdamW(model.parameters(), lr=lr,\n                                 weight_decay=weight_decay)\n\n\ndef compute_loss(pred, target, loss_name):\n    if loss_name == \"MSE\":  return F.mse_loss(pred, target)\n    elif loss_name == \"L1\": return F.l1_loss(pred, target)\n    elif loss_name == \"Huber\": return F.smooth_l1_loss(pred, target)\n\n\nmodel = CoordMLP().to(device)\noptimizer = make_optimizer(model, \"Adam\", 2e-3, 0.0)\n\n# View + panels\nview = vultorch.View(\"09 - Live Hyperparameter Tuning\", 1100, 900)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.30)\npred_panel = view.panel(\"Prediction\")\nmetrics_panel = view.panel(\"Metrics\")\n\npred_rgba = vultorch.create_tensor(H, W, 4, device, name=\"pred\",\n                                    window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\"iter\": 0, \"loss\": 1.0, \"lr\": 2e-3,\n         \"optimizer_idx\": 0, \"loss_idx\": 0, \"weight_decay\": 0.0,\n         \"steps_per_frame\": 6, \"needs_reset\": False,\n         \"prev_optimizer_idx\": 0, ...}\n\n\n@ctrl.on_frame\ndef draw_controls():\n    log_lr = ctrl.slider(\"log10(LR)\", -5.0, -1.0, default=-2.7)\n    state[\"lr\"] = 10.0 ** log_lr\n    for pg in optimizer.param_groups:\n        pg[\"lr\"] = state[\"lr\"]\n\n    state[\"optimizer_idx\"] = ctrl.combo(\"Optimizer\", OPTIMIZER_NAMES)\n    state[\"loss_idx\"] = ctrl.combo(\"Loss\", LOSS_NAMES)\n    state[\"weight_decay\"] = ctrl.slider(\"Weight Decay\", 0.0, 0.1)\n\n    if ctrl.button(\"Reset Model\"):\n        state[\"needs_reset\"] = True\n\n\n# Training loop \u2014 step()/end_step() instead of run()\ntry:\n    while view.step():\n        if state[\"needs_reset\"]:\n            model = CoordMLP().to(device)\n            optimizer = make_optimizer(...)\n            state[\"iter\"] = 0\n            state[\"needs_reset\"] = False\n\n        if state[\"optimizer_idx\"] != state[\"prev_optimizer_idx\"]:\n            optimizer = make_optimizer(...)\n            state[\"prev_optimizer_idx\"] = state[\"optimizer_idx\"]\n\n        for _ in range(state[\"steps_per_frame\"]):\n            optimizer.zero_grad(set_to_none=True)\n            out = model(coords)\n            loss = compute_loss(out, target, LOSS_NAMES[state[\"loss_idx\"]])\n            loss.backward()\n            optimizer.step()\n            state[\"iter\"] += 1\n\n        with torch.no_grad():\n            pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n            pred_rgba[:, :, :3] = pred\n\n        view.end_step()\nfinally:\n    view.close()\n</code></pre> <p>(Abridged \u2014 see <code>examples/09_live_tuning.py</code> for the complete code.)</p>"},{"location":"09_live_tuning/#what-just-happened","title":"What just happened?","text":""},{"location":"09_live_tuning/#step-end_step-your-training-loop-in-charge","title":"step() / end_step() \u2014 your training loop in charge","text":"<p>In examples 01\u201308 we used <code>view.run()</code>. That's convenient \u2014 Vultorch owns the loop and calls your callbacks. But for training, you want to own the loop:</p> <pre><code>try:\n    while view.step():\n        # ... your training code here ...\n        view.end_step()\nfinally:\n    view.close()\n</code></pre> <p><code>step()</code> processes one frame's worth of input and rendering. It returns <code>False</code> when the user closes the window. <code>end_step()</code> finishes the frame. Your training code goes between them.</p> <p>Think of it like matplotlib's <code>ion()</code> mode \u2014 the plot updates, but your script keeps running. Except here it's 60 fps, zero-copy, and you get sliders.</p>"},{"location":"09_live_tuning/#log-scale-lr-slider","title":"Log-scale LR slider","text":"<p>Linear sliders are terrible for learning rates. The difference between 1e-4 and 2e-4 matters, but on a linear slider from 0 to 0.1, that's an invisible notch. Log scale fixes this:</p> <pre><code>log_lr = ctrl.slider(\"log10(LR)\", -5.0, -1.0, default=-2.7)\nstate[\"lr\"] = 10.0 ** log_lr\n</code></pre> <p>Slider position -5.0 = LR 1e-5, position -1.0 = LR 0.1. Now you can finely control anything from tiny fine-tuning rates to aggressive warm-up rates.</p> <p>To apply it immediately:</p> <pre><code>for pg in optimizer.param_groups:\n    pg[\"lr\"] = state[\"lr\"]\n</code></pre> <p>PyTorch optimizers read <code>lr</code> from <code>param_groups</code> on every step. Change it there, and the next <code>optimizer.step()</code> uses the new value. No re-creation needed.</p>"},{"location":"09_live_tuning/#optimizer-hot-swap","title":"Optimizer hot-swap","text":"<p>When you switch the combo from Adam to SGD, we need to create a new optimizer object \u2014 there's no way to morph one into the other:</p> <pre><code>if state[\"optimizer_idx\"] != state[\"prev_optimizer_idx\"]:\n    optimizer = make_optimizer(\n        model, OPTIMIZER_NAMES[state[\"optimizer_idx\"]],\n        state[\"lr\"], state[\"weight_decay\"])\n    state[\"prev_optimizer_idx\"] = state[\"optimizer_idx\"]\n</code></pre> <p>The model's parameters stay the same \u2014 only the optimizer state (momentum buffers, Adam's running averages) gets reset. This is actually useful: sometimes switching to SGD for a few iterations can shake the model out of a local minimum that Adam got stuck in.</p>"},{"location":"09_live_tuning/#loss-function-switching","title":"Loss function switching","text":"<p>Different loss functions emphasize different error patterns:</p> <ul> <li>MSE \u2014 penalizes large errors quadratically. Standard for PSNR.</li> <li>L1 \u2014 penalizes all errors equally. More robust to outliers, but   gradients are constant near zero (can cause oscillation).</li> <li>Huber \u2014 MSE near zero, L1 far away. The \"best of both worlds\"   that everyone uses for depth estimation.</li> </ul> <p>Switching at runtime lets you see the effect on convergence without restarting. Try MSE for 1000 iterations, then switch to Huber \u2014 you might see the loss drop further as Huber handles outlier pixels better.</p>"},{"location":"09_live_tuning/#model-reset","title":"Model reset","text":"<pre><code>if ctrl.button(\"Reset Model\"):\n    state[\"needs_reset\"] = True\n</code></pre> <p>Deferred to the training loop (not inside the callback) so that the model re-creation happens at the right time. The new model gets fresh random weights, a fresh optimizer, and the counters reset to zero.</p>"},{"location":"09_live_tuning/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p><code>step()</code> / <code>end_step()</code> \u2014 use these instead of <code>run()</code> when    you want custom training loop control. Your <code>while</code> loop owns    the iteration.</p> </li> <li> <p>Log-scale LR slider \u2014 learning rates span orders of magnitude.    A linear slider is useless; use <code>10 ** slider_value</code>.</p> </li> <li> <p>Optimizer hot-swap \u2014 change Adam to SGD to AdamW at runtime.    The model parameters survive, only optimizer state resets.</p> </li> <li> <p>Loss function switching \u2014 MSE, L1, Huber at runtime. Different    losses reveal different training dynamics.</p> </li> <li> <p>Immediate feedback \u2014 every slider change takes effect on the    very next training step. No restart, no checkpoint, no boilerplate.</p> </li> </ol> <p>Tip</p> <p>Start with Adam at LR 2e-3, then once the loss plateaus, try switching to SGD with momentum. The different optimization landscape can sometimes improve final quality.</p> <p>Note</p> <p>PSNR is always computed from MSE regardless of which loss function is active, so the PSNR number stays comparable across loss modes.</p>"},{"location":"10_gaussian2d/","title":"10 \u2014 2D Gaussian Splatting","text":"<p>Example file: <code>examples/10_gaussian2d.py</code></p> <p>3D Gaussian Splatting (3DGS) took neural rendering by storm. But the core idea is surprisingly simple: scatter a bunch of colored blobs, alpha-composite them together, compare to a target image, and backprop.</p> <p>This example strips 3DGS down to its 2D essence. No camera projection, no spherical harmonics, no tile-based rasterizer. Just N learnable 2D Gaussians \u2014 each with position, scale, rotation, color, and opacity \u2014 rendered via differentiable alpha compositing and optimized with plain PyTorch autograd.</p> <p>Watch the Gaussians wander across the canvas, scale up, rotate, change color, and gradually reassemble the target image. That's the algorithm that renders photorealistic scenes at 100+ fps.</p>"},{"location":"10_gaussian2d/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>nn.Parameter</code> for Gaussians Position, log-scale, rotation, raw color, raw opacity Every Gaussian attribute is learnable \u2014 autograd handles the rest Inverse covariance $\\Sigma^{-1}$ from scale + rotation The math that gives each Gaussian its elliptical shape Alpha compositing $C = \\sum_i \\alpha_i T_i c_i$ where $T_i = \\prod_{j"},{"location":"11_3d_inspector/","title":"11 \u2014 3D Surface Inspector","text":"<p>Example file: <code>examples/11_3d_inspector.py</code></p> <p>You've rendered a depth map, a normal map, or a texture from your NeRF. Now you want to see it from a different angle, catch the light on it, check for artifacts at grazing angles. In matplotlib you'd be wrestling with <code>plot_surface()</code> and <code>set_azim()</code>. Here, you just drag the mouse.</p> <p>SceneView is Vultorch's 3D viewer widget. It takes any tensor, maps it onto a plane in 3D space, adds Blinn-Phong lighting, and lets you orbit around it with mouse drag. MSAA anti-aliasing, adjustable FOV, light direction, shininess \u2014 all in real time.</p>"},{"location":"11_3d_inspector/#new-friends","title":"New friends","text":"New thing What it does Why it matters <code>SceneView</code> 3D plane viewer with orbit camera Inspect textures/outputs in 3D with mouse interaction <code>Camera</code> azimuth, elevation, distance, fov Orbit around the scene; dragged by mouse or set programmatically <code>Light</code> direction, intensity, ambient, specular, shininess Blinn-Phong shading with full control <code>.set_tensor()</code> Upload any tensor to the 3D scene RGB, RGBA, grayscale \u2014 auto-expanded to RGBA <code>.render()</code> Draw the scene as an ImGui image Handles mouse drag, camera sync, resize, all automatically <code>.msaa</code> Multi-sample anti-aliasing (1/2/4/8) Smooth edges at different quality/performance tradeoffs <code>.background</code> Background color tuple Sets the clear color behind the 3D plane"},{"location":"11_3d_inspector/#what-were-building","title":"What we're building","text":"<p>Four procedural textures generated on the GPU \u2014 checkerboard, radial gradient, sine pattern, normal map \u2014 displayed one at a time on a 3D plane. Mouse drag to orbit, with a control sidebar for camera, lighting, MSAA, and background color.</p>"},{"location":"11_3d_inspector/#full-code","title":"Full code","text":"<pre><code>import math\n\nimport torch\nimport vultorch\nfrom vultorch import ui\n\ndevice = \"cuda\"\nH, W = 256, 256\n\nys = torch.linspace(-1, 1, H, device=device)\nxs = torch.linspace(-1, 1, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n\n\ndef make_checkerboard(freq=8.0):\n    check = ((xx * freq).floor() + (yy * freq).floor()) % 2\n    return torch.stack([check*0.9+0.1, check*0.1+0.2, check*0.3+0.4], dim=-1)\n\n\ndef make_radial_gradient():\n    r = (xx**2 + yy**2).sqrt()\n    val = (1.0 - r).clamp(0, 1)\n    return torch.stack([val, val*0.5, val*0.2], dim=-1)\n\n\ndef make_sine_pattern(freq=6.0):\n    s1 = (torch.sin(xx * freq * math.pi) * 0.5 + 0.5)\n    s2 = (torch.sin(yy * freq * math.pi) * 0.5 + 0.5)\n    val = s1 * s2\n    return torch.stack([val*0.2+0.1, val*0.8+0.1, val*0.5+0.3], dim=-1)\n\n\ndef make_normal_map():\n    nx = xx * 0.5 + 0.5\n    ny = yy * 0.5 + 0.5\n    nz = (1.0 - xx**2 - yy**2).clamp(0, 1).sqrt() * 0.5 + 0.5\n    return torch.stack([nx, ny, nz], dim=-1)\n\n\nTEXTURE_NAMES = [\"Checkerboard\", \"Radial Gradient\", \"Sine Pattern\", \"Normal Map\"]\nTEXTURE_FNS = [make_checkerboard, make_radial_gradient,\n               make_sine_pattern, make_normal_map]\n\n# View + panels\nview = vultorch.View(\"11 - 3D Surface Inspector\", 1200, 800)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nscene_panel = view.panel(\"3D View\")\n\n# SceneView lives inside the panel\nscene = vultorch.SceneView(\"Inspector\", 800, 600, msaa=4)\ncurrent_texture = make_checkerboard()\n\n\n@ctrl.on_frame\ndef draw_controls():\n    state[\"texture_idx\"] = ctrl.combo(\"Texture\", TEXTURE_NAMES)\n    state[\"fov\"] = ctrl.slider(\"FOV\", 10.0, 120.0, default=45.0)\n    state[\"distance\"] = ctrl.slider(\"Distance\", 1.0, 10.0, default=3.0)\n    state[\"auto_rotate\"] = ctrl.checkbox(\"Auto Rotate\")\n\n    # Light controls\n    state[\"light_az\"] = ctrl.slider(\"Light Az\", -3.14, 3.14)\n    state[\"light_el\"] = ctrl.slider(\"Light El\", -3.14, 3.14)\n    state[\"ambient\"] = ctrl.slider(\"Ambient\", 0.0, 1.0, default=0.15)\n    state[\"specular\"] = ctrl.slider(\"Specular\", 0.0, 2.0, default=0.5)\n    state[\"shininess\"] = ctrl.slider(\"Shininess\", 1.0, 128.0, default=32.0)\n\n    state[\"msaa_idx\"] = ctrl.combo(\"MSAA\", [\"1\", \"2\", \"4\", \"8\"])\n    state[\"bg_color\"] = ctrl.color_picker(\"Background\")\n\n    if ctrl.button(\"Reset Camera\"):\n        scene.camera.reset()\n\n\n@scene_panel.on_frame\ndef draw_scene():\n    # Apply settings to camera, light, background\n    scene.camera.fov = state[\"fov\"]\n    scene.camera.distance = state[\"distance\"]\n    if state[\"auto_rotate\"]:\n        scene.camera.azimuth += 0.02\n\n    scene.light.direction = (cos(el)*sin(az), sin(el), cos(el)*cos(az))\n    scene.light.ambient = state[\"ambient\"]\n    scene.light.specular = state[\"specular\"]\n    scene.light.shininess = state[\"shininess\"]\n    scene.msaa = msaa_val\n    scene.background = state[\"bg_color\"]\n\n    scene.set_tensor(current_texture)\n    scene.render()  # draws the 3D view right here\n\n\nview.run()\n</code></pre> <p>(Abridged \u2014 see <code>examples/11_3d_inspector.py</code> for the complete code.)</p>"},{"location":"11_3d_inspector/#what-just-happened","title":"What just happened?","text":""},{"location":"11_3d_inspector/#sceneview-3d-in-a-panel","title":"SceneView \u2014 3D in a panel","text":"<p>SceneView is a self-contained 3D viewer widget. You create it once, then call <code>set_tensor()</code> and <code>render()</code> each frame:</p> <pre><code>scene = vultorch.SceneView(\"Inspector\", 800, 600, msaa=4)\n\n# Inside a panel callback:\nscene.set_tensor(my_tensor)  # upload the texture\nscene.render()               # render + display + handle mouse\n</code></pre> <p><code>render()</code> does everything: pushes camera/light settings to the GPU, renders the scene offscreen, displays it as an ImGui image, handles mouse drag for orbit/pan/zoom, and pulls the camera state back so Python sees the updated azimuth/elevation.</p>"},{"location":"11_3d_inspector/#camera-orbit-with-mathmouse","title":"Camera \u2014 orbit with math.mouse","text":"<p>The camera is defined by 5 values:</p> Property Default What it controls <code>azimuth</code> 0.0 Horizontal rotation (radians) <code>elevation</code> 0.6 Vertical rotation (radians) <code>distance</code> 3.0 Distance from target <code>target</code> (0,0,0) Look-at point <code>fov</code> 45.0 Field of view (degrees) <p>Left-drag rotates (azimuth + elevation), right-drag pans (target), middle-drag/scroll zooms (distance). All built-in \u2014 no code needed.</p> <p>You can also set values programmatically:</p> <pre><code>scene.camera.fov = 90.0       # wide-angle lens\nscene.camera.distance = 5.0   # far away\nscene.camera.azimuth += 0.02  # auto-rotate\n</code></pre>"},{"location":"11_3d_inspector/#light-blinn-phong-shading","title":"Light \u2014 Blinn-Phong shading","text":"<p>The light is a directional light source with Blinn-Phong shading:</p> <pre><code>scene.light.direction = (0.3, -1.0, 0.5)  # direction vector\nscene.light.intensity = 1.0                # overall brightness\nscene.light.ambient = 0.15                 # fill light\nscene.light.specular = 0.5                 # highlight strength\nscene.light.shininess = 32.0               # highlight sharpness\n</code></pre> <p>Low ambient + high specular = dramatic, contrasty look. High ambient + low specular = flat, evenly-lit look. Adjusting these interactively helps you spot surface artifacts that only show up at certain lighting angles.</p>"},{"location":"11_3d_inspector/#msaa-anti-aliasing-quality","title":"MSAA \u2014 anti-aliasing quality","text":"<p>MSAA (Multi-Sample Anti-Aliasing) smooths jagged edges:</p> MSAA Samples/pixel Quality Performance 1 1 Aliased Fastest 2 2 Slightly smooth Fast 4 4 Smooth Moderate 8 8 Very smooth Slowest <pre><code>scene.msaa = 4  # good default\n</code></pre> <p>For most visualization work, 4\u00d7 MSAA is the sweet spot. Drop to 1 if you need maximum frame rate, or go to 8 for screenshots.</p>"},{"location":"11_3d_inspector/#procedural-textures-on-gpu","title":"Procedural textures on GPU","text":"<p>All four textures are pure PyTorch tensor operations \u2014 no CPU, no PIL, no file I/O:</p> <pre><code>def make_checkerboard(freq=8.0):\n    check = ((xx * freq).floor() + (yy * freq).floor()) % 2\n    return torch.stack([check*0.9+0.1, check*0.1+0.2, check*0.3+0.4], dim=-1)\n</code></pre> <p>This is important because in a real neural rendering pipeline, the tensor you'd display here would be your NeRF's rendered output, your 3DGS's depth map, or your diffusion model's generated texture. The SceneView doesn't care where the tensor came from \u2014 it just displays it.</p>"},{"location":"11_3d_inspector/#key-takeaways","title":"Key takeaways","text":"<ol> <li> <p>SceneView = 3D plane viewer. Upload a tensor, render, orbit with mouse.    That's it.</p> </li> <li> <p>Camera has azimuth/elevation/distance/fov \u2014 set by mouse drag or    by Python code. <code>camera.reset()</code> goes back to defaults.</p> </li> <li> <p>Light is Blinn-Phong: direction, intensity, ambient, specular, shininess.    Interactive lighting reveals surface artifacts.</p> </li> <li> <p>MSAA goes from 1 (fast, aliased) to 8 (slow, smooth). Use 4 for    daily work.</p> </li> <li> <p>Any tensor works \u2014 RGB, RGBA, grayscale. SceneView auto-expands    to RGBA internally.</p> </li> </ol> <p>Tip</p> <p>Use auto-rotate (<code>scene.camera.azimuth += 0.02</code>) to quickly scan a surface for artifacts \u2014 problems that are invisible from the default angle become obvious when the surface rotates.</p> <p>Note</p> <p>SceneView renders a flat plane. For actual 3D geometry (meshes, point clouds), you'd need to extend the C++ renderer. But for inspecting per-pixel outputs like depth maps, normal maps, and textures, a lit 3D plane with orbit camera is exactly what you need.</p>"},{"location":"12_neural_workstation/","title":"12 \u2014 Neural Rendering Workstation","text":"<p>The capstone</p> <p>This is the final example \u2014 a fully-featured neural rendering IDE in a single Python script.  Use it as a template for publishing polished, interactive demos of your own research.</p>"},{"location":"12_neural_workstation/#new-friends-in-this-chapter","title":"New friends in this chapter","text":"Name What it does Analogy Open Image OS file dialog loads any PNG/JPEG/BMP into training <code>plt.imread()</code> + re-training, but one button Positional encoding Fourier features $[\\sin(2^l \\pi x), \\cos(2^l \\pi x)]$ The NeRF trick for learning high-frequency detail Architecture sliders Change hidden width, depth, PE levels at runtime Editing your model config without restarting Error gain Amplify the error heatmap to see subtle differences Cranking contrast on an image Pause / resume Stop training while the UI stays alive <code>Ctrl-C</code> without killing the process Save snapshots <code>Canvas.save()</code> dumps current GPU tensor to PNG <code>plt.savefig()</code> for live GPU data Training speed Iterations/sec counter Like <code>tqdm</code> but built-in"},{"location":"12_neural_workstation/#why-no-depth-head","title":"Why no depth head?","text":"<p>Previous versions had a dual-head MLP with RGB + depth.  But this example reconstructs a 2D image \u2014 there's no meaningful depth to predict.  So we keep it clean: one head, three outputs (RGB), direct MSE against the target image.  When you build a real NeRF, you'd add the density/depth head back to your own model.</p>"},{"location":"12_neural_workstation/#positional-encoding-the-nerf-trick","title":"Positional encoding \u2014 the NeRF trick","text":"<p>Raw <code>(x, y)</code> coordinates can only represent low-frequency functions. Positional encoding lifts them into a higher-dimensional space:</p> <pre><code>def positional_encoding(x, L):\n    if L == 0:\n        return x\n    freqs = 2.0 ** torch.arange(L, device=x.device)\n    xf = (x.unsqueeze(-1) * freqs * math.pi).reshape(*x.shape[:-1], -1)\n    return torch.cat([x, xf.sin(), xf.cos()], dim=-1)\n</code></pre> <p>With $L = 6$, a 2D input becomes $2 + 2 \\times 6 \\times 2 = 26$ dimensions.  This lets the MLP learn sharp edges and fine textures. The PE Levels slider lets you see the difference live \u2014 set it to 0 and watch the prediction turn blurry.</p>"},{"location":"12_neural_workstation/#opening-images-from-your-os","title":"Opening images from your OS","text":"<p>Click Open Image... and a native file dialog appears (via <code>tkinter.filedialog</code>).  The dialog runs in a background thread so the render loop never blocks:</p> <pre><code>def open_file_dialog():\n    import tkinter as tk\n    from tkinter import filedialog\n    root = tk.Tk()\n    root.withdraw()\n    root.attributes(\"-topmost\", True)\n    path = filedialog.askopenfilename(\n        title=\"Select an image\",\n        filetypes=[(\"Images\", \"*.png *.jpg *.jpeg *.bmp *.tga *.hdr\"),\n                   (\"All files\", \"*.*\")])\n    root.destroy()\n    return path if path else None\n</code></pre> <p>When the dialog returns a path, the main loop picks it up:</p> <pre><code>if S[\"pending_image\"]:\n    gt, coords, target, H, W = load_image(S[\"pending_image\"], RES)\n    S[\"img_name\"] = Path(S[\"pending_image\"]).name\n    # Recreate display tensors and reset model...\n</code></pre> <p>This is a pattern you can reuse in any Vultorch demo: thread the blocking OS call, pass the result back through a shared variable.</p>"},{"location":"12_neural_workstation/#live-architecture-tuning","title":"Live architecture tuning","text":"<p>Three sliders control the network structure:</p> <pre><code>new_h  = ctrl.slider_int(\"Hidden\",    32, 256, default=128)\nnew_l  = ctrl.slider_int(\"Layers\",     2,   8, default=4)\nnew_pe = ctrl.slider_int(\"PE Levels\",  0,  10, default=6)\n</code></pre> <p>Changing any slider sets <code>arch_dirty = True</code> and shows a yellow warning.  You then click Apply &amp; Reset to rebuild the model:</p> <pre><code>if S[\"arch_dirty\"]:\n    ctrl.text_colored(1, 0.8, 0, 1, \"  Architecture changed!\")\n    if ctrl.button(\"Apply &amp; Reset\", width=170):\n        model = make_model(S[\"hidden\"], S[\"layers\"], S[\"pe\"])\n        optimizer = make_optimizer(...)\n</code></pre> <p>This two-step pattern (slider \u2192 confirm button) prevents the model from being destroyed on every frame while you're dragging the slider.</p>"},{"location":"12_neural_workstation/#error-heatmap-with-adjustable-gain","title":"Error heatmap with adjustable gain","text":"<pre><code>err = (gt - pr).abs().mean(dim=-1)       # per-pixel L1\nerr_t[:, :, :3] = apply_turbo(\n    (err * S[\"err_gain\"]).clamp_(0, 1))\n</code></pre> <p>The Error Gain slider (1\u201320\u00d7) amplifies subtle errors.  At gain 1 most pixels look blue; at gain 15 you'll see exactly where the model is still struggling.</p>"},{"location":"12_neural_workstation/#pause-snapshot-speed","title":"Pause, snapshot, speed","text":"Feature How it works Pause <code>if not S[\"paused\"]:</code> skips the training loop; the window, controls, and display keep running Save Snapshots <code>rgb_cv.save(\"snapshot_pred.png\")</code> writes the canvas's GPU tensor to disk via stb_image_write Speed counter <code>(iters_now - iters_then) / elapsed</code> measured every 0.5 seconds"},{"location":"12_neural_workstation/#metrics-panel","title":"Metrics panel","text":"<pre><code>@met_pan.on_frame\ndef draw_met():\n    met_pan.text(f\"Loss: {S['loss']:.6f}   PSNR: {S['psnr']:.1f} dB   \"\n                 f\"Speed: {S['its_sec']:.0f} it/s\")\n    met_pan.separator()\n    if S[\"loss_h\"]:\n        met_pan.plot(S[\"loss_h\"], label=\"##loss\",\n                     overlay=f\"loss {S['loss']:.5f}\", height=70)\n    if S[\"psnr_h\"]:\n        met_pan.plot(S[\"psnr_h\"], label=\"##psnr\",\n                     overlay=f\"PSNR {S['psnr']:.1f} dB\", height=70)\n</code></pre> <p><code>panel.plot()</code> renders a sparkline from a Python list.  The last 500 values are kept, giving you a scrolling live chart \u2014 your TensorBoard, built into the training window.</p>"},{"location":"12_neural_workstation/#full-code","title":"Full code","text":"examples/12_neural_workstation.py<pre><code>--8&lt;-- \"examples/12_neural_workstation.py\"\n</code></pre>"},{"location":"12_neural_workstation/#what-just-happened","title":"What just happened?","text":"<p>In one Python file you built a complete, publishable demo:</p> <ol> <li>Open any image from your operating system via file dialog</li> <li>Positional encoding with adjustable frequency levels</li> <li>Live architecture tuning \u2014 change hidden size, depth, PE levels</li> <li>Three loss functions \u2014 MSE, L1, Huber \u2014 switchable at runtime</li> <li>Three optimizers \u2014 Adam, SGD, AdamW \u2014 hot-swappable</li> <li>Error heatmap with turbo colormap and adjustable gain</li> <li>Pause / resume without killing the process</li> <li>Save snapshots of prediction and error to PNG</li> <li>Training speed counter (iterations/sec)</li> <li>Loss &amp; PSNR curves \u2014 scrolling live charts</li> </ol> <p>No matplotlib.  No TensorBoard.  No Jupyter.  No web browser. One window, one script, and everything synchronized at GPU speed.</p> <p>This is what \"Vultorch = your neural rendering IDE\" looks like.</p>"},{"location":"12_neural_workstation/#key-takeaways","title":"Key takeaways","text":"Concept Code Purpose Positional encoding <code>positional_encoding(x, L)</code> High-frequency detail via Fourier features File dialog <code>tkinter.filedialog</code> in thread Load any image without blocking Architecture sliders <code>slider_int(\"Hidden\", ...)</code> Live topology tuning Error gain <code>(err * gain).clamp_(0, 1)</code> Amplify subtle reconstruction errors Pause <code>checkbox(\"Pause Training\")</code> Freeze training, UI stays live Snapshot <code>canvas.save(\"file.png\")</code> Dump GPU tensor to disk Speed <code>(it - it_last) / dt</code> Iterations/sec counter step()/end_step() Training-loop-owned rendering You control the outer loop <p>Congratulations!</p> <p>You've completed all 12 Vultorch tutorials.  You now have every tool you need to build real-time, GPU-accelerated visualization into your neural rendering research workflow \u2014 and to publish polished interactive demos of your work.</p>"},{"location":"13_snake_rl/","title":"13 \u2014 Snake RL","text":"<p>Beyond neural rendering</p> <p>Vultorch isn't just for NeRF and 3DGS.  Anything you can write into a <code>torch.Tensor</code> \u2014 reinforcement learning, physics simulation, signal processing \u2014 can be visualized at 60 fps with zero-copy GPU tensors.</p>"},{"location":"13_snake_rl/#new-friends-in-this-chapter","title":"New friends in this chapter","text":"Name What it does Analogy Snake environment A grid-based game, pure Python + deque OpenAI Gym, but 50 lines DQN agent 3-layer MLP that maps observations \u2192 Q-values The simplest deep RL algorithm Experience replay Store transitions, sample random batches A <code>deque</code> that makes RL stable \u03b5-greedy Random moves with decaying probability Exploration vs exploitation Q-value heatmap Color-coded grid showing the agent's \"thinking\" Like an attention map, but for RL Manual mode Take over the snake with buttons Debug your env by playing it yourself"},{"location":"13_snake_rl/#why-snake","title":"Why Snake?","text":"<p>Snake is the perfect RL demo:</p> <ul> <li>Simple rules \u2014 even non-ML people understand it instantly</li> <li>Visual feedback \u2014 you can see the agent getting smarter on the 32\u00d716 board</li> <li>Fast training \u2014 a DQN learns basic food-seeking in ~2000 episodes</li> <li>Fits in one file \u2014 environment + agent + training + visualization</li> </ul> <p>And it proves an important point: Vultorch is a general-purpose GPU visualization tool, not just a neural rendering library.</p>"},{"location":"13_snake_rl/#the-environment-50-lines-of-pure-python","title":"The environment \u2014 50 lines of pure Python","text":"<pre><code>class SnakeEnv:\n    def reset(self):\n        self.snake = deque([(ROWS // 2, COLS // 2)])\n        self.dir = RIGHT\n        self._place_food()\n        ...\n        return self._obs()\n\n    def step(self, action):\n        # action: 0=straight, 1=turn left, 2=turn right\n        ...\n        return obs, reward, done\n</code></pre> <p>The observation is an 11-dimensional vector:</p> Dims Meaning 0\u20132 Danger ahead / left / right (wall or body) 3\u20136 Current direction (one-hot) 7\u201310 Food direction (up / right / down / left) <p>This is enough for a small MLP to learn. No pixels, no CNN \u2014 just the essential spatial relationships.</p>"},{"location":"13_snake_rl/#the-dqn-agent-3-lines-of-pytorch","title":"The DQN agent \u2014 3 lines of PyTorch","text":"<pre><code>class DQN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(11, 128), nn.ReLU(True),\n            nn.Linear(128, 128), nn.ReLU(True),\n            nn.Linear(128, 3),  # straight, left, right\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <p>Three actions (straight / turn left / turn right) instead of four (up/down/left/right) \u2014 this makes learning much easier because the agent doesn't need to learn \"don't reverse into yourself\".</p>"},{"location":"13_snake_rl/#experience-replay-and-training","title":"Experience replay and training","text":"<pre><code>class ReplayBuffer:\n    def __init__(self, cap=50000):\n        self.buf = deque(maxlen=cap)\n\n    def push(self, s, a, r, s2, done):\n        self.buf.append((s, a, r, s2, done))\n\n    def sample(self, n):\n        batch = random.sample(self.buf, min(n, len(self.buf)))\n        ...\n</code></pre> <p>The DQN training step is textbook:</p> <pre><code>def train_dqn():\n    s, a, r, s2, d = buf.sample(BATCH)\n    with torch.no_grad():\n        q2 = target(s2).max(dim=1).values\n        y = r + GAMMA * q2 * (1 - d)\n    q = policy(s).gather(1, a.unsqueeze(1)).squeeze(1)\n    loss = F.smooth_l1_loss(q, y)\n    opt.zero_grad(); loss.backward(); opt.step()\n</code></pre> <p>A target network (updated every 500 steps) stabilizes learning. \u03b5 decays from 1.0 to 0.01 \u2014 the agent starts fully random and gradually shifts to exploiting what it's learned.</p>"},{"location":"13_snake_rl/#rendering-the-game-board","title":"Rendering the game board","text":"<pre><code>def render_game():\n    board = torch.zeros(ROWS, COLS, 3)\n    board[:, :] = torch.tensor([0.05, 0.05, 0.08])  # dark background\n\n    # Snake body (gradient from bright to dark green)\n    for i, (r, c) in enumerate(env.snake):\n        if i == 0:\n            board[r, c] = torch.tensor([1.0, 0.9, 0.0])  # head: yellow\n        else:\n            t = 1.0 - i / max(len(env.snake), 1) * 0.5\n            board[r, c] = torch.tensor([0.0, t, 0.2])\n\n    # Food: red\n    board[fr, fc] = torch.tensor([1.0, 0.15, 0.15])\n\n    game_t.copy_(cpu_rgba.to(disp_dev))\n</code></pre> <p>The grid is 32\u00d716 (2:1 landscape ratio).  The <code>filter=\"nearest\"</code> setting on the canvas prevents blurring \u2014 each grid cell is a crisp pixel, just like the Conway's Game of Life example.</p>"},{"location":"13_snake_rl/#q-value-heatmap","title":"Q-value heatmap","text":"<p>The Q-value panel visualizes the agent's decisions:</p> <ul> <li>Green = agent wants to go straight</li> <li>Blue = agent wants to turn left</li> <li>Orange = agent wants to turn right</li> <li>The top-left 3 cells show Q-value magnitude for each action</li> </ul> <p>This is functionally similar to an attention map in a transformer \u2014 it shows you what the model is thinking, not just what it does.</p>"},{"location":"13_snake_rl/#manual-mode","title":"Manual mode","text":"<p>Check the Manual Mode box and three buttons appear: Left, Fwd, Right.  Now you play the snake.  This is invaluable for debugging RL environments \u2014 if you can't solve the game yourself, the agent probably can't either.</p>"},{"location":"13_snake_rl/#the-training-loop","title":"The training loop","text":"<pre><code>while view.step():\n    for _ in range(S[\"speed\"]):\n        # \u03b5-greedy action selection\n        if random.random() &lt; S[\"eps\"]:\n            a = random.randint(0, 2)\n        else:\n            a = policy(obs).argmax().item()\n\n        next_obs, reward, done = env.step(a)\n        buf.push(obs, a, reward, next_obs, float(done))\n        train_dqn()\n\n        if done:\n            obs = env.reset()\n\n    render_game()\n    render_qvalues()\n    view.end_step()\n</code></pre> <p>The Steps/Frame slider controls how many environment steps happen per rendered frame.  Crank it up to 50 for faster training; set it to 1 to watch every move in slow motion.</p>"},{"location":"13_snake_rl/#full-code","title":"Full code","text":"examples/13_snake_rl.py<pre><code>--8&lt;-- \"examples/13_snake_rl.py\"\n</code></pre>"},{"location":"13_snake_rl/#what-just-happened","title":"What just happened?","text":"<p>In one Python file you built:</p> <ol> <li>A Snake game environment with reward shaping</li> <li>A DQN agent with experience replay and target network</li> <li>Live visualization \u2014 game board + Q-value heatmap + reward curves</li> <li>Manual mode \u2014 play the game yourself to debug the env</li> <li>\u03b5-greedy exploration with live epsilon display</li> </ol> <p>All rendered at 60 fps via Vultorch's zero-copy tensor display.  No separate Gym renderer, no matplotlib, no logging to disk.</p> <p>The takeaway: if your data lives in a tensor, Vultorch can visualize it \u2014 whether that's neural radiance fields, Gaussian splats, cellular automata, or a snake learning to find food.</p>"},{"location":"13_snake_rl/#key-takeaways","title":"Key takeaways","text":"Concept Code Purpose Snake env <code>SnakeEnv</code> class 32\u00d716 grid RL environment DQN <code>nn.Sequential(11 \u2192 128 \u2192 128 \u2192 3)</code> Deep Q-Network Replay buffer <code>deque(maxlen=50000)</code> Stable off-policy learning \u03b5-greedy <code>random() &lt; eps</code> Exploration vs exploitation Nearest filter <code>filter=\"nearest\"</code> Pixel-perfect grid display Q-value heatmap Color-coded agent decisions Visual debugging Manual mode <code>checkbox(\"Manual Mode\")</code> Debug the environment step()/end_step() Training-loop-owned rendering RL loop controls pace"},{"location":"api/","title":"API Reference","text":"<p>Complete reference for all public classes and functions in the <code>vultorch</code> package.</p>"},{"location":"api/#module-level-attributes","title":"Module-level Attributes","text":""},{"location":"api/#vultorch__version__","title":"<code>vultorch.__version__</code>","text":"<pre><code>__version__: str\n</code></pre> <p>Package version string (e.g. <code>\"0.5.0\"</code>).</p>"},{"location":"api/#vultorchhas_cuda","title":"<code>vultorch.HAS_CUDA</code>","text":"<pre><code>HAS_CUDA: bool\n</code></pre> <p><code>True</code> if the native extension was compiled with CUDA support. When <code>False</code>, all tensor display falls back to CPU staging (host-visible <code>memcpy</code>).</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#vultorchshow","title":"<code>vultorch.show()</code>","text":"<pre><code>def show(\n    tensor: torch.Tensor,\n    *,\n    name: str = \"tensor\",\n    width: float = 0,\n    height: float = 0,\n    filter: str = \"linear\",\n    window: Window | None = None,\n) -&gt; None\n</code></pre> <p>Display a tensor in the current ImGui context.</p> <p>Parameters:</p> Parameter Type Default Description <code>tensor</code> <code>torch.Tensor</code> (required) CUDA or CPU tensor. dtype: <code>float32</code>, <code>float16</code>, or <code>uint8</code>. Shape: <code>(H, W)</code> or <code>(H, W, C)</code> with C \u2208 {1, 3, 4}. <code>name</code> <code>str</code> <code>\"tensor\"</code> Unique label for caching when showing multiple tensors. <code>width</code> <code>float</code> <code>0</code> Display width in pixels. <code>0</code> = auto-fit to tensor. <code>height</code> <code>float</code> <code>0</code> Display height in pixels. <code>0</code> = auto-fit to tensor. <code>filter</code> <code>str</code> <code>\"linear\"</code> Sampling filter: <code>\"nearest\"</code> or <code>\"linear\"</code>. <code>window</code> <code>Window \\| None</code> <code>None</code> Target window. Defaults to <code>Window._current</code>. <p>Behavior:</p> <ul> <li>1-channel and 3-channel tensors are automatically expanded to RGBA.</li> <li>RGBA expansion buffers are cached per <code>name</code> to avoid per-frame allocation.</li> <li>On CUDA: uses zero-copy GPU\u2192GPU path. On CPU: uses host-visible staging buffer.</li> <li><code>uint8</code> tensors are divided by 255; <code>float16</code> tensors are converted to <code>float32</code>.</li> </ul> <p>Raises: <code>RuntimeError</code> if no active <code>Window</code> exists.</p>"},{"location":"api/#vultorchcreate_tensor","title":"<code>vultorch.create_tensor()</code>","text":"<pre><code>def create_tensor(\n    height: int,\n    width: int,\n    channels: int = 4,\n    device: str = \"cuda:0\",\n    *,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Allocate a Vulkan-shared CUDA tensor for true zero-copy display.</p> <p>Parameters:</p> Parameter Type Default Description <code>height</code> <code>int</code> (required) Tensor height in pixels. <code>width</code> <code>int</code> (required) Tensor width in pixels. <code>channels</code> <code>int</code> <code>4</code> Number of channels: 1, 3, or 4. <code>device</code> <code>str</code> <code>\"cuda:0\"</code> CUDA device string, or <code>\"cpu\"</code>. <code>name</code> <code>str</code> <code>\"tensor\"</code> Texture slot name (must match <code>show(..., name=...)</code>). <code>window</code> <code>Window \\| None</code> <code>None</code> Target window. Defaults to <code>Window._current</code>. <p>Returns: <code>torch.Tensor</code> of shape <code>(height, width, channels)</code>.</p> <p>Note</p> <p>Only <code>channels=4</code> gives true zero-copy via Vulkan external memory. For 1 or 3 channels, a regular CUDA tensor is returned and <code>show()</code> handles RGBA expansion with a GPU\u2192GPU copy.</p> <p>Raises: <code>RuntimeError</code> if no active <code>Window</code> exists.</p>"},{"location":"api/#vultorchimread","title":"<code>vultorch.imread()</code>","text":"<pre><code>def imread(\n    path: str,\n    *,\n    channels: int = 4,\n    size: tuple[int, int] | None = None,\n    device: str = \"cuda\",\n    shared: bool = False,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Load an image file into a <code>float32</code> tensor. Uses stb_image internally \u2014 no PIL or numpy needed.</p> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> (required) File path (PNG, JPG, BMP, TGA, HDR, \u2026). <code>channels</code> <code>int</code> <code>4</code> Desired channels: 1 (gray), 3 (RGB), or 4 (RGBA). <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> Optional <code>(height, width)</code> to resize with bilinear interpolation. <code>device</code> <code>str</code> <code>\"cuda\"</code> Target device (<code>\"cuda\"</code> or <code>\"cpu\"</code>). <code>shared</code> <code>bool</code> <code>False</code> If <code>True</code>, allocate via <code>create_tensor</code> for zero-copy display. <code>name</code> <code>str</code> <code>\"tensor\"</code> Texture slot name (only used when <code>shared=True</code>). <code>window</code> <code>Window \\| None</code> <code>None</code> Target window (only used when <code>shared=True</code>). <p>Returns: <code>torch.Tensor</code> of shape <code>(H, W, C)</code> with values in <code>[0, 1]</code>.</p> <p>Example:</p> <pre><code>import vultorch\ngt = vultorch.imread(\"photo.png\", channels=3, size=(256, 256), device=\"cuda\")\n</code></pre>"},{"location":"api/#vultorchimwrite","title":"<code>vultorch.imwrite()</code>","text":"<pre><code>def imwrite(\n    path: str,\n    tensor: torch.Tensor,\n    *,\n    channels: int = 0,\n    size: tuple[int, int] | None = None,\n    quality: int = 95,\n) -&gt; None\n</code></pre> <p>Save a tensor to an image file. Format is inferred from the extension.</p> <p>Parameters:</p> Parameter Type Default Description <code>path</code> <code>str</code> (required) Output file path. Extension selects format: <code>.png</code>, <code>.jpg</code>, <code>.bmp</code>, <code>.tga</code>, <code>.hdr</code>. <code>tensor</code> <code>torch.Tensor</code> (required) <code>(H, W)</code>, <code>(H, W, 1)</code>, <code>(H, W, 3)</code>, or <code>(H, W, 4)</code> tensor. <code>channels</code> <code>int</code> <code>0</code> Override output channels. <code>0</code> = use tensor's channel count. <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> Optional <code>(height, width)</code> to resize before saving. <code>quality</code> <code>int</code> <code>95</code> JPEG quality (1\u2013100). Ignored for other formats. <p>Behavior:</p> <ul> <li><code>.hdr</code> writes 32-bit float data; all other formats quantize to 8-bit.</li> <li>If the tensor has more channels than <code>channels</code>, extras are dropped. If fewer, missing channels are filled (alpha \u2192 1.0).</li> <li>Tensor is moved to CPU and converted to <code>float32</code> before writing.</li> </ul> <p>Example:</p> <pre><code>vultorch.imwrite(\"output.png\", pred_tensor, channels=3)\nvultorch.imwrite(\"output.jpg\", pred_tensor, quality=90)\n</code></pre>"},{"location":"api/#classes","title":"Classes","text":""},{"location":"api/#vultorchwindow","title":"<code>vultorch.Window</code>","text":"<pre><code>class Window:\n    _current: Window | None   # singleton reference\n\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>High-level wrapper around the Vulkan + SDL3 + ImGui engine. Creating a <code>Window</code> automatically makes it the current target for <code>show()</code> and <code>create_tensor()</code>.</p>"},{"location":"api/#methods","title":"Methods","text":"Method Signature Description <code>poll()</code> <code>\u2192 bool</code> Process OS events. Returns <code>False</code> when the window should close. <code>begin_frame()</code> <code>\u2192 bool</code> Begin a new ImGui frame. Returns <code>False</code> if the frame was skipped (minimized). <code>end_frame()</code> <code>\u2192 None</code> Submit the frame to the GPU and present. <code>activate()</code> <code>\u2192 None</code> Make this window the current target for module-level helpers. <code>upload_tensor(tensor, *, name)</code> <code>\u2192 None</code> Upload a tensor for display (CUDA or CPU). <code>get_texture_id(name)</code> <code>\u2192 int</code> ImGui texture ID for a named tensor. <code>get_texture_size(name)</code> <code>\u2192 (int, int)</code> <code>(width, height)</code> for a named tensor. <code>destroy()</code> <code>\u2192 None</code> Release all GPU / window resources. Safe to call multiple times."},{"location":"api/#properties","title":"Properties","text":"Property Type Description <code>tensor_texture_id</code> <code>int</code> ImGui texture ID of the default <code>\"tensor\"</code> slot. <code>tensor_size</code> <code>(int, int)</code> <code>(width, height)</code> of the default <code>\"tensor\"</code> slot."},{"location":"api/#usage","title":"Usage","text":"<pre><code>import vultorch\nfrom vultorch import ui\n\nwin = vultorch.Window(\"Demo\", 1280, 720)\nwhile win.poll():\n    if not win.begin_frame():\n        continue\n    ui.begin(\"Panel\", True, 0)\n    vultorch.show(tensor)\n    ui.end()\n    win.end_frame()\nwin.destroy()\n</code></pre>"},{"location":"api/#vultorchcamera","title":"<code>vultorch.Camera</code>","text":"<pre><code>class Camera:\n    azimuth: float     # horizontal angle (radians), default 0.0\n    elevation: float   # vertical angle (radians), default 0.6\n    distance: float    # distance from target, default 3.0\n    target: tuple      # (x, y, z) look-at point, default (0, 0, 0)\n    fov: float         # field of view (degrees), default 45.0\n</code></pre> <p>Orbit camera parameters used by <code>SceneView</code>. Call <code>reset()</code> to restore defaults.</p>"},{"location":"api/#vultorchlight","title":"<code>vultorch.Light</code>","text":"<pre><code>class Light:\n    direction: tuple   # (x, y, z), default (0.3, -1.0, 0.5)\n    color: tuple       # (r, g, b), default (1, 1, 1)\n    intensity: float   # default 1.0\n    ambient: float     # ambient term, default 0.15\n    specular: float    # specular term, default 0.5\n    shininess: float   # Blinn-Phong exponent, default 32.0\n    enabled: bool      # default True\n</code></pre> <p>Blinn-Phong directional light parameters used by <code>SceneView</code>.</p>"},{"location":"api/#vultorchsceneview","title":"<code>vultorch.SceneView</code>","text":"<pre><code>class SceneView:\n    def __init__(self, name: str = \"SceneView\",\n                 width: int = 800, height: int = 600,\n                 msaa: int = 4) -&gt; None: ...\n</code></pre> <p>3D tensor viewer \u2014 renders a tensor on a lit plane with orbit camera and MSAA.</p>"},{"location":"api/#attributes","title":"Attributes","text":"Attribute Type Default Description <code>name</code> <code>str</code> <code>\"SceneView\"</code> ImGui window label. <code>camera</code> <code>Camera</code> (auto) Orbit camera (drag to rotate). <code>light</code> <code>Light</code> (auto) Directional light. <code>background</code> <code>tuple</code> <code>(0.12, 0.12, 0.14)</code> Background color <code>(r, g, b)</code>. <code>msaa</code> <code>int</code> <code>4</code> Multi-sample anti-aliasing level (1/2/4/8)."},{"location":"api/#methods_1","title":"Methods","text":"Method Description <code>set_tensor(tensor)</code> Upload a tensor to the scene's texture. <code>render()</code> Process mouse interaction, render the scene, and display as an ImGui image."},{"location":"api/#usage_1","title":"Usage","text":"<pre><code>scene = vultorch.SceneView(\"3D View\", 800, 600, msaa=4)\n# inside frame loop:\nscene.set_tensor(tensor)\nscene.render()\n</code></pre>"},{"location":"api/#declarative-api","title":"Declarative API","text":"<p>The declarative API provides a higher-level abstraction for building multi-panel visualization apps.</p>"},{"location":"api/#vultorchview","title":"<code>vultorch.View</code>","text":"<pre><code>class View:\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>Top-level window with automatic docking layout.</p>"},{"location":"api/#methods_2","title":"Methods","text":"Method Signature Description <code>panel(name, *, side, width)</code> <code>\u2192 Panel</code> Create or retrieve a dockable panel. <code>side</code>: <code>\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code> / <code>None</code>. <code>on_frame(fn)</code> <code>\u2192 fn</code> Decorator \u2014 register a per-frame callback. <code>run()</code> <code>\u2192 None</code> Blocking event loop. <code>step()</code> <code>\u2192 bool</code> Non-blocking: process one frame. Returns <code>False</code> on close. <code>end_step()</code> <code>\u2192 None</code> Finish the frame started by <code>step()</code>. <code>close()</code> <code>\u2192 None</code> Destroy the window."},{"location":"api/#properties_1","title":"Properties","text":"Property Type Description <code>fps</code> <code>float</code> Current frames per second. <code>time</code> <code>float</code> Elapsed time in seconds. <code>window</code> <code>Window</code> Underlying <code>Window</code> instance."},{"location":"api/#usage-blocking","title":"Usage \u2014 Blocking","text":"<pre><code>view = vultorch.View(\"Demo\", 1280, 720)\nview.panel(\"Viewer\").canvas(\"img\").bind(tensor)\n\n@view.on_frame\ndef update():\n    speed = controls.slider(\"Speed\", 0, 10)\n    tensor[:,:,0] = (x + view.time * speed).sin()\n\nview.run()\n</code></pre>"},{"location":"api/#usage-training-loop","title":"Usage \u2014 Training Loop","text":"<pre><code>view = vultorch.View(\"Train\", 1024, 768)\noutput = view.panel(\"Output\").canvas(\"result\")\nfor epoch in range(100):\n    result = model(input)\n    output.bind(result)\n    if not view.step():\n        break\n    view.end_step()\nview.close()\n</code></pre>"},{"location":"api/#vultorchpanel","title":"<code>vultorch.Panel</code>","text":"<pre><code>class Panel:\n    # Created via View.panel() \u2014 not instantiated directly\n</code></pre> <p>A dockable panel containing canvases and widgets.</p>"},{"location":"api/#canvas-factory","title":"Canvas Factory","text":"Method Signature Description <code>canvas(name, *, filter, fit)</code> <code>\u2192 Canvas</code> Create a named canvas. <code>filter</code>: <code>\"linear\"</code> / <code>\"nearest\"</code>. <code>fit</code>: auto-fill panel space."},{"location":"api/#per-panel-callback","title":"Per-Panel Callback","text":"Method Signature Description <code>on_frame(fn)</code> <code>\u2192 fn</code> Decorator \u2014 register a per-frame callback that runs inside the panel's ImGui window."},{"location":"api/#layout","title":"Layout","text":"Method Description <code>row()</code> Context manager \u2014 place child widgets side-by-side."},{"location":"api/#widgets","title":"Widgets","text":"<p>All widget methods manage state automatically across frames.</p> Method Signature Description <code>text(text)</code> <code>\u2192 None</code> Static text. <code>text_colored(r, g, b, a, text)</code> <code>\u2192 None</code> Colored text. <code>text_wrapped(text)</code> <code>\u2192 None</code> Auto-wrapping text. <code>separator()</code> <code>\u2192 None</code> Horizontal separator line. <code>button(label, width, height)</code> <code>\u2192 bool</code> Button. Returns <code>True</code> when clicked. <code>width</code>/<code>height</code> default to <code>0</code> (auto-size). <code>checkbox(label, *, default)</code> <code>\u2192 bool</code> Checkbox with stateful toggle. <code>slider(label, min, max, *, default)</code> <code>\u2192 float</code> Float slider. <code>slider_int(label, min, max, *, default)</code> <code>\u2192 int</code> Integer slider. <code>color_picker(label, *, default)</code> <code>\u2192 (r, g, b)</code> Color picker (3-float tuple). <code>combo(label, items, *, default)</code> <code>\u2192 int</code> Dropdown combo box. Returns selected index. <code>input_text(label, *, default, max_length)</code> <code>\u2192 str</code> Text input field. <code>plot(values, *, label, overlay, width, height)</code> <code>\u2192 None</code> Line plot from a list of floats. <code>progress(fraction, *, overlay)</code> <code>\u2192 None</code> Progress bar (0.0 \u2013 1.0)."},{"location":"api/#vultorchcanvas","title":"<code>vultorch.Canvas</code>","text":"<pre><code>class Canvas:\n    # Created via Panel.canvas() \u2014 not instantiated directly\n</code></pre> <p>A display surface that renders a bound tensor as an ImGui image.</p>"},{"location":"api/#methods_3","title":"Methods","text":"Method Signature Description <code>bind(tensor)</code> <code>\u2192 Canvas</code> Bind a tensor for display. Returns <code>self</code> for chaining. <code>alloc(height, width, channels, device)</code> <code>\u2192 torch.Tensor</code> Allocate Vulkan-shared memory and auto-bind. Returns the tensor. <code>save(path, *, channels, size, quality)</code> <code>\u2192 None</code> Save the bound tensor to an image file via <code>imwrite()</code>."},{"location":"api/#properties_2","title":"Properties","text":"Property Type Default Description <code>filter</code> <code>str</code> <code>\"linear\"</code> <code>\"linear\"</code> or <code>\"nearest\"</code>. <code>fit</code> <code>bool</code> <code>True</code> Auto-fill available panel space."},{"location":"api/#imgui-bindings-vultorchui","title":"ImGui Bindings (<code>vultorch.ui</code>)","text":"<p>The <code>vultorch.ui</code> submodule exposes Dear ImGui functions (docking branch). All functions map directly to their ImGui C++ counterparts.</p>"},{"location":"api/#windows","title":"Windows","text":"<pre><code>ui.begin(name: str, opened: bool = True, flags: int = 0) -&gt; tuple[bool, bool]\nui.end() -&gt; None\nui.begin_child(id: str, width=0.0, height=0.0, child_flags=0, window_flags=0) -&gt; bool\nui.end_child() -&gt; None\n</code></pre>"},{"location":"api/#text","title":"Text","text":"<pre><code>ui.text(text: str) -&gt; None\nui.text_colored(r, g, b, a, text: str) -&gt; None\nui.text_disabled(text: str) -&gt; None\nui.text_wrapped(text: str) -&gt; None\nui.label_text(label: str, text: str) -&gt; None\nui.bullet_text(text: str) -&gt; None\n</code></pre>"},{"location":"api/#buttons","title":"Buttons","text":"<pre><code>ui.button(label: str, width=0.0, height=0.0) -&gt; bool\nui.small_button(label: str) -&gt; bool\nui.invisible_button(id: str, width, height) -&gt; bool\nui.arrow_button(id: str, direction: int) -&gt; bool\nui.radio_button(label: str, active: bool) -&gt; bool\n</code></pre>"},{"location":"api/#inputs","title":"Inputs","text":"<pre><code>ui.checkbox(label, value: bool) -&gt; bool\nui.slider_float(label, value, min=0.0, max=1.0, format=\"%.3f\") -&gt; float\nui.slider_float2(label, v1, v2, min, max) -&gt; tuple[float, float]\nui.slider_float3(label, v1, v2, v3, min, max) -&gt; tuple[float, float, float]\nui.slider_float4(label, v1, v2, v3, v4, min, max) -&gt; tuple\nui.slider_int(label, value, min=0, max=100) -&gt; int\nui.slider_angle(label, value, min=-360, max=360) -&gt; float\nui.drag_float(label, value, speed=1.0) -&gt; float\nui.drag_float2(label, v1, v2, speed=1.0) -&gt; tuple\nui.drag_float3(label, v1, v2, v3, speed=1.0) -&gt; tuple\nui.drag_int(label, value, speed=1.0) -&gt; int\nui.input_float(label, value) -&gt; float\nui.input_float2(label, v1, v2) -&gt; tuple\nui.input_float3(label, v1, v2, v3) -&gt; tuple\nui.input_float4(label, v1, v2, v3, v4) -&gt; tuple\nui.input_int(label, value) -&gt; int\nui.input_text(label, text, max_length=256) -&gt; str\nui.input_text_multiline(label, text, max_length=1024) -&gt; str\n</code></pre>"},{"location":"api/#colors","title":"Colors","text":"<pre><code>ui.color_edit3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_edit4(label, r, g, b, a, flags=0) -&gt; tuple[float, float, float, float]\nui.color_picker3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_picker4(label, r, g, b, a, flags=0) -&gt; tuple\n</code></pre>"},{"location":"api/#selection","title":"Selection","text":"<pre><code>ui.combo(label, current: int, items: list[str]) -&gt; int\nui.listbox(label, current: int, items: list[str], height_items=-1) -&gt; int\nui.tree_node(label: str) -&gt; bool\nui.tree_pop() -&gt; None\nui.collapsing_header(label: str) -&gt; bool\nui.selectable(label: str, selected: bool = False) -&gt; bool\n</code></pre>"},{"location":"api/#tabs","title":"Tabs","text":"<pre><code>ui.begin_tab_bar(id: str) -&gt; bool\nui.end_tab_bar() -&gt; None\nui.begin_tab_item(label: str) -&gt; bool\nui.end_tab_item() -&gt; None\n</code></pre>"},{"location":"api/#display","title":"Display","text":"<pre><code>ui.progress_bar(fraction, sx=-1.0, sy=0.0, overlay=\"\")\nui.image(texture_id: int, width, height, uv0x=0, uv0y=0, uv1x=1, uv1y=1)\nui.image_button(id: str, texture_id: int, width, height) -&gt; bool\nui.plot_lines(label, values: list[float], offset=0, overlay=\"\", ...)\nui.plot_histogram(label, values: list[float], offset=0, overlay=\"\", ...)\n</code></pre>"},{"location":"api/#layout_1","title":"Layout","text":"<pre><code>ui.separator()\nui.same_line(offset=0.0, spacing=-1.0)\nui.new_line()\nui.spacing()\nui.dummy(width, height)\nui.indent(width=0.0)\nui.unindent(width=0.0)\nui.begin_group()\nui.end_group()\nui.push_item_width(width)\nui.pop_item_width()\nui.columns(count=1, id=None, border=True)\nui.next_column()\n</code></pre>"},{"location":"api/#tables","title":"Tables","text":"<pre><code>ui.begin_table(id: str, columns: int, flags=0) -&gt; bool\nui.end_table()\nui.table_next_row(flags=0, min_row_height=0.0)\nui.table_next_column() -&gt; bool\nui.table_set_column_index(index: int) -&gt; bool\nui.table_setup_column(label: str, flags=0, init_width=0.0)\nui.table_headers_row()\n</code></pre>"},{"location":"api/#menus","title":"Menus","text":"<pre><code>ui.begin_main_menu_bar() -&gt; bool\nui.end_main_menu_bar()\nui.begin_menu_bar() -&gt; bool\nui.end_menu_bar()\nui.begin_menu(label: str, enabled=True) -&gt; bool\nui.end_menu()\nui.menu_item(label: str, shortcut=\"\", selected=False, enabled=True) -&gt; bool\n</code></pre>"},{"location":"api/#popups","title":"Popups","text":"<pre><code>ui.open_popup(id: str)\nui.begin_popup(id: str) -&gt; bool\nui.begin_popup_modal(name: str, flags=0) -&gt; bool\nui.end_popup()\nui.close_current_popup()\n</code></pre>"},{"location":"api/#tooltips","title":"Tooltips","text":"<pre><code>ui.begin_tooltip()\nui.end_tooltip()\nui.set_tooltip(text: str)\n</code></pre>"},{"location":"api/#id-stack","title":"ID Stack","text":"<pre><code>ui.push_id_str(id: str)\nui.push_id_int(id: int)\nui.pop_id()\nui.get_id(id: str) -&gt; int\n</code></pre>"},{"location":"api/#style","title":"Style","text":"<pre><code>ui.push_style_color(idx: int, r, g, b, a)\nui.pop_style_color(count=1)\nui.push_style_var_float(idx: int, value: float)\nui.push_style_var_vec2(idx: int, x: float, y: float)\nui.pop_style_var(count=1)\nui.style_colors_dark()\nui.style_colors_light()\nui.style_colors_classic()\n</code></pre>"},{"location":"api/#cursor-window-info","title":"Cursor &amp; Window Info","text":"<pre><code>ui.get_cursor_pos() -&gt; tuple[float, float]\nui.set_cursor_pos(x, y)\nui.get_content_region_avail() -&gt; tuple[float, float]\nui.get_window_size() -&gt; tuple[float, float]\nui.get_window_pos() -&gt; tuple[float, float]\nui.set_next_window_pos(x, y, cond=0)\nui.set_next_window_size(width, height, cond=0)\n</code></pre>"},{"location":"api/#docking","title":"Docking","text":"<pre><code>ui.dock_space_over_viewport(flags=0) -&gt; int\nui.dock_space(id: int, sx=0.0, sy=0.0, flags=0) -&gt; int\nui.set_next_window_dock_id(dock_id: int, cond=0)\nui.dock_builder_add_node(node_id=0, flags=0) -&gt; int\nui.dock_builder_remove_node(node_id: int)\nui.dock_builder_set_node_size(node_id, width, height)\nui.dock_builder_set_node_pos(node_id, x, y)\nui.dock_builder_split_node(node_id, split_dir, ratio) -&gt; tuple[int, int]\nui.dock_builder_dock_window(window_name: str, node_id: int)\nui.dock_builder_finish(node_id: int)\nui.dock_builder_get_node(node_id: int) -&gt; int\n</code></pre>"},{"location":"api/#drawing","title":"Drawing","text":"<pre><code>ui.draw_line(x1, y1, x2, y2, col=0xFFFFFFFF, thickness=1.0)\nui.draw_rect(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_rect_filled(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_circle(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_circle_filled(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_text(x, y, col: int, text: str)\nui.bg_draw_image(texture_id, x1, y1, x2, y2)\n</code></pre>"},{"location":"api/#input-state","title":"Input State","text":"<pre><code>ui.is_item_hovered() -&gt; bool\nui.is_item_active() -&gt; bool\nui.is_item_clicked() -&gt; bool\nui.is_item_focused() -&gt; bool\nui.is_item_edited() -&gt; bool\nui.is_item_deactivated_after_edit() -&gt; bool\nui.get_mouse_pos() -&gt; tuple[float, float]\nui.is_mouse_clicked(button: int) -&gt; bool\nui.is_mouse_double_clicked(button: int) -&gt; bool\nui.is_mouse_dragging(button: int, lock_threshold=-1.0) -&gt; bool\nui.get_mouse_drag_delta(button=0, lock_threshold=-1.0) -&gt; tuple[float, float]\nui.is_key_pressed(key: int) -&gt; bool\nui.is_key_down(key: int) -&gt; bool\n</code></pre>"},{"location":"api/#utility","title":"Utility","text":"<pre><code>ui.get_io_framerate() -&gt; float\nui.get_io_delta_time() -&gt; float\nui.get_time() -&gt; float\nui.get_frame_count() -&gt; int\nui.get_display_size() -&gt; tuple[float, float]\nui.col32(r: int, g: int, b: int, a: int = 255) -&gt; int\nui.show_demo_window()\nui.show_metrics_window()\n</code></pre>"},{"location":"api/#internal-helpers","title":"Internal Helpers","text":""},{"location":"api/#vultorch_normalize_tensor","title":"<code>vultorch._normalize_tensor()</code>","text":"<pre><code>def _normalize_tensor(tensor) -&gt; tuple[Tensor, int, int, int]\n</code></pre> <p>Normalize tensor dtype and shape for display. Returns <code>(tensor, height, width, channels)</code>.</p> <ul> <li>Converts <code>uint8</code> \u2192 <code>float32</code> (\u00f7 255), <code>float16</code> \u2192 <code>float32</code>.</li> <li>Accepts 2D <code>(H, W)</code> and 3D <code>(H, W, C)</code> with C \u2208 {1, 3, 4}.</li> <li>Raises <code>ValueError</code> for unsupported dtype, shape, or channel count.</li> </ul>"},{"location":"zh/","title":"Vultorch \u6559\u7a0b","text":"<p>\u9010\u6b65\u5b66\u4e60 Vultorch\uff0c\u6bcf\u4e2a\u7ae0\u8282\u5bf9\u5e94 <code>examples/</code> \u76ee\u5f55\u4e2d\u7684\u4e00\u4e2a\u53ef\u8fd0\u884c\u811a\u672c\u3002</p> \u7ae0\u8282 \u4e3b\u9898 \u6838\u5fc3\u6982\u5ff5 01 \u2014 Hello Tensor \u6700\u5c0f\u793a\u4f8b View, Panel, Canvas, bind, run 02 \u2014 \u591a\u9762\u677f \u591a\u9762\u677f\u4e0e\u591a\u753b\u5e03 \u5e03\u5c40, side, \u591a\u753b\u5e03 03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5 \u62df\u5408 GT \u56fe\u50cf \u81ea\u5b9a\u4e49\u505c\u9760\u5e03\u5c40, create_tensor, \u9010\u50cf\u7d20\u4f18\u5316 04 \u2014 \u5eb7\u5a01\u751f\u547d\u6e38\u620f GPU \u5143\u80de\u81ea\u52a8\u673a create_tensor \u7528\u4e8e\u6a21\u62df, filter=\"nearest\", \u4fa7\u8fb9\u680f, \u6309\u94ae, \u989c\u8272\u9009\u62e9\u5668 05 \u2014 \u56fe\u7247\u67e5\u770b\u5668 \u52a0\u8f7d\u3001\u53d8\u6362\u3001\u4fdd\u5b58\u56fe\u7247 imread, imwrite, Canvas.save, combo, input_text, \u6ee4\u6ce2\u5207\u6362 06 \u2014 \u50cf\u7d20\u753b\u677f \u5728 GPU tensor \u4e0a\u4ea4\u4e92\u5f0f\u7ed8\u753b \u9f20\u6807\u4ea4\u4e92, \u5c4f\u5e55\u2192\u50cf\u7d20\u6620\u5c04, \u5e95\u56fe\u6a21\u5f0f 07 \u2014 \u591a\u901a\u9053\u67e5\u770b\u5668 RGB + depth + normal + alpha \u540c\u5c4f \u591a\u4e2a\u96f6\u62f7\u8d1d tensor, turbo \u8272\u56fe, \u5149\u7ebf-\u7403\u4f53\u6c42\u4ea4 08 \u2014 GT vs \u9884\u6d4b \u5b9e\u65f6\u8bad\u7ec3\u5bf9\u6bd4\u4e0e\u8bef\u5dee\u70ed\u529b\u56fe \u8bef\u5dee\u70ed\u529b\u56fe, PSNR, loss \u66f2\u7ebf, \u8bef\u5dee\u6a21\u5f0f\u5207\u6362 09 \u2014 \u5b9e\u65f6\u8d85\u53c2\u6570\u8c03\u4f18 \u8fd0\u884c\u65f6\u4fee\u6539 LR\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570 step()/end_step(), \u5bf9\u6570 LR, \u4f18\u5316\u5668\u70ed\u5207\u6362 10 \u2014 \u4e8c\u7ef4\u9ad8\u65af\u6cfc\u6e85 \u53ef\u5fae\u5206\u4e8c\u7ef4\u9ad8\u65af\u6e32\u67d3 nn.Parameter, alpha \u5408\u6210, cumprod \u900f\u5c04\u7387 11 \u2014 3D \u8868\u9762\u68c0\u67e5\u5668 \u5e26 Blinn-Phong \u5149\u7167\u7684\u8f68\u9053\u76f8\u673a SceneView, Camera, Light, MSAA, \u7a0b\u5e8f\u5316\u7eb9\u7406 12 \u2014 \u795e\u7ecf\u6e32\u67d3\u5de5\u4f5c\u7ad9 \u538b\u8f74\uff1a\u53cc\u5934 MLP \u516d\u9762\u677f\u5de5\u4f5c\u7ad9 \u53cc\u5934 MLP, \u516d\u9762\u677f, \u6682\u505c/\u6062\u590d, \u5feb\u7167, \u4f18\u5316\u5668\u70ed\u5207\u6362 13 \u2014 \u8d2a\u5403\u86c7 RL DQN \u5b66\u4e60\u73a9\u8d2a\u5403\u86c7 RL \u53ef\u89c6\u5316, DQN, \u03b5-\u8d2a\u5fc3, Q \u503c\u70ed\u529b\u56fe, \u624b\u52a8\u6a21\u5f0f"},{"location":"zh/01_hello_tensor/","title":"01 \u2014 Hello Tensor","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/01_hello_tensor.py</code></p> <p>\u4f60\u662f\u5426\u53d7\u591f\u4e86\u6bcf\u4e2a\u4ed3\u5e93\u90fd\u6709\u4e00\u5957\u81ea\u5df1\u5b9e\u73b0\u7684\u53ef\u89c6\u5316\u65b9\u6848 \u2014\u2014 \u8fd9\u4e2a\u7528 matplotlib \u624b\u52a8\u5237\u65b0\uff0c \u90a3\u4e2a\u7528 <code>cv2.imshow</code> \u7136\u540e <code>waitKey(1)</code>\uff0c\u8fd8\u6709\u7684\u76f4\u63a5\u5b58 PNG \u8ba9\u4f60\u5f00\u4e2a\u56fe\u7247\u67e5\u770b\u5668\uff1f</p> <p>Vultorch \u53ea\u9700 \u56db\u884c\u4ee3\u7801 \u5c31\u628a CUDA tensor \u642c\u5230\u5c4f\u5e55\u4e0a\u3002\u4e0d\u5b58\u56fe\u3001\u4e0d\u8fc7 CPU\u3001 \u4e0d\u9700\u8981 <code>plt.pause(0.001)</code> \u8fd9\u79cd\u9ed1\u9b54\u6cd5\u3002</p>"},{"location":"zh/01_hello_tensor/#_1","title":"\u4f60\u9700\u8981\u8bb0\u4f4f\u7684\u4e1c\u897f","text":"<p>\u5982\u679c\u4f60\u7528\u8fc7 matplotlib\uff0c\u90a3\u4f60\u5df2\u7ecf\u719f\u6089\u8fd9\u79cd\u5957\u5a03\uff1afigure \u2192 axes \u2192 plot\u3002 Vultorch \u662f\u540c\u6837\u7684\u601d\u8def\uff0c\u53ea\u4e0d\u8fc7\u662f\u7ed9 GPU tensor \u7528\u7684\uff1a</p> <pre><code>View          \u2190 \u64cd\u4f5c\u7cfb\u7edf\u7a97\u53e3\uff08\u7c7b\u6bd4 plt.figure\uff09\n \u2514\u2500 Panel     \u2190 \u7a97\u53e3\u91cc\u7684\u4e00\u5757\u533a\u57df\uff08\u7c7b\u6bd4 plt.subplot\uff09\n     \u2514\u2500 Canvas  \u2190 \u663e\u793a tensor \u7684\u69fd\u4f4d\uff08\u7c7b\u6bd4 ax.imshow\uff09\n         \u2514\u2500 bind(tensor)  \u2190 \u628a\u6570\u636e\u63a5\u4e0a\u53bb\n</code></pre> <p>\u4e00\u5171\u5c31\u56db\u4e2a\u5bf9\u8c61\uff1a</p> \u5bf9\u8c61 \u53ef\u4ee5\u7406\u89e3\u4e3a\u2026\u2026 \u5199\u6cd5 View \u5c4f\u5e55\u4e0a\u7684\u90a3\u4e2a\u7a97\u53e3 <code>vultorch.View(\"title\", w, h)</code> Panel \u7a97\u53e3\u91cc\u7684\u4e00\u4e2a\u77e9\u5f62\u533a\u57df <code>view.panel(\"name\")</code> Canvas \u533a\u57df\u91cc\u7684\u4e00\u4e2a\u201c\u76f8\u6846\u201d <code>panel.canvas(\"name\")</code> bind() \u628a\u4f60\u7684 tensor \u9489\u5230\u76f8\u6846\u91cc <code>canvas.bind(t)</code> <p>\u94fe\u8d77\u6765\uff0c\u8c03 <code>run()</code>\uff0c\u6536\u5de5\u3002</p> <p>\u9762\u677f\u5230\u5e95\u662f\u4ec0\u4e48\uff1f</p> <p>Panel\uff08\u9762\u677f\uff09\u5c31\u662f\u7a97\u53e3\u91cc\u4e00\u4e2a\u53ef\u4ee5\u62d6\u52a8\u3001\u8c03\u6574\u5927\u5c0f\u7684\u5b50\u7a97\u53e3\u3002 \u60f3\u8c61\u6210\u684c\u9762\u4e0a\u7684\u4fbf\u7b7e\u7eb8 \u2014\u2014 \u4f60\u53ef\u4ee5\u968f\u4fbf\u62d6\u5b83\u3001\u628a\u5b83\u8d34\u5230\u7a97\u53e3\u8fb9\u7f18\u3001 \u6216\u8005\u548c\u5176\u4ed6\u9762\u677f\u53e0\u5728\u4e00\u8d77\u3002\u4f60\u4e0d\u9700\u8981\u7ba1\u8fd9\u4e9b \u2014\u2014 Vultorch \u4f1a\u81ea\u52a8\u5e2e\u4f60\u6392\u597d\u3002 \u4e0b\u4e00\u7ae0\u4f1a\u8be6\u7ec6\u8bb2\u3002</p>"},{"location":"zh/01_hello_tensor/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\n# \u4e00\u5f20 256\u00d7256 \u7684 RGB \u6e10\u53d8 \u2014 \u4efb\u4f55 (H,W,C) float32 CUDA tensor \u90fd\u884c\nH, W = 256, 256\nx = torch.linspace(0, 1, W, device=\"cuda\")\ny = torch.linspace(0, 1, H, device=\"cuda\")\nt = torch.stack([\n    x.unsqueeze(0).expand(H, W),\n    y.unsqueeze(1).expand(H, W),\n    torch.full((H, W), 0.3, device=\"cuda\"),\n], dim=-1)  # (256, 256, 3)\n\nview   = vultorch.View(\"01 - Hello Tensor\", 512, 512)\npanel  = view.panel(\"Viewer\")\ncanvas = panel.canvas(\"gradient\")\ncanvas.bind(t)\nview.run()  # \u963b\u585e\uff0c\u76f4\u5230\u4f60\u5173\u95ed\u7a97\u53e3\n</code></pre> <p>\u6ca1\u4e86\u3002\u4e0d\u9700\u8981\u624b\u5199\u4e8b\u4ef6\u5faa\u73af\uff0c\u4e0d\u9700\u8981 <code>begin_frame()</code> / <code>end_frame()</code>\u3002</p>"},{"location":"zh/01_hello_tensor/#_3","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 \u6211\u4eec\u5728 CUDA \u4e0a\u9020\u4e86\u4e00\u5f20 RGB \u6e10\u53d8\u3002Vultorch \u652f\u6301    <code>(H,W)</code> / <code>(H,W,1)</code> / <code>(H,W,3)</code> / <code>(H,W,4)</code>\uff0c    float32 / float16 / uint8 \u90fd\u884c\uff0cRGBA \u6269\u5c55\u5b83\u81ea\u5df1\u641e\u5b9a\u3002</p> </li> <li> <p>\u5bf9\u8c61\u6811 \u2014 <code>View</code> \u2192 <code>Panel</code> \u2192 <code>Canvas</code> \u2192 <code>bind(tensor)</code>\u3002    \u753b\u5e03\u9ed8\u8ba4\u94fa\u6ee1\u6574\u4e2a\u9762\u677f\uff08<code>fit=True</code>\uff09\uff0c\u610f\u601d\u662f\u5b83\u4f1a\u81ea\u52a8\u62c9\u4f38    \u586b\u6ee1\u6240\u6709\u53ef\u7528\u7a7a\u95f4 \u2014\u2014 \u5c31\u50cf\u4e00\u4e2a <code>imshow</code> \u586b\u6ee1\u6574\u4e2a figure \u4e00\u6837\u3002</p> </li> <li> <p>\u8fd0\u884c \u2014 <code>view.run()</code> \u662f\u4e00\u4e2a\u963b\u585e\u5faa\u73af\uff08\u8ddf <code>plt.show()</code> \u7c7b\u4f3c\uff09\u3002    \u5b83\u4fdd\u6301\u7a97\u53e3\u6253\u5f00\uff0c\u6bcf\u79d2\u91cd\u65b0\u753b tensor \u7ea6 60 \u6b21\uff0c\u5e76\u5e2e\u4f60\u5904\u7406    \u64cd\u4f5c\u7cfb\u7edf\u4e8b\u4ef6\uff08\u7f29\u653e\u7a97\u53e3\u3001\u70b9\u5173\u95ed\u7b49\uff09\u3002\u5173\u7a97\u53e3\u5c31\u9000\u51fa \u2014\u2014    <code>run()</code> \u8fd4\u56de\u540e\u4f60\u7684 Python \u811a\u672c\u7ee7\u7eed\u5f80\u4e0b\u8dd1\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u56db\u884c\u521d\u59cb\u5316\u53ef\u4ee5\u538b\u6210\u4e00\u884c\uff0c\u5982\u679c\u4f60\u559c\u6b22\u70ab\u6280\u7684\u8bdd\uff1a <code>view.panel(\"Viewer\").canvas(\"gradient\").bind(t)</code></p> <p>\u4e3a\u4ec0\u4e48\u4e0d\u7528 plt.imshow\uff1f</p> <p>matplotlib \u4f1a\u628a\u4f60\u7684 tensor \u62f7\u5230 CPU\uff0c\u8f6c\u6210 numpy \u6570\u7ec4\uff0c\u7528 CPU \u6e32\u67d3\uff0c\u518d\u7ed8\u5230\u7a97\u53e3\u4e0a\u3002Vultorch \u628a\u6574\u4e2a\u6d41\u7a0b\u7559\u5728 GPU \u4e0a \u2014\u2014 tensor \u901a\u8fc7 Vulkan \u4ece CUDA \u663e\u5b58\u76f4\u63a5\u523b\u5230\u5c4f\u5e55\u3002\u6240\u4ee5\u5373\u4f7f\u662f\u5927\u56fe\u7247\uff0c \u4e5f\u80fd\u7a33\u5b9a 60 FPS \u5237\u65b0\u3002</p>"},{"location":"zh/02_multi_panel/","title":"02 \u2014 \u591a\u9762\u677f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/02_imgui_controls.py</code></p> <p>\u4e00\u4e2a\u9762\u677f\u633a\u597d\u7684\u3002\u4f46\u5b9e\u9645\u505a\u7814\u7a76\u7684\u65f6\u5019\uff0c\u4f60\u60f3\u540c\u65f6\u770b loss map\u3001\u68af\u5ea6\u573a\u3001 \u6a21\u578b\u8f93\u51fa \u2014\u2014 \u800c\u4e14\u6700\u597d\u4e0d\u7528\u81ea\u5df1\u5199\u4efb\u4f55\u5e03\u5c40\u4ee3\u7801\u3002</p> <p>\u597d\u6d88\u606f\uff1aVultorch \u7684\u9762\u677f\u4f1a\u201c\u81ea\u52a8\u6392\u5217\u201d\u3002\u4f60\u53ea\u7ba1\u521b\u5efa\uff0c\u5b83\u4eec\u5c31\u4f1a\u50cf\u5e7b\u706f\u7247\u4e00\u6837 \u5728\u7a97\u53e3\u91cc\u4e00\u5c42\u5c42\u53e0\u597d\u3002\u60f3\u628a\u67d0\u4e2a\u9762\u677f\u653e\u5230\u53f3\u8fb9\uff1f\u4f20\u4e2a <code>side=\"right\"</code>\uff0c Vultorch \u5c31\u5e2e\u4f60\u628a\u7a97\u53e3\u5207\u5f00 \u2014\u2014 \u4e0d\u7528\u5199 CSS\uff0c\u4e0d\u7528\u7b97\u5750\u6807\uff0c\u4e0d\u7528\u8ddf <code>plt.subplot</code> \u7684\u7f16\u53f7\u6597\u667a\u6597\u52c7\u3002</p> <p>\u66f4\u723d\u7684\u662f\uff1a\u7a0b\u5e8f\u8dd1\u8d77\u6765\u4ee5\u540e\uff0c\u4f60\u53ef\u4ee5\u7528\u9f20\u6807\u62d6\u9762\u677f\u7684\u6807\u9898\u680f\u6765\u91cd\u65b0\u6392\u5217\uff0c \u62d6\u8fb9\u6846\u6765\u8c03\u5927\u5c0f\uff0c\u751a\u81f3\u628a\u5b83\u62d6\u51fa\u6765\u53d8\u6210\u4e00\u4e2a\u72ec\u7acb\u7684\u6d6e\u52a8\u7a97\u53e3\u3002 \u5c31\u50cf\u4e00\u4e2a\u4e0d\u7528\u914d\u7f6e\u7684\u5e73\u94fa\u5f0f\u7a97\u53e3\u7ba1\u7406\u5668\u3002</p> <p>\u672c\u7ae0\u6f14\u793a\u4e24\u79cd\u6a21\u5f0f\uff1a</p> <ul> <li>\u6bcf\u4e2a\u9762\u677f\u4e00\u4e2a\u753b\u5e03 \u2014 \u4e09\u4e2a\u9762\u677f\u5782\u76f4\u5806\u53e0\uff0c\u5404\u81ea\u4e00\u5f20\u56fe\u3002</li> <li>\u4e00\u4e2a\u9762\u677f\u591a\u4e2a\u753b\u5e03 \u2014 \u4e00\u4e2a\u9762\u677f\u91cc\u585e\u4e09\u4e2a\u753b\u5e03\uff0c\u81ea\u52a8\u5747\u5206\u3002</li> </ul>"},{"location":"zh/02_multi_panel/#_1","title":"\u5e03\u5c40","text":"<p>\u7a97\u53e3\u957f\u8fd9\u6837\uff1a</p> \u5de6\u4fa7\uff08\u4e3b\u533a\u57df\uff09 \u53f3\u4fa7\uff08<code>side=\"right\"</code>\uff09 Red \u9762\u677f \u2014 <code>red_img</code> Combined \u9762\u677f Green \u9762\u677f \u2014 <code>green_img</code> <code>c_red</code> \u753b\u5e03 Blue \u9762\u677f \u2014 <code>blue_img</code> <code>c_green</code> \u753b\u5e03 <code>c_blue</code> \u753b\u5e03 <p>\u5de6\u8fb9\uff1a3 \u4e2a\u72ec\u7acb\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\u3002\u53f3\u8fb9\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\u5171\u4eab\u7a7a\u95f4\u3002</p>"},{"location":"zh/02_multi_panel/#_2","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nH, W = 128, 128\ndevice = \"cuda\"\n\n# \u4e09\u5f20\u4e0d\u540c\u7684 tensor\nx = torch.linspace(0, 1, W, device=device)\nred = torch.zeros(H, W, 3, device=device)\nred[:, :, 0] = x.unsqueeze(0).expand(H, W)\n\ny = torch.linspace(0, 1, H, device=device)\ngreen = torch.zeros(H, W, 3, device=device)\ngreen[:, :, 1] = y.unsqueeze(1).expand(H, W)\n\nblue = torch.zeros(H, W, 3, device=device)\ncx = (torch.arange(W, device=device) // 32) % 2\ncy = (torch.arange(H, device=device) // 32) % 2\nblue[:, :, 2] = (cx.unsqueeze(0) ^ cy.unsqueeze(1)).float()\n\nview = vultorch.View(\"02 - Multi-Panel\", 1200, 600)\n\n# \u5de6\u4fa7\uff1a3 \u4e2a\u9762\u677f\uff0c\u5404\u4e00\u4e2a\u753b\u5e03\npanel_r = view.panel(\"Red\")\npanel_g = view.panel(\"Green\")\npanel_b = view.panel(\"Blue\")\npanel_r.canvas(\"red_img\").bind(red)\npanel_g.canvas(\"green_img\").bind(green)\npanel_b.canvas(\"blue_img\").bind(blue)\n\n# \u53f3\u4fa7\uff1a1 \u4e2a\u9762\u677f\uff0c3 \u4e2a\u753b\u5e03\uff08\u81ea\u52a8\u5782\u76f4\u5747\u5206\uff09\ncombined = view.panel(\"Combined\", side=\"right\", width=0.5)\ncombined.canvas(\"c_red\").bind(red)\ncombined.canvas(\"c_green\").bind(green)\ncombined.canvas(\"c_blue\").bind(blue)\n\nview.run()\n</code></pre>"},{"location":"zh/02_multi_panel/#_3","title":"\u8981\u70b9","text":"<ol> <li> <p>\u81ea\u52a8\u5e03\u5c40 \u2014 \u4e0d\u5199 <code>side=</code> \u7684\u9762\u677f\u81ea\u52a8\u5782\u76f4\u5806\u53e0\uff0c\u5c31\u50cf    <code>plt.subplot(3, 1, ...)</code> \u4f46\u4e0d\u7528\u4f60\u7b97\u7f16\u53f7\u3002</p> </li> <li> <p><code>side=\"right\"</code> + <code>width=0.5</code> \u2014 \u610f\u601d\u662f\u201c\u628a\u7a97\u53e3\u5207\u5f00\uff0c    \u628a\u53f3\u8fb9 50% \u7ed9\u8fd9\u4e2a\u9762\u677f\u201d\u3002<code>0.5</code> \u662f\u6bd4\u4f8b\uff0c\u4e0d\u662f\u50cf\u7d20\u3002    <code>width=0.3</code> \u5c31\u662f\u53f3\u8fb9\u5360 30%\u3002    \u53ef\u9009\u65b9\u5411\u6709 <code>\"left\"</code>\u3001<code>\"right\"</code>\u3001<code>\"top\"</code>\u3001<code>\"bottom\"</code>\u3002</p> </li> <li> <p>\u591a\u753b\u5e03 \u2014 \u5bf9\u540c\u4e00\u4e2a\u9762\u677f\u591a\u6b21\u8c03\u7528 <code>panel.canvas()</code>\u3002    \u591a\u4e2a <code>fit=True</code>\uff08\u9ed8\u8ba4\uff09\u7684\u753b\u5e03\u4f1a\u81ea\u52a8\u5747\u5206\u9762\u677f\u7684\u9ad8\u5ea6\u3002    <code>fit=True</code> \u7684\u610f\u601d\u5c31\u662f\u201c\u62c9\u4f38\u586b\u6ee1\u53ef\u7528\u7a7a\u95f4\u201d \u2014\u2014    \u8ddf <code>imshow</code> \u586b\u6ee1\u6574\u4e2a axes \u4e00\u4e2a\u9053\u7406\u3002\u4e0d\u7528\u624b\u7b97\u9ad8\u5ea6\u3002</p> </li> <li> <p>\u4f9d\u7136\u4e0d\u9700\u8981\u56de\u8c03 \u2014 \u9759\u6001\u6570\u636e\u53ea\u9700 <code>bind()</code> + <code>run()</code>\u3002    \u52a8\u6001\u66f4\u65b0\u540e\u9762\u7684\u7ae0\u8282\u4f1a\u8bb2\u3002</p> </li> <li> <p>\u968f\u4fbf\u62d6 \u2014 \u8bd5\u8bd5\uff1a\u7528\u9f20\u6807\u6293\u4f4f\u9762\u677f\u7684\u6807\u9898\u680f\u62d6\u5230\u53e6\u4e00\u4e2a\u8fb9\u7f18\uff0c    \u6216\u8005\u62c9\u51fa\u6765\u53d8\u6210\u6d6e\u52a8\u7a97\u53e3\u3002\u8fd0\u884c\u65f6\u7684\u6240\u6709\u9762\u677f\u90fd\u53ef\u4ee5\u7528\u9f20\u6807\u91cd\u65b0\u6392\u5217\u3002</p> </li> </ol> <p>\u8bf4\u660e</p> <p>\u540c\u4e00\u4e2a tensor \u53ef\u4ee5\u540c\u65f6\u7ed1\u5b9a\u591a\u4e2a\u753b\u5e03 \u2014\u2014 <code>red</code> \u540c\u65f6\u51fa\u73b0\u5728\u5de6\u8fb9\u7684 Red \u9762\u677f\u548c\u53f3\u8fb9\u7684 Combined \u9762\u677f\u91cc\u3002</p>"},{"location":"zh/03_training_test/","title":"03 \u2014 \u8bad\u7ec3\u6d4b\u8bd5","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/03_training_test.py</code></p> <p>\u4f60\u6709\u6ca1\u6709\u7ecf\u5386\u8fc7\u8fd9\u79cd\u4e8b\uff1a\u8bad\u7ec3\u8dd1\u4e86\u534a\u5c0f\u65f6\uff0c\u7ec8\u7aef\u91cc loss \u6570\u5b57\u54d7\u54d7\u5f80\u4e0b\u6389\uff0c \u770b\u8d77\u6765\u633a\u6b63\u5e38 \u2014\u2014 \u7ed3\u679c\u4e00\u51fa\u56fe\u53d1\u73b0\u6a21\u578b\u8f93\u51fa\u5168\u662f\u7070\u7684\uff1f</p> <p>\u76ef\u7740\u547d\u4ee4\u884c\u91cc\u7684\u6570\u5b57\u731c\u6a21\u578b\u72b6\u6001\uff0c\u8ddf\u770b\u80a1\u7968 K \u7ebf\u731c\u660e\u5929\u6da8\u8dcc\u4e00\u6837\u4e0d\u9760\u8c31\u3002 \u672c\u7ae0\u76f4\u63a5\u628a GT \u548c\u9884\u6d4b\u5e76\u6392\u653e\u5728\u5c4f\u5e55\u4e0a\u3002\u7f51\u7edc\u5230\u5e95\u6709\u6ca1\u6709\u5728\u5b66\uff0c\u4e00\u773c\u5c31\u77e5\u9053\u3002</p>"},{"location":"zh/03_training_test/#_1","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u5f88\u5c0f\u7684 MLP\uff082 \u2192 64 \u2192 64 \u2192 3\uff09\u53bb\u5b9e\u65f6\u62df\u5408\u4e00\u5f20 256\u00d7256 \u7684 PyTorch logo\u3002 \u7a97\u53e3\u5206\u4e09\u5757\uff1a</p> \u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 GT \u9762\u677f \u2014 \u76ee\u6807\u56fe\uff08\u4f60\u8981\u62df\u5408\u7684\u4e1c\u897f\uff09 \u53f3\u4fa7 Prediction \u9762\u677f \u2014 \u7f51\u7edc\u5b9e\u65f6\u8f93\u51fa\uff0c\u6bcf\u5e27\u5237\u65b0 \u4e0b\u65b9 Info \u9762\u677f \u2014 FPS\u3001loss\u3001\u8fed\u4ee3\u6b21\u6570\u3001\u8fdb\u5ea6\u6761\uff0c\u8fd8\u80fd\u62d6\u6ed1\u6761 <p>\u6240\u6709\u6570\u503c\u90fd\u5728\u753b\u9762\u91cc\uff0c\u4e0d\u7528\u518d\u5728\u7ec8\u7aef\u91cc\u7ffb\u6765\u7ffb\u53bb\u627e\u3002</p>"},{"location":"zh/03_training_test/#_2","title":"\u65b0\u670b\u53cb","text":"<p>\u524d\u4e24\u7ae0\u90fd\u662f\u9759\u6001\u6570\u636e \u2014\u2014 <code>bind()</code> + <code>run()</code>\uff0c\u5b8c\u4e8b\u3002 \u8fd9\u6b21\u6211\u4eec\u8981\u8ba9\u4e1c\u897f\u201c\u52a8\u8d77\u6765\u201d\uff1a</p> \u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u5199\u6cd5 @view.on_frame \u6bcf\u5e27\u8c03\u7528\u4e00\u6b21\u7684\u51fd\u6570 \u2014\u2014 \u8bad\u7ec3\u4ee3\u7801\u653e\u8fd9\u91cc <code>@view.on_frame</code> @panel.on_frame \u5728\u67d0\u4e2a\u9762\u677f\u5185\u90e8\u8c03\u7528\u7684\u51fd\u6570 \u2014\u2014 \u4ea4\u4e92\u63a7\u4ef6\u653e\u8fd9\u91cc <code>@info_panel.on_frame</code> create_tensor \u521b\u5efa\u4e00\u4e2a\u548c\u663e\u793a\u5c42\u5171\u4eab\u663e\u5b58\u7684 CUDA tensor\uff0c\u5199\u8fdb\u53bb\u7684\u6570\u636e\u7acb\u5373\u51fa\u73b0\u5728\u5c4f\u5e55\u4e0a <code>vultorch.create_tensor(H, W, ...)</code> vultorch.imread \u52a0\u8f7d\u56fe\u7247\uff0c\u96f6\u4f9d\u8d56\uff08\u4e0d\u9700\u8981 PIL\uff09 <code>vultorch.imread(path, channels=3)</code> side=\"bottom\" \u628a\u9762\u677f\u653e\u5230\u7a97\u53e3\u5e95\u90e8 <code>view.panel(\"Info\", side=\"bottom\")</code> <p>\u4ec0\u4e48\u662f\u201c\u63a7\u4ef6\u201d\uff08widget\uff09\uff1f</p> <p>\u63a7\u4ef6\u5c31\u662f\u5c4f\u5e55\u4e0a\u53ef\u4ee5\u770b\u5230\u7684\u4ea4\u4e92\u5143\u7d20 \u2014\u2014 \u6309\u94ae\u3001\u6ed1\u6761\u3001\u6587\u5b57\u6807\u7b7e\u3001 \u8fdb\u5ea6\u6761\u3002\u5728 Vultorch \u91cc\uff0c\u4f60\u901a\u8fc7 <code>panel.text(\"\u4f60\u597d\")</code>\u3001 <code>panel.slider(\"x\", 0, 1)</code> \u8fd9\u6837\u7684 Python \u65b9\u6cd5\u8c03\u7528\u6765\u521b\u5efa\u63a7\u4ef6\u3002 \u4e0d\u9700\u8981 HTML\uff0c\u4e0d\u9700\u8981 CSS\uff0c\u4e0d\u9700\u8981 Qt \u2014\u2014 \u5c31\u662f Python \u51fd\u6570\u8c03\u7528\u3002</p> <p>View \u56de\u8c03\u91cc\u5199 PyTorch \u4ee3\u7801\uff0cPanel \u56de\u8c03\u91cc\u753b\u63a7\u4ef6\uff0c Vultorch \u6bcf\u5e27\u5e2e\u4f60\u628a tensor \u642c\u4e0a\u5c4f\u5e55\u3002</p>"},{"location":"zh/03_training_test/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\n\nclass TinyMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 3),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\n\nH, W = gt.shape[0], gt.shape[1]\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\nmodel = TinyMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# -- \u89c6\u56fe + \u9762\u677f\uff08\u9ad8\u5c42\u58f0\u660e\u5f0f API\uff09------------------------------------\nview = vultorch.View(\"03 - Training Test\", 1280, 760)\ninfo_panel = view.panel(\"Info\", side=\"bottom\", width=0.28)\ngt_panel = view.panel(\"GT\", side=\"left\", width=0.5)\npred_panel = view.panel(\"Prediction\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\n# \u9884\u6d4b\u7528 4 \u901a\u9053 \u2014 GPU \u96f6\u62f7\u8d1d\u663e\u793a\npred_rgba = vultorch.create_tensor(H, W, channels=4, device=device,\n                                   name=\"pred\", window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nstate = {\n    \"iter\": 0,\n    \"loss\": 1.0,\n    \"ema\": 1.0,\n    \"steps_per_frame\": 6,\n}\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n        state[\"ema\"] = state[\"ema\"] * 0.98 + state[\"loss\"] * 0.02\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n\n@info_panel.on_frame\ndef draw_info():\n    info_panel.text(f\"FPS: {view.fps:.1f}\")\n    info_panel.text(f\"Iteration: {state['iter']}\")\n    info_panel.text(f\"Loss (MSE): {state['loss']:.6f}\")\n    info_panel.text(f\"EMA Loss: {state['ema']:.6f}\")\n\n    state[\"steps_per_frame\"] = info_panel.slider_int(\n        \"Steps / Frame\", 1, 32, default=6\n    )\n    progress = min(1.0, state[\"iter\"] / 3000.0)\n    info_panel.progress(progress,\n                        overlay=f\"Training progress {progress * 100:.1f}%\")\n    info_panel.text_wrapped(\n        \"\u5de6\u8fb9\u662f GT\uff0c\u53f3\u8fb9\u662f\u9884\u6d4b\u3002\u60f3\u66f4\u5feb\u6536\u655b\u5c31\u63d0\u9ad8 Steps / Frame\u3002\"\n    )\n\n\nview.run()\n</code></pre> <p>\u641e\u5b9a\u3002\u8dd1\u8d77\u6765\u4e4b\u540e\u4f60\u4f1a\u770b\u5230\u53f3\u8fb9\u90a3\u5768\u7070\u8272\u5728\u51e0\u79d2\u5185\u53d8\u6210 PyTorch logo\u3002</p>"},{"location":"zh/03_training_test/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u6570\u636e \u2014 <code>vultorch.imread</code> \u76f4\u63a5\u628a\u56fe\u7247\u8bfb\u6210 float32 CUDA tensor\uff08\u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 numpy\uff09\u3002    \u5750\u6807\u7528 <code>meshgrid</code> \u5c55\u6210 <code>(H*W, 2)</code>\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684 <code>(x, y)</code> \u5f52\u4e00\u5316\u5230 <code>[-1, 1]</code>\u3002</p> </li> <li> <p>\u6a21\u578b \u2014 \u4e24\u5c42 64 \u5bbd\u7684 MLP\uff0c\u8f93\u5165 <code>(x, y)</code>\uff0c\u8f93\u51fa <code>(r, g, b)</code>\u3002    \u8fd9\u4e2a\u7f51\u7edc\u5c0f\u5230\u53ef\u4ee5\u8dd1\u5728\u56de\u8c03\u91cc\u4e0d\u6389\u5e27\u3002</p> </li> <li> <p>\u5e03\u5c40 \u2014 <code>side=\"bottom\", width=0.28</code> \u628a Info \u653e\u5230\u5e95\u90e8\uff0c    \u5360\u7a97\u53e3\u9ad8\u5ea6\u7684 28%\u3002\uff08\u662f\u7684\uff0c<code>width=</code> \u5728\u9762\u677f\u653e\u5728\u4e0a\u4e0b\u65b9\u65f6\u63a7\u5236\u7684\u662f\u9ad8\u5ea6 \u2014\u2014    \u5b83\u5b9e\u9645\u4e0a\u662f\u201c\u5206\u5272\u65b9\u5411\u4e0a\u7684\u5c3a\u5bf8\u6bd4\u4f8b\u201d\u3002\uff09    <code>side=\"left\", width=0.5</code> \u628a GT \u653e\u5230\u5269\u4f59\u7a7a\u95f4\u7684\u5de6\u534a\u8fb9\u3002    Prediction \u81ea\u52a8\u586b\u6ee1\u5269\u4e0b\u7684\u533a\u57df\u3002</p> </li> <li> <p>\u4e24\u4e2a\u56de\u8c03 \u2014</p> <ul> <li> <p><code>@view.on_frame</code> \u2014 \u6bcf\u5e27\u6267\u884c\u4e00\u6b21\uff0c\u5728\u9762\u677f\u7ed8\u5236\u4e4b\u524d\u8dd1\u3002   \u8bad\u7ec3\u5faa\u73af\u3001\u6570\u636e\u66f4\u65b0\u3001\u6a21\u578b\u8ba1\u7b97\u90fd\u653e\u8fd9\u91cc\u3002</p> </li> <li> <p><code>@info_panel.on_frame</code> \u2014 \u5728 Info \u9762\u677f\u5185\u90e8\u6267\u884c\u3002   \u6bcf\u4e2a <code>panel.text()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.progress()</code>   \u8c03\u7528\u90fd\u4f1a\u5728\u9762\u677f\u91cc\u521b\u5efa\u4e00\u4e2a\u63a7\u4ef6\uff08\u6587\u5b57\u3001\u6ed1\u6761\u3001\u8fdb\u5ea6\u6761\uff09\u3002   \u4e0d\u7528\u64cd\u5fc3\u4f4d\u7f6e \u2014\u2014 \u63a7\u4ef6\u8ddf <code>print()</code> \u4e00\u6837\u81ea\u4e0a\u800c\u4e0b\u6392\u5217\u3002</p> </li> </ul> </li> </ol>"},{"location":"zh/03_training_test/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p><code>@view.on_frame</code> \u2014 \u4e00\u4e2a\u666e\u901a\u7684 Python \u51fd\u6570\uff0c\u6bcf\u663e\u793a\u4e00\u5e27\u8c03\u7528\u4e00\u6b21    \uff08\u5927\u7ea6\u6bcf\u79d2 60 \u6b21\uff09\u3002\u56de\u8c03\u91cc\u53ef\u4ee5\u8dd1\u4efb\u610f PyTorch \u4ee3\u7801\u3002    \u6bcf\u5e27\u7ed3\u675f\u65f6 Vultorch \u81ea\u52a8\u628a\u7ed1\u5b9a\u7684 tensor \u642c\u5230\u5c4f\u5e55\uff0c\u4e0d\u7528\u4f60\u7ba1\u3002</p> </li> <li> <p><code>create_tensor</code> \u2014 \u8ddf\u666e\u901a <code>torch.zeros</code> \u4e00\u6837\u7528\uff0c    \u4f46\u5e95\u5c42\u662f Vulkan/CUDA \u5171\u4eab\u663e\u5b58\u3002\u4f60\u5f80\u91cc\u9762\u5199\u6570\u636e\uff0c    \u4e0b\u4e00\u5e27\u5c4f\u5e55\u5c31\u81ea\u52a8\u66f4\u65b0 \u2014\u2014 \u4e0d\u7528 <code>.cpu()</code>\uff0c\u4e0d\u7528 <code>upload()</code>\uff0c    \u4ec0\u4e48\u90fd\u4e0d\u7528\u3002</p> </li> <li> <p>\u5e03\u5c40\u7b80\u5199 \u2014 <code>side=\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code>    \u5207\u7a97\u53e3\uff0c<code>width=</code> \u63a7\u5236\u5207\u591a\u5927\uff080\u20131 \u7684\u6bd4\u4f8b\uff09\u3002\u5c31\u8fd9\u4e48\u591a\u3002    \u4e0d\u9700\u8981\u5750\u6807\uff0c\u4e0d\u9700\u8981\u7f51\u683c\u3002</p> </li> <li> <p>\u9762\u677f\u63a7\u4ef6 \u2014 <code>@panel.on_frame</code> \u5728\u9762\u677f\u5185\u90e8\u8fd0\u884c\u3002    \u8c03\u7528 <code>panel.text()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.progress()</code> \u2014\u2014    \u6bcf\u4e2a\u8c03\u7528\u521b\u5efa\u4e00\u4e2a\u63a7\u4ef6\uff0c\u81ea\u4e0a\u800c\u4e0b\u6392\u5217\uff0c\u8ddf <code>print()</code> \u8f93\u51fa\u4e00\u6837\u3002</p> </li> <li> <p>\u4e0d\u5237\u7ec8\u7aef \u2014 \u6240\u6709\u72b6\u6001\u4fe1\u606f\u90fd\u5728 Info \u9762\u677f\u91cc\uff0c    \u4f60\u7684\u7ec8\u7aef\u53ef\u4ee5\u7559\u7740\u770b warning \u548c traceback\uff0c\u5e72\u51c0\u591a\u4e86\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p><code>Steps / Frame</code> \u6ed1\u6761\u62c9\u5230 32 \u6536\u655b\u98de\u5feb\u3002 \u4f46\u4e5f\u522b\u592a\u8d2a \u2014\u2014 \u62c9\u592a\u9ad8\u5e27\u7387\u4f1a\u6389\u4e0b\u6765\uff0c\u56e0\u4e3a\u6bcf\u5e27\u7684\u8bad\u7ec3\u65f6\u95f4\u53d8\u957f\u4e86\u3002</p> <p>\u8bf4\u660e</p> <p><code>create_tensor</code> \u53ea\u5728\u521d\u59cb\u5316\u65f6\u8c03\u7528\u4e00\u6b21\uff0c\u4e0d\u5728\u5e27\u5faa\u73af\u91cc\u3002 \u4e4b\u540e\u6bcf\u5e27\u53ea\u9700\u8981\u5f80\u8fd9\u4e2a tensor \u91cc\u5199\u6570\u636e\uff0c\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002</p>"},{"location":"zh/04_conway/","title":"04 \u2014 \u5eb7\u5a01\u751f\u547d\u6e38\u620f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/04_conway.py</code></p> <p>Loss \u66f2\u7ebf\uff0c\u8bad\u7ec3\u53ef\u89c6\u5316 \u2014\u2014 \u524d\u9762\u592a\u6b63\u7ecf\u4e86\u3002 \u6b47\u4e00\u6b47\uff0c\u6765\u641e\u4e2a\u597d\u73a9\u7684\uff1a\u5eb7\u5a01\u751f\u547d\u6e38\u620f\uff0c\u5168\u8dd1\u5728 GPU \u4e0a\uff0c\u96f6\u62f7\u8d1d\u663e\u793a\uff0c \u53f3\u8fb9\u4e00\u6392\u6309\u94ae\u548c\u6ed1\u6761\u8ba9\u4f60\u5f53\u4e0a\u5e1d\u3002</p> <p>\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8fd9\u4e00\u7ae0\u8981\u8bf4\u660e\u4e00\u4ef6\u4e8b\uff1a<code>create_tensor</code> \u4e0d\u53ea\u662f\u7ed9\u8bad\u7ec3\u7528\u7684\u3002 \u4efb\u4f55 GPU \u8ba1\u7b97\u90fd\u53ef\u4ee5\u901a\u8fc7 Vultorch \u7684\u5171\u4eab\u663e\u5b58\u663e\u793a\u51fa\u6765 \u2014\u2014 \u6a21\u62df\u3001 \u7a0b\u5e8f\u5316\u751f\u6210\u3001\u7269\u7406\u4eff\u771f\uff0c\u53ea\u8981\u662f CUDA \u4e0a\u8dd1\u7684\uff0c\u90fd\u884c\u3002</p>"},{"location":"zh/04_conway/#_1","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a 256\u00d7256 \u7684\u5143\u80de\u81ea\u52a8\u673a\uff0c\u5e26\u63a7\u5236\u9762\u677f\uff1a</p> \u533a\u57df \u5185\u5bb9 \u5de6\u4fa7 Controls \u2014 \u64ad\u653e/\u6682\u505c\u3001\u5355\u6b65\u3001\u901f\u5ea6\u6ed1\u6761\u3001\u7ecf\u5178\u56fe\u6848\u6309\u94ae\u3001\u989c\u8272\u9009\u62e9\u5668 \u53f3\u4fa7 Grid \u2014 \u6a21\u62df\u753b\u9762\uff0c\u50cf\u7d20\u7ea7\u7cbe\u786e\uff08<code>filter=\"nearest\"</code>\uff09 <p>\u6574\u4e2a\u6a21\u62df\u8dd1\u5728 GPU \u4e0a\u3002\u663e\u793a\u7528\u7684 tensor \u901a\u8fc7 <code>create_tensor</code> \u5206\u914d\uff0c \u96f6\u62f7\u8d1d \u2014\u2014 \u7f51\u683c\u6570\u636e\u4e0d\u7ecf\u8fc7 CPU\u3002</p>"},{"location":"zh/04_conway/#_2","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u4e3a\u4ec0\u4e48\u91cd\u8981 <code>filter=\"nearest\"</code> \u6bcf\u4e2a\u50cf\u7d20\u663e\u793a\u4e3a\u6e05\u6670\u7684\u65b9\u5757\uff0c\u4e0d\u505a\u6a21\u7cca \u6ca1\u6709\u5b83\u7684\u8bdd\u53cc\u7ebf\u6027\u63d2\u503c\u4f1a\u628a\u683c\u5b50\u8fb9\u754c\u7cca\u6389\u3002\u8ddf <code>plt.imshow(data, interpolation='nearest')</code> \u4e00\u4e2a\u9053\u7406 <code>side=\"left\"</code> \u4fa7\u8fb9\u680f \u628a\u9762\u677f\u653e\u5230\u5de6\u8fb9\uff0c\u5360\u7a97\u53e3 22% \u7ed9\u4f60\u4e00\u4e2a\u6c38\u4e45\u7684\u63a7\u5236\u6761 <code>@panel.on_frame</code> \u9762\u677f\u7ea7\u63a7\u4ef6\u56de\u8c03 \u6309\u94ae\u3001\u6ed1\u6761\u3001\u989c\u8272\u9009\u62e9\u5668\u90fd\u653e\u8fd9\u91cc <code>panel.button(\u6807\u7b7e)</code> \u4e00\u4e2a\u53ef\u70b9\u51fb\u7684\u6309\u94ae \u88ab\u70b9\u51fb\u7684\u90a3\u4e00\u5e27\u8fd4\u56de <code>True</code> <code>with panel.row():</code> \u628a\u63a5\u4e0b\u6765\u7684\u63a7\u4ef6\u653e\u5728\u540c\u4e00\u884c \u9ed8\u8ba4\u63a7\u4ef6\u662f\u4e00\u884c\u4e00\u4e2a\u7684\uff08\u8ddf <code>print()</code> \u4e00\u6837\uff09\u3002\u7528 <code>with panel.row():</code> \u53ef\u4ee5\u628a\u4e24\u4e2a\u6309\u94ae\u5e76\u6392\u3002\u5c31\u662f\u4e2a Python <code>with</code> \u5757 \u2014\u2014 \u5757\u91cc\u7684\u4e1c\u897f\u90fd\u653e\u540c\u4e00\u884c <code>panel.color_picker</code> RGB \u989c\u8272\u9009\u62e9\u5668 \u70b9\u8272\u5757\u6253\u5f00\u8c03\u8272\u677f \u5faa\u73af padding + conv2d GPU \u5e76\u884c\u6570\u90bb\u5c45 \u6574\u4e2a\u6a21\u62df\u5c31\u662f\u4e00\u6b21\u5377\u79ef"},{"location":"zh/04_conway/#_3","title":"\u6a21\u62df\u7684\u6838\u5fc3\u6280\u5de7","text":"<p>\u6570\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u91cc\u7684\u90bb\u5c45\u4e2a\u6570\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u6b21 2D \u5377\u79ef\uff0c\u7528\u4e00\u4e2a 3\u00d73 \u7684\u5377\u79ef\u6838\uff08\u4e2d\u5fc3\u4e3a 0\uff09\uff1a</p> <pre><code>1 1 1\n1 0 1\n1 1 1\n</code></pre> <p>PyTorch \u7684 <code>F.conv2d</code> \u4e00\u4e2a GPU kernel \u8c03\u7528\u5c31\u641e\u5b9a \u2014\u2014 \u4e0d\u9700\u8981\u5faa\u73af\uff0c \u4e0d\u9700\u8981\u9010\u50cf\u7d20\u7684\u903b\u8f91\u3002circular padding \u8ba9\u8fb9\u7f18\u73af\u7ed5\uff0c\u98de\u884c\u5668\u98de\u51fa\u53f3\u8fb9 \u4f1a\u4ece\u5de6\u8fb9\u5192\u51fa\u6765\u3002</p> <pre><code>kernel = torch.tensor([[1, 1, 1],\n                        [1, 0, 1],\n                        [1, 1, 1]], dtype=torch.float32, device=device)\npadded = F.pad(inp, (1, 1, 1, 1), mode='circular')\nneighbours = F.conv2d(padded, kernel.reshape(1, 1, 3, 3)).squeeze()\n</code></pre> <p>\u7136\u540e\u89c4\u5219\u5c31\u662f\u4e24\u4e2a\u5e03\u5c14 mask\uff1a</p> <pre><code>survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\nbirth   = (grid == 0) &amp; (neighbours == 3)\ngrid[:] = (survive | birth).float()\n</code></pre>"},{"location":"zh/04_conway/#_4","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 \u7f51\u683c\u53c2\u6570 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGRID_H, GRID_W = 256, 256\n\n# \u2500\u2500 \u89c6\u56fe + \u9762\u677f \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"04 - Conway's Game of Life\", 1024, 768)\ngrid_panel = view.panel(\"Grid\")\nctrl_panel = view.panel(\"Controls\", side=\"left\", width=0.22)\n\n# \u2500\u2500 \u663e\u793a tensor\uff08RGBA\uff0c\u96f6\u62f7\u8d1d\uff09\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndisplay = vultorch.create_tensor(GRID_H, GRID_W, channels=4,\n                                 device=device, name=\"grid\",\n                                 window=view.window)\ncanvas = grid_panel.canvas(\"grid\", filter=\"nearest\")\ncanvas.bind(display)\n\n# \u2500\u2500 \u6a21\u62df\u72b6\u6001 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ngrid = torch.zeros(GRID_H, GRID_W, dtype=torch.float32, device=device)\n\nstate = {\n    \"running\": False,\n    \"generation\": 0,\n    \"speed\": 1,\n    \"prob\": 0.3,\n    \"alive_color\": (0.0, 1.0, 0.4),\n    \"dead_color\": (0.05, 0.05, 0.08),\n}\n\n\ndef randomize():\n    grid[:] = (torch.rand(GRID_H, GRID_W, device=device) &lt; state[\"prob\"]).float()\n    state[\"generation\"] = 0\n\ndef clear():\n    grid.zero_()\n    state[\"generation\"] = 0\n\ndef step_simulation():\n    kernel = torch.tensor([[1, 1, 1],\n                            [1, 0, 1],\n                            [1, 1, 1]], dtype=torch.float32, device=device)\n    inp = grid.unsqueeze(0).unsqueeze(0)\n    k = kernel.unsqueeze(0).unsqueeze(0)\n    padded = torch.nn.functional.pad(inp, (1, 1, 1, 1), mode='circular')\n    neighbours = torch.nn.functional.conv2d(padded, k).squeeze()\n\n    survive = (grid == 1) &amp; ((neighbours == 2) | (neighbours == 3))\n    birth = (grid == 0) &amp; (neighbours == 3)\n    grid[:] = (survive | birth).float()\n    state[\"generation\"] += 1\n\ndef grid_to_display():\n    alive_r, alive_g, alive_b = state[\"alive_color\"]\n    dead_r, dead_g, dead_b = state[\"dead_color\"]\n    display[:, :, 0] = dead_r + (alive_r - dead_r) * grid\n    display[:, :, 1] = dead_g + (alive_g - dead_g) * grid\n    display[:, :, 2] = dead_b + (alive_b - dead_b) * grid\n    display[:, :, 3] = 1.0\n\nrandomize()\n\n\n@view.on_frame\ndef update():\n    if state[\"running\"]:\n        for _ in range(state[\"speed\"]):\n            step_simulation()\n    grid_to_display()\n\n\n@ctrl_panel.on_frame\ndef draw_controls():\n    ctrl_panel.text(f\"Generation: {state['generation']}\")\n    ctrl_panel.text(f\"Alive cells: {int(grid.sum().item())}\")\n    ctrl_panel.text(f\"FPS: {view.fps:.1f}\")\n    ctrl_panel.separator()\n\n    with ctrl_panel.row():\n        label = \"Pause\" if state[\"running\"] else \"Play\"\n        if ctrl_panel.button(label, width=80):\n            state[\"running\"] = not state[\"running\"]\n        if ctrl_panel.button(\"Step\", width=80):\n            step_simulation()\n\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Randomize\", width=80):\n            randomize()\n        if ctrl_panel.button(\"Clear\", width=80):\n            clear()\n\n    ctrl_panel.separator()\n    state[\"speed\"] = ctrl_panel.slider_int(\"Speed\", 1, 20, default=1)\n    state[\"prob\"] = ctrl_panel.slider(\"Cell Probability\", 0.05, 0.8,\n                                       default=0.3)\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Colors\")\n    state[\"alive_color\"] = ctrl_panel.color_picker(\n        \"Alive\", default=(0.0, 1.0, 0.4))\n    state[\"dead_color\"] = ctrl_panel.color_picker(\n        \"Dead\", default=(0.05, 0.05, 0.08))\n\n    ctrl_panel.separator()\n    ctrl_panel.text(\"Patterns\")\n    with ctrl_panel.row():\n        if ctrl_panel.button(\"Glider\", width=80):\n            clear()\n            grid[1, 2] = 1; grid[2, 3] = 1\n            grid[3, 1] = 1; grid[3, 2] = 1; grid[3, 3] = 1\n        if ctrl_panel.button(\"Pulsar\", width=80):\n            clear()\n            # ... \u653e\u7f6e Pulsar \u56fe\u6848 ...\n        if ctrl_panel.button(\"Gosper Gun\", width=100):\n            clear()\n            # ... \u653e\u7f6e Gosper \u6ed1\u7fd4\u673a\u67aa ...\n\n    ctrl_panel.separator()\n    ctrl_panel.text_wrapped(\n        \"\u70b9 Play \u5f00\u59cb\uff0c\u6216\u8005 Step \u5355\u6b65\u3002\"\n        \"Randomize \u91cd\u65b0\u968f\u673a\u586b\u5145\u3002\"\n    )\n\n\nview.run()\n</code></pre> <p>\uff08\u5b8c\u6574\u7684\u793a\u4f8b\u6587\u4ef6\u4e2d\u5305\u542b\u6240\u6709\u56fe\u6848\u653e\u7f6e\u7684\u8f85\u52a9\u51fd\u6570\u3002\uff09</p>"},{"location":"zh/04_conway/#_5","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<ol> <li> <p>\u7f51\u683c \u2014 \u4e00\u4e2a\u666e\u901a\u7684 <code>(256, 256)</code> float32 CUDA tensor\u3002    <code>1.0</code> \u5c31\u662f\u6d3b\u7684\uff0c<code>0.0</code> \u5c31\u662f\u6b7b\u7684\u3002\u4e0d\u9700\u8981\u7c7b\uff0c\u4e0d\u9700\u8981\u82b1\u54e8\u7684\u6570\u636e\u7ed3\u6784 \u2014\u2014 \u5c31\u4e00\u4e2a tensor\u3002</p> </li> <li> <p>\u6a21\u62df \u2014 <code>step_simulation()</code> \u7528 <code>F.conv2d</code> \u914d\u5408 circular padding \u6570\u90bb\u5c45\uff0c    \u7136\u540e\u7528\u5e03\u5c14 mask \u5e94\u7528\u51fa\u751f/\u5b58\u6d3b\u89c4\u5219\u3002\u6574\u4ee3\u53ea\u9700\u8981\u4e24\u4e2a GPU kernel\u3002</p> </li> <li> <p>\u663e\u793a \u2014 <code>create_tensor</code> \u5206\u914d Vulkan/CUDA \u5171\u4eab\u663e\u5b58\u3002    <code>grid_to_display()</code> \u5728\u5b58\u6d3b\u548c\u6b7b\u4ea1\u989c\u8272\u4e4b\u95f4\u63d2\u503c\uff0c\u5199\u8fdb\u53bb\u5c31\u884c\u3002\u96f6\u62f7\u8d1d\u4e0a\u5c4f\u3002</p> </li> <li> <p>\u63a7\u5236\u9762\u677f \u2014 <code>@ctrl_panel.on_frame</code> \u628a\u6240\u6709\u63a7\u4ef6\u753b\u5728 Controls \u9762\u677f\u5185\u90e8\u3002    <code>panel.button()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.color_picker()</code>\u3001    <code>with panel.row()</code> \u8ba9\u5e03\u5c40\u7d27\u51d1\u3002\u72b6\u6001\u5c31\u7528\u4e00\u4e2a\u666e\u901a Python dict \u7ba1\u7406\u3002</p> </li> </ol>"},{"location":"zh/04_conway/#_6","title":"\u8981\u70b9","text":"<ol> <li> <p><code>create_tensor</code> \u4e0d\u53ea\u662f\u7ed9\u8bad\u7ec3\u7528\u7684 \u2014 \u4efb\u4f55 GPU \u8ba1\u7b97\u53ea\u8981\u4ea7\u51fa    \u7c7b\u4f3c\u56fe\u50cf\u7684 tensor\uff0c\u90fd\u53ef\u4ee5\u96f6\u62f7\u8d1d\u663e\u793a\u3002</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014 \u50cf\u7d20\u753b / \u7f51\u683c\u6a21\u62df\u7684\u5fc5\u9009\u9879\u3002    \u6ca1\u6709\u5b83\u7684\u8bdd\u53cc\u7ebf\u6027\u63d2\u503c\u4f1a\u628a\u683c\u5b50\u8fb9\u754c\u7cca\u6210\u4e00\u5768\u3002    \u8ddf <code>plt.imshow(data, interpolation='nearest')</code> \u4e00\u4e2a\u610f\u601d \u2014\u2014    \u4f60\u60f3\u770b\u5230\u771f\u5b9e\u7684\u50cf\u7d20\u3002</p> </li> <li> <p>\u5377\u79ef = \u6570\u90bb\u5c45 \u2014 \u4e00\u4e2a\u5c0f\u6280\u5de7\uff0c\u7528\u4e00\u4e2a GPU kernel \u66ff\u4ee3\u5d4c\u5957 Python \u5faa\u73af\u3002    \u5373\u4f7f\u7f51\u683c\u5f88\u5927\uff0c\u6e38\u620f\u4e5f\u80fd\u8dd1\u5230\u51e0\u767e FPS\u3002</p> </li> <li> <p>\u9762\u677f\u63a7\u4ef6 \u2014 \u5728 <code>@panel.on_frame</code> \u91cc\u8c03\u7528    <code>panel.button()</code>\u3001<code>panel.slider_int()</code>\u3001<code>panel.color_picker()</code>\u3002    \u6bcf\u4e2a\u8c03\u7528\u521b\u5efa\u4e00\u4e2a\u63a7\u4ef6\uff0c\u81ea\u4e0a\u800c\u4e0b\u6392\u5217\uff0c\u8ddf <code>print()</code> \u8f93\u51fa\u4e00\u6837\u3002    \u4e0d\u9700\u8981\u5199\u4efb\u4f55\u5b9a\u4f4d\u4ee3\u7801\u3002</p> </li> <li> <p><code>with panel.row():</code> \u2014 \u9ed8\u8ba4\u63a7\u4ef6\u6bcf\u884c\u4e00\u4e2a\u3002    \u628a\u51e0\u4e2a\u63a7\u4ef6\u8c03\u7528\u5305\u5728 <code>with panel.row():</code> \u91cc\uff0c\u5b83\u4eec\u5c31\u4f1a\u5e76\u6392\u5728\u540c\u4e00\u884c\u3002    \u5c31\u662f\u4e2a Python <code>with</code> \u5757\uff0c\u6ca1\u4ec0\u4e48\u590d\u6742\u7684\u3002</p> </li> <li> <p>\u7ecf\u5178\u56fe\u6848 \u2014 Glider\u3001Pulsar\u3001Gosper Gun \u6309\u94ae\u5c55\u793a\u4e86\u600e\u4e48\u901a\u8fc7    \u76f4\u63a5\u5f80 grid tensor \u91cc\u5199\u503c\u6765\u8bbe\u521d\u59cb\u6761\u4ef6\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>Speed \u6ed1\u6761\u62c9\u5230 20\uff0c\u770b\u7f51\u683c\u4ee5\u6bcf\u5e27 20 \u4ee3\u7684\u901f\u5ea6\u6f14\u5316\u3002 \u73b0\u4ee3 GPU \u4e0a\u5e27\u7387\u4f9d\u7136\u7a33\u7a33 60+\u3002</p> <p>\u8bf4\u660e</p> <p>\u7f51\u683c\u73af\u7ed5\u662f\u56e0\u4e3a\u7528\u4e86 <code>mode='circular'</code> padding\u3002 \u6ed1\u7fd4\u673a\u4ece\u53f3\u8fb9\u98de\u51fa\u53bb\u4f1a\u4ece\u5de6\u8fb9\u5192\u51fa\u6765\u3002</p>"},{"location":"zh/05_image_viewer/","title":"05 \u2014 \u56fe\u7247\u67e5\u770b\u5668","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/05_image_viewer.py</code></p> <p>\u76ee\u524d\u4e3a\u6b62\u6211\u4eec\u4e00\u76f4\u5728\u51ed\u7a7a\u9020 tensor \u2014\u2014 \u6e10\u53d8\u3001\u68cb\u76d8\u683c\u3001\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u3002 \u5f88\u9177\uff0c\u4f46\u603b\u6709\u90a3\u4e48\u4e00\u5929\uff0c\u4f60\u60f3\u770b\u4e00\u5f20\u771f\u6b63\u7684\u56fe\u7247\u3002\u5c31\u662f\u90a3\u79cd\u5b89\u5b89\u9759\u9759\u8eba\u5728 \u786c\u76d8\u4e0a\u7684 <code>.png</code> \u6587\u4ef6\u3002\u50cf\u4e2a\u6b63\u5e38\u4eba\u4e00\u6837\u3002</p> <p>\"\u7528 PIL \u4e0d\u5c31\u884c\u4e86\u3002\" \u597d\uff0c\u7136\u540e\u4f60\u8fd8\u9700\u8981 <code>torchvision.transforms</code>\uff0c \u7136\u540e\u9700\u8981 <code>numpy</code>\uff0c\u7136\u540e\u9700\u8981 <code>cv2.cvtColor</code> \u56e0\u4e3a\u4e0d\u77e5\u9053\u8c01\u628a RGB \u548c BGR \u641e\u53cd\u4e86\uff0c\u7136\u540e\u4f60\u6253\u5f00\u4e86\u4e09\u4e2a Stack Overflow \u6807\u7b7e\u9875\uff0c\u8bd5\u56fe\u641e\u660e\u767d\u4e3a\u4ec0\u4e48\u56fe\u7247 \u4e0a\u4e0b\u98a0\u5012\u800c\u4e14\u504f\u7eff\u3002</p> <p>Vultorch \u5185\u7f6e\u4e86\u56fe\u7247\u8bfb\u5199\u3002\u4e00\u4e2a\u51fd\u6570\u8fdb\uff0c\u4e00\u4e2a\u51fd\u6570\u51fa\u3002 \u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 OpenCV\uff0c\u4e0d\u9700\u8981\u6000\u7591\u4eba\u751f\u3002</p>"},{"location":"zh/05_image_viewer/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u5199\u6cd5 imread \u628a\u56fe\u7247\u6587\u4ef6\u8bfb\u6210 CUDA tensor <code>vultorch.imread(\"photo.png\")</code> imwrite \u628a tensor \u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6 <code>vultorch.imwrite(\"out.png\", t)</code> Canvas.save() \u4fdd\u5b58\u753b\u5e03\u7ed1\u5b9a\u7684 tensor <code>canvas.save(\"out.png\")</code> panel.combo() \u4e0b\u62c9\u9009\u62e9\u83dc\u5355 <code>panel.combo(\"\u9009\u9879\", [\"A\",\"B\"])</code> panel.input_text() \u6587\u672c\u8f93\u5165\u6846 <code>panel.input_text(\"\u8def\u5f84\")</code> canvas.filter \u91c7\u6837\u6a21\u5f0f\uff08<code>\"linear\"</code> / <code>\"nearest\"</code>\uff09 <code>canvas.filter = \"nearest\"</code>"},{"location":"zh/05_image_viewer/#_2","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u8ff7\u4f60\u56fe\u7247\u67e5\u770b\u5668\uff1a\u52a0\u8f7d\u4e00\u5f20\u56fe\uff0c\u4ece\u4e0b\u62c9\u83dc\u5355\u9009\u53d8\u6362\uff0c \u7528\u6ed1\u6761\u8c03\u4eae\u5ea6 / \u5bf9\u6bd4\u5ea6\uff0c\u7136\u540e\u628a\u7ed3\u679c\u5b58\u76d8\u3002</p> \u5de6\u4fa7 \u53f3\u4fa7\uff08\u4e24\u4e2a\u753b\u5e03\uff09 Controls \u2014 \u53d8\u6362\u9009\u62e9\u3001\u4eae\u5ea6/\u5bf9\u6bd4\u5ea6\u6ed1\u6761\u3001\u6ee4\u6ce2\u5207\u6362\u3001\u4fdd\u5b58 \u539f\u56fe\uff08\u4e0a\uff09 \u53d8\u6362\u540e\uff08\u4e0b\uff09"},{"location":"zh/05_image_viewer/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\n\nimport torch\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u2500\u2500 \u52a0\u8f7d\u56fe\u7247 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\noriginal = vultorch.imread(img_path, channels=3, device=device)\nH, W, C = original.shape\n\n# \u7528\u4e8e\u53d8\u6362\u7684\u5de5\u4f5c\u526f\u672c\uff08\u539f\u56fe\u6c38\u8fdc\u4e0d\u52a8\uff09\ntransformed = original.clone()\n\n# \u2500\u2500 \u89c6\u56fe + \u9762\u677f \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nview = vultorch.View(\"05 - Image Viewer\", 1024, 768)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nimg_panel = view.panel(\"Image\")\n\ncanvas_orig = img_panel.canvas(\"Original\")\ncanvas_orig.bind(original)\n\ncanvas_xform = img_panel.canvas(\"Transformed\")\ncanvas_xform.bind(transformed)\n\n# \u2500\u2500 \u72b6\u6001 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTRANSFORMS = [\n    \"None\",\n    \"Horizontal Flip\",\n    \"Vertical Flip\",\n    \"Grayscale\",\n    \"Invert\",\n    \"Sepia\",\n]\n\nstate = {\n    \"brightness\": 0.0,\n    \"contrast\": 1.0,\n    \"last_transform\": -1,\n    \"last_brightness\": None,\n    \"last_contrast\": None,\n}\n\n\ndef apply_transform(img, idx):\n    if idx == 0:    return img.clone()\n    elif idx == 1:  return img.flip(1)               # \u6c34\u5e73\u7ffb\u8f6c\n    elif idx == 2:  return img.flip(0)               # \u5782\u76f4\u7ffb\u8f6c\n    elif idx == 3:                                    # \u7070\u5ea6\n        gray = img[:,:,0]*0.299 + img[:,:,1]*0.587 + img[:,:,2]*0.114\n        return gray.unsqueeze(-1).expand_as(img).contiguous()\n    elif idx == 4:  return 1.0 - img                 # \u53cd\u8272\n    elif idx == 5:                                    # \u590d\u53e4\u8272\u8c03\n        r = img[:,:,0]*0.393 + img[:,:,1]*0.769 + img[:,:,2]*0.189\n        g = img[:,:,0]*0.349 + img[:,:,1]*0.686 + img[:,:,2]*0.168\n        b = img[:,:,0]*0.272 + img[:,:,1]*0.534 + img[:,:,2]*0.131\n        return torch.stack([r, g, b], dim=-1).clamp(0, 1)\n    return img.clone()\n\n\ndef apply_brightness_contrast(img, brightness, contrast):\n    return ((img - 0.5) * contrast + 0.5 + brightness).clamp(0, 1)\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Image: {img_path.name}\")\n    ctrl.text(f\"Size: {W} \u00d7 {H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    # \u53d8\u6362\u9009\u62e9\u5668\n    ctrl.text(\"Transform\")\n    xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n\n    ctrl.separator()\n\n    # \u4eae\u5ea6 / \u5bf9\u6bd4\u5ea6\n    ctrl.text(\"Adjustments\")\n    brightness = ctrl.slider(\"Brightness\", -1.0, 1.0, default=0.0)\n    contrast   = ctrl.slider(\"Contrast\",    0.0, 3.0, default=1.0)\n\n    changed = (xform_idx != state[\"last_transform\"]\n               or brightness != state[\"last_brightness\"]\n               or contrast   != state[\"last_contrast\"])\n\n    if changed:\n        result = apply_transform(original, xform_idx)\n        result = apply_brightness_contrast(result, brightness, contrast)\n        transformed[:] = result\n        state[\"last_transform\"]  = xform_idx\n        state[\"last_brightness\"] = brightness\n        state[\"last_contrast\"]   = contrast\n\n    ctrl.separator()\n\n    # \u6ee4\u6ce2\u5207\u6362\n    ctrl.text(\"Sampling Filter\")\n    filter_idx = ctrl.combo(\"##filter\", [\"Linear\", \"Nearest\"], default=0)\n    canvas_orig.filter  = \"nearest\" if filter_idx == 1 else \"linear\"\n    canvas_xform.filter = \"nearest\" if filter_idx == 1 else \"linear\"\n\n    ctrl.separator()\n\n    # \u4fdd\u5b58\n    ctrl.text(\"Save Output\")\n    save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n\n    if ctrl.button(\"Save Image\", width=140):\n        try:\n            canvas_xform.save(save_path)\n            state[\"save_msg\"] = f\"Saved to {save_path}\"\n        except Exception as e:\n            state[\"save_msg\"] = f\"Error: {e}\"\n\n    if \"save_msg\" in state:\n        ctrl.text_wrapped(state[\"save_msg\"])\n\n\nview.run()\n</code></pre>"},{"location":"zh/05_image_viewer/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/05_image_viewer/#imread","title":"imread \u2014 \u544a\u522b\u4f9d\u8d56\u5730\u72f1","text":"<pre><code>original = vultorch.imread(img_path, channels=3, device=device)\n</code></pre> <p>\u4e00\u884c\u641e\u5b9a\u3002\u8fd4\u56de <code>(H, W, 3)</code> \u7684 float32 CUDA tensor\uff0c\u503c\u57df <code>[0, 1]</code>\u3002 \u652f\u6301 PNG\u3001JPEG\u3001BMP\u3001TGA\u3001HDR\u3001PSD\u3001GIF\uff08\u7b2c\u4e00\u5e27\uff09\u3002 \u5e95\u5c42\u7528\u7684\u662f <code>stb_image</code> \u2014\u2014 \u4e0d\u9700\u8981\u4efb\u4f55 Python \u56fe\u50cf\u5e93\u3002</p> <p>\u53ef\u9009\u53c2\u6570\uff1a</p> <ul> <li><code>channels=4</code> \u2014\u2014 \u5f3a\u5236\u8f93\u51fa RGBA\u3002</li> <li><code>size=(256, 256)</code> \u2014\u2014 \u52a0\u8f7d\u540e\u7f29\u653e\uff08\u53cc\u7ebf\u6027\u63d2\u503c\uff09\u3002</li> <li><code>device=\"cpu\"</code> \u2014\u2014 \u5982\u679c\u4f60\u60f3\u7559\u5728 CPU \u4e0a\u3002</li> <li><code>shared=True</code> \u2014\u2014 \u7528 <code>create_tensor</code> \u5206\u914d\uff0c\u96f6\u62f7\u8d1d\u663e\u793a\u3002</li> </ul>"},{"location":"zh/05_image_viewer/#combo","title":"combo \u2014 \u4e0b\u62c9\u83dc\u5355","text":"<pre><code>xform_idx = ctrl.combo(\"##transform\", TRANSFORMS, default=0)\n</code></pre> <p>\u663e\u793a\u4e00\u4e2a\u4e0b\u62c9\u83dc\u5355\uff0c\u91cc\u9762\u662f\u5217\u8868\u4e2d\u7684\u9009\u9879\u3002\u8fd4\u56de\u9009\u4e2d\u9879\u7684\u7d22\u5f15\uff08int\uff09\u3002 \u72b6\u6001\u7531\u9762\u677f\u81ea\u52a8\u7ba1\u7406 \u2014\u2014 \u53ea\u9700\u8981\u4f20 <code>default=</code> \u8bbe\u521d\u59cb\u503c\u5c31\u884c\u3002</p> <p><code>##</code> \u524d\u7f00\u4f1a\u9690\u85cf ImGui \u7684\u6807\u7b7e\u6587\u5b57\uff08<code>##</code> \u540e\u9762\u7684\u5185\u5bb9\u53ea\u4f5c\u4e3a\u5185\u90e8 ID\uff09\u3002 \u5f53\u4f60\u4e0d\u60f3\u5728\u63a7\u4ef6\u65c1\u8fb9\u663e\u793a\u6587\u5b57\u7684\u65f6\u5019\u5f88\u6709\u7528\u3002</p>"},{"location":"zh/05_image_viewer/#input_text","title":"input_text \u2014 \u6587\u672c\u8f93\u5165","text":"<pre><code>save_path = ctrl.input_text(\"Path\", default=\"output.png\")\n</code></pre> <p>\u8fd4\u56de\u5f53\u524d\u5b57\u7b26\u4e32\u3002\u8f93\u4e2a\u6587\u4ef6\u540d\uff0c\u70b9\u4fdd\u5b58\u5c31\u884c\u3002 \u9ed8\u8ba4 <code>max_length=256</code> \u2014\u2014 \u5199\u4e2a\u8def\u5f84\u7ef0\u7ef0\u6709\u4f59\u3002</p>"},{"location":"zh/05_image_viewer/#canvassave","title":"Canvas.save() \u2014 \u4e00\u884c\u5bfc\u51fa","text":"<pre><code>canvas_xform.save(save_path)\n</code></pre> <p>\u628a\u5f53\u524d\u753b\u5e03\u7ed1\u5b9a\u7684 tensor \u4fdd\u5b58\u5230\u6587\u4ef6\u3002\u683c\u5f0f\u6839\u636e\u6269\u5c55\u540d\u81ea\u52a8\u5224\u65ad \uff08<code>.png</code>\u3001<code>.jpg</code>\u3001<code>.bmp</code>\u3001<code>.tga</code>\u3001<code>.hdr</code>\uff09\u3002 \u5e95\u5c42\u8c03\u7528\u7684\u662f <code>vultorch.imwrite()</code>\u3002</p>"},{"location":"zh/05_image_viewer/#filter-nearest-vs-linear","title":"filter \u2014 nearest vs linear","text":"<pre><code>canvas_orig.filter = \"nearest\"   # \u50cf\u7d20\u7ea7\u7cbe\u786e\uff0c\u653e\u5927\u540e\u6709\u952f\u9f7f\ncanvas_orig.filter = \"linear\"    # \u53cc\u7ebf\u6027\u63d2\u503c\uff0c\u5e73\u6ed1\n</code></pre> <p>\u53ef\u4ee5\u968f\u65f6\u5207\u6362\u91c7\u6837\u6ee4\u6ce2\u5668\u3002\u8bd5\u8bd5\u5728\u56fe\u7247\u88ab\u62c9\u4f38\u7684\u65f6\u5019\u5207\u6362 \u2014\u2014 <code>\"nearest\"</code> \u8ba9\u4f60\u770b\u5230\u539f\u59cb\u50cf\u7d20\uff0c<code>\"linear\"</code> \u628a\u5b83\u4eec\u7cca\u6210\u5e73\u6ed1\u6e10\u53d8\u3002 \u505a\u79d1\u7814\u53ef\u89c6\u5316\uff08\u5206\u5272 mask\u3001attention map\uff09\u7684\u65f6\u5019\uff0c\u57fa\u672c\u4e0a\u6c38\u8fdc\u9009 <code>\"nearest\"</code>\u3002</p>"},{"location":"zh/05_image_viewer/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p><code>imread</code> / <code>imwrite</code> \u2014 \u96f6\u4f9d\u8d56\u7684\u56fe\u7247\u8bfb\u5199\u3002\u76f4\u63a5\u8bfb\u5230 CUDA tensor\uff0c    \u76f4\u63a5\u4ece tensor \u5199\u6587\u4ef6\u3002\u4e0d\u9700\u8981 PIL\uff0c\u4e0d\u9700\u8981 numpy\uff0c    \u4e0d\u9700\u8981\u8ddf <code>cv2.cvtColor</code> \u6597\u667a\u6597\u52c7\u3002</p> </li> <li> <p><code>combo</code> \u2014 \u4e0b\u62c9\u9009\u62e9\u63a7\u4ef6\u3002\u8fd4\u56de int \u7d22\u5f15\u3002\u9002\u5408\u6a21\u5f0f\u5207\u6362\u3001    \u9884\u8bbe\u9009\u62e9\u3001\u679a\u4e3e\u7c7b\u578b\u7684\u9009\u9879\u3002</p> </li> <li> <p><code>input_text</code> \u2014 \u81ea\u7531\u6587\u672c\u8f93\u5165\u3002\u9002\u5408\u6587\u4ef6\u8def\u5f84\u3001\u6a21\u578b\u540d\u3001    \u5b9e\u9a8c\u6807\u7b7e\u8fd9\u7c7b\u9700\u8981\u6253\u5b57\u7684\u573a\u666f\u3002</p> </li> <li> <p><code>Canvas.save()</code> \u2014 \u4e00\u884c\u628a\u7ed1\u5b9a\u7684 tensor \u5b58\u4e3a\u56fe\u7247\u3002    \u6269\u5c55\u540d\u51b3\u5b9a\u683c\u5f0f\u3002</p> </li> <li> <p>\u6309\u9700\u91cd\u7b97 \u2014 \u53ea\u5728\u6ed1\u6761\u6216\u4e0b\u62c9\u503c\u771f\u6b63\u53d8\u5316\u65f6\u624d\u91cd\u65b0\u8ba1\u7b97\u53d8\u6362\u3002    \u901a\u8fc7\u68c0\u67e5 <code>changed</code> \u6765\u907f\u514d\u6bcf\u5e27\u90fd\u767d\u767d\u70e7 GPU\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p><code>imread</code> \u652f\u6301 <code>size=(H, W)</code> \u53c2\u6570\uff0c\u53ef\u4ee5\u5728\u52a0\u8f7d\u65f6\u7f29\u653e\u3002 \u4f60\u7684\u56fe\u7247\u662f 4K \u7684\uff0c\u4f46\u53ea\u9700\u8981 256\u00d7256 \u9884\u89c8\uff1f\u4e00\u4e2a\u53c2\u6570\u641e\u5b9a\u3002</p> <p>\u8bf4\u660e</p> <p><code>imwrite</code> \u63a5\u53d7 <code>[0, 1]</code> \u7684 float32 tensor\uff0c\u4e5f\u63a5\u53d7 <code>[0, 255]</code> \u7684 uint8 tensor\u3002 \u683c\u5f0f\u8f6c\u6362\u5b83\u81ea\u5df1\u5904\u7406\u3002</p>"},{"location":"zh/06_pixel_canvas/","title":"06 \u2014 \u50cf\u7d20\u753b\u677f","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/06_pixel_canvas.py</code></p> <p>\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u663e\u793a\u7684\u6bcf\u4e2a tensor \u4ece\u89c2\u4f17\u7684\u89d2\u5ea6\u6765\u770b\u90fd\u662f\"\u53ea\u8bfb\"\u7684 \u2014\u2014 GPU \u7b97\u5b8c, Vultorch \u663e\u793a\uff0c\u5b8c\u4e8b\u3002 \u4f46\u5982\u679c\u7528\u6237\u60f3\u7528\u9f20\u6807\u753b\u8fdb tensor \u5462\uff1f\u5b9e\u65f6\u5730\uff1f</p> <p>\u8fd9\u5c31\u662f\u672c\u7ae0\u7684\u5185\u5bb9\uff1a\u628a\u4e00\u4e2a\u96f6\u62f7\u8d1d GPU tensor \u53d8\u6210\u4ea4\u4e92\u5f0f\u753b\u677f\u3002 \u5de6\u952e\u753b\uff0c\u53f3\u952e\u64e6\uff0c\u9009\u989c\u8272\uff0c\u8c03\u7b14\u5237\u5927\u5c0f \u2014\u2014 \u641e\u5b9a\u3002</p> <p>\u9b54\u6cd5\u5728\u4e8e\u6839\u672c\u6ca1\u6709\u9b54\u6cd5\u3002 <code>create_tensor</code> \u7ed9\u4f60\u7684\u5c31\u662f\u4e00\u4e2a\u666e\u901a\u7684 <code>torch.Tensor</code>\uff0c\u5728 CUDA \u4e0a\u3002 \u4f60\u7528\u6807\u51c6\u7d22\u5f15\u5f80\u91cc\u5199\u50cf\u7d20\uff08<code>tensor[y, x, :3] = color</code>\uff09\u3002 \u56e0\u4e3a\u662f\u96f6\u62f7\u8d1d\uff0c\u663e\u793a\u81ea\u52a8\u66f4\u65b0\uff0c\u4e0d\u9700\u8981\u8c03\u4efb\u4f55\u4e0a\u4f20\u51fd\u6570\u3002</p>"},{"location":"zh/06_pixel_canvas/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u4e3a\u4ec0\u4e48\u91cd\u8981 <code>ui.get_mouse_pos()</code> \u8fd4\u56de\u9f20\u6807\u5149\u6807\u7684\u5c4f\u5e55\u50cf\u7d20\u5750\u6807 <code>(x, y)</code> \u4f60\u9700\u8981\u77e5\u9053\u7528\u6237\u6307\u5411\u753b\u5e03\u7684\u54ea\u4e2a\u4f4d\u7f6e <code>ui.is_item_hovered()</code> \u9f20\u6807\u5728\u6700\u540e\u4e00\u4e2a\u7ed8\u5236\u7684\u63a7\u4ef6\uff08\u753b\u5e03\u56fe\u7247\uff09\u4e0a\u65b9\u65f6\u8fd4\u56de <code>True</code> \u53ea\u5728\u5149\u6807\u5728\u753b\u5e03\u4e0a\u65f6\u624d\u753b\uff0c\u4e0d\u8981\u753b\u5230\u63a7\u5236\u9762\u677f\u4e0a <code>ui.is_mouse_clicked(0)</code> \u5de6\u952e\u6309\u4e0b\u7684\u90a3\u4e00\u5e27\u8fd4\u56de <code>True</code> \u68c0\u6d4b\u5355\u6b21\u70b9\u51fb <code>ui.is_mouse_dragging(0)</code> \u5de6\u952e\u6309\u4f4f\u4e14\u9f20\u6807\u79fb\u52a8\u671f\u95f4\u6301\u7eed\u8fd4\u56de <code>True</code> \u6301\u7eed\u7ed8\u753b \u2014\u2014 \u62d6\u62fd\u65f6\u6bcf\u5e27\u90fd\u89e6\u53d1 \u5c4f\u5e55\u2192\u50cf\u7d20\u6620\u5c04 \u628a\u5c4f\u5e55\u5750\u6807\u8f6c\u6362\u4e3a tensor \u7684 <code>[y, x]</code> \u7d22\u5f15 \u753b\u5e03\u56fe\u7247\u88ab\u62c9\u4f38\u5230\u9762\u677f\u5927\u5c0f\uff1b\u4f60\u9700\u8981\u53cd\u5411\u8ba1\u7b97\u624d\u80fd\u77e5\u9053\u9f20\u6807\u5728\u54ea\u4e2a\u50cf\u7d20\u4e0a"},{"location":"zh/06_pixel_canvas/#_2","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a 128\u00d7128 \u7684\u50cf\u7d20\u753b\u677f\uff1a\u5de6\u952e\u753b\uff0c\u53f3\u952e\u64e6\uff0c\u4fa7\u8fb9\u680f\u63a7\u5236\u7b14\u5237\u5927\u5c0f\u3001\u989c\u8272\u3001 \u6e05\u9664\u6309\u94ae\u548c\u7f51\u683c\u663e\u793a\u5f00\u5173\u3002</p>"},{"location":"zh/06_pixel_canvas/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import torch\nimport vultorch\nfrom vultorch import ui\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nH, W = 128, 128\n\nview = vultorch.View(\"06 - Pixel Canvas\", 900, 700)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.24)\ndraw_panel = view.panel(\"Canvas\")\n\n# \u96f6\u62f7\u8d1d RGBA tensor\ncanvas_tensor = vultorch.create_tensor(H, W, channels=4, device=device,\n                                        name=\"canvas\", window=view.window)\ncanvas_tensor[:, :, :3] = 0.1\ncanvas_tensor[:, :, 3] = 1.0\ncanvas = draw_panel.canvas(\"canvas\", filter=\"nearest\", fit=True)\ncanvas.bind(canvas_tensor)\n\n# \u6301\u4e45\u5316\u7684\u5e95\u56fe\uff08\u9632\u6b62\u7f51\u683c\u53e0\u52a0\u6548\u679c\u7d2f\u79ef\uff09\nbacking = torch.zeros(H, W, 3, device=device)\nbacking[:] = 0.1\n\nstate = {\n    \"brush_size\": 1,\n    \"brush_color\": (1.0, 0.3, 0.1),\n    \"show_grid\": False,\n    \"bg_color\": (0.1, 0.1, 0.1),\n}\n\n\ndef draw_brush(cy, cx, size, r, g, b):\n    half = size // 2\n    y0, y1 = max(0, cy - half), min(H, cy + half + 1)\n    x0, x1 = max(0, cx - half), min(W, cx + half + 1)\n    backing[y0:y1, x0:x1, 0] = r\n    backing[y0:y1, x0:x1, 1] = g\n    backing[y0:y1, x0:x1, 2] = b\n\n\ndef clear_canvas():\n    r, g, b = state[\"bg_color\"]\n    backing[:, :, 0] = r\n    backing[:, :, 1] = g\n    backing[:, :, 2] = b\n\n\ndef refresh_display():\n    canvas_tensor[:, :, :3] = backing\n    if state[\"show_grid\"]:\n        for i in range(0, H, 8):\n            canvas_tensor[i, :, :3] = canvas_tensor[i, :, :3].clamp(0, 0.85) + 0.15\n        for j in range(0, W, 8):\n            canvas_tensor[:, j, :3] = canvas_tensor[:, j, :3].clamp(0, 0.85) + 0.15\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"Canvas: {W}\u00d7{H}\")\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n\n    state[\"brush_size\"] = ctrl.slider_int(\"Brush Size\", 1, 16, default=1)\n    state[\"brush_color\"] = ctrl.color_picker(\"Brush Color\",\n                                              default=(1.0, 0.3, 0.1))\n    state[\"bg_color\"] = ctrl.color_picker(\"Background\",\n                                           default=(0.1, 0.1, 0.1))\n    ctrl.separator()\n\n    if ctrl.button(\"Clear\", width=120):\n        clear_canvas()\n\n    state[\"show_grid\"] = ctrl.checkbox(\"Show Grid (8px)\", default=False)\n\n    ctrl.separator()\n    ctrl.text_wrapped(\n        \"\u5de6\u952e\u5728\u753b\u5e03\u4e0a\u753b\u753b\uff0c\u53f3\u952e\u64e6\u9664\uff08\u6d82\u80cc\u666f\u8272\uff09\u3002\"\n        \"\u4e0a\u65b9\u53ef\u4ee5\u8c03\u6574\u7b14\u5237\u5927\u5c0f\u548c\u989c\u8272\u3002\"\n    )\n\n\n@draw_panel.on_frame\ndef handle_drawing():\n    if not ui.is_item_hovered():\n        refresh_display()\n        return\n\n    mx, my = ui.get_mouse_pos()\n\n    # \u5c4f\u5e55\u5750\u6807 \u2192 tensor \u50cf\u7d20\u5750\u6807\n    wp_x, wp_y = ui.get_window_pos()\n    win_w, win_h = ui.get_window_size()\n    content_x = wp_x + 8\n    content_y = wp_y + 26\n    content_w = win_w - 16\n    content_h = win_h - 34\n\n    u = max(0.0, min(1.0, (mx - content_x) / max(content_w, 1)))\n    v = max(0.0, min(1.0, (my - content_y) / max(content_h, 1)))\n    px = int(u * (W - 1))\n    py = int(v * (H - 1))\n\n    painting = ui.is_mouse_clicked(0) or ui.is_mouse_dragging(0, 0.0)\n    erasing  = ui.is_mouse_clicked(1) or ui.is_mouse_dragging(1, 0.0)\n\n    if painting:\n        r, g, b = state[\"brush_color\"]\n        draw_brush(py, px, state[\"brush_size\"], r, g, b)\n    elif erasing:\n        r, g, b = state[\"bg_color\"]\n        draw_brush(py, px, state[\"brush_size\"], r, g, b)\n\n    refresh_display()\n\n\nview.run()\n</code></pre>"},{"location":"zh/06_pixel_canvas/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/06_pixel_canvas/#tensor","title":"\u5c4f\u5e55\u5750\u6807 \u2192 tensor \u50cf\u7d20","text":"<p>\u8fd9\u662f\u672c\u4f8b\u7684\u6838\u5fc3\u6280\u5de7\u3002\u753b\u5e03\u56fe\u7247\u88ab\u62c9\u4f38\u5230\u6574\u4e2a\u9762\u677f\uff0c\u6240\u4ee5\u4e00\u4e2a 128\u00d7128 \u7684 tensor \u53ef\u80fd\u663e\u793a\u4e3a 600\u00d7500 \u7684\u5c4f\u5e55\u50cf\u7d20\u3002\u5f53\u9f20\u6807\u5728\u5c4f\u5e55\u4f4d\u7f6e <code>(mx, my)</code> \u65f6\uff0c \u4f60\u9700\u8981\u7b97\u51fa\u5b83\u5bf9\u5e94 tensor \u7684\u54ea\u4e2a\u50cf\u7d20\uff1a</p> <pre><code># \u9762\u677f\u7a97\u53e3\u4f4d\u7f6e\u548c\u5927\u5c0f\nwp_x, wp_y = ui.get_window_pos()\nwin_w, win_h = ui.get_window_size()\n\n# \u5185\u5bb9\u533a\u57df\uff08\u51cf\u53bb\u6807\u9898\u680f\u548c\u5185\u8fb9\u8ddd\uff09\ncontent_x = wp_x + 8\ncontent_y = wp_y + 26\n\n# \u5f52\u4e00\u5316\u5230 [0, 1]\nu = (mx - content_x) / content_w\nv = (my - content_y) / content_h\n\n# \u7f29\u653e\u5230\u50cf\u7d20\u7d22\u5f15\npx = int(u * (W - 1))\npy = int(v * (H - 1))\n</code></pre> <p>\u8fd9\u548c\u5149\u6805\u5316\u5668\u91cc\u628a\u5f52\u4e00\u5316\u8bbe\u5907\u5750\u6807\u8f6c\u6362\u4e3a\u50cf\u7d20\u5750\u6807\u662f\u5b8c\u5168\u76f8\u540c\u7684\u601d\u8def \u2014\u2014 \u53ea\u662f\u65b9\u5411\u53cd\u8fc7\u6765\u3002</p>"},{"location":"zh/06_pixel_canvas/#is_item_hovered-is_mouse_dragging","title":"is_item_hovered + is_mouse_dragging","text":"<p>ImGui \u4f1a\u8ffd\u8e2a\u9f20\u6807\u5728\u54ea\u4e2a\u63a7\u4ef6\u4e0a\u65b9\u3002\u5f53\u753b\u5e03\u56fe\u7247\u88ab\u7ed8\u5236\u540e\uff08\u9762\u677f\u81ea\u52a8\u5b8c\u6210\uff09\uff0c <code>is_item_hovered()</code> \u544a\u8bc9\u4f60\u5149\u6807\u662f\u5426\u5728\u56fe\u7247\u4e0a\u3002</p> <p><code>is_mouse_dragging(button, threshold)</code> \u5728\u6309\u94ae\u6309\u4f4f\u4e14\u9f20\u6807\u79fb\u52a8\u8d85\u8fc7 <code>threshold</code> \u50cf\u7d20\u65f6\u8fd4\u56de <code>True</code>\u3002\u8bbe <code>threshold=0.0</code> \u8868\u793a\u7acb\u5373\u89e6\u53d1 \u2014\u2014 \u76f8\u5f53\u4e8e\"\u6309\u94ae\u662f\u5426\u6b63\u5728\u88ab\u6309\u4f4f\"\u3002</p> <p>\u7ec4\u5408\u8d77\u6765\u5c31\u5b9e\u73b0\u4e86\u8fde\u7eed\u7ed8\u753b\uff1a</p> <pre><code>if ui.is_item_hovered():\n    if ui.is_mouse_dragging(0, 0.0):\n        draw_brush(py, px, ...)\n</code></pre>"},{"location":"zh/06_pixel_canvas/#_5","title":"\u5e95\u56fe + \u663e\u793a\u5237\u65b0","text":"<p>\u6211\u4eec\u4fdd\u6301\u4e00\u4e2a\u5355\u72ec\u7684 <code>backing</code> tensor\uff08RGB\uff0c\u6ca1\u6709 alpha\uff09\u6765\u5b58\u50a8\u5b9e\u9645\u7684\u50cf\u7d20\u753b\u3002 \u6bcf\u5e27\u628a\u5b83\u590d\u5236\u5230\u663e\u793a tensor\uff0c\u7136\u540e\u53ef\u9009\u5730\u53e0\u52a0\u7f51\u683c\u7ebf\u3002 \u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u7f51\u683c\u7ebf\u968f\u65f6\u95f4\"\u70e4\u8fdb\"\u753b\u4f5c\u91cc\u3002</p> <pre><code>canvas_tensor[:, :, :3] = backing          # \u590d\u5236\u753b\u4f5c\nif state[\"show_grid\"]:\n    canvas_tensor[::8, :, :3] += 0.15      # \u63d0\u4eae\u7f51\u683c\u884c\n    canvas_tensor[:, ::8, :3] += 0.15      # \u63d0\u4eae\u7f51\u683c\u5217\n</code></pre>"},{"location":"zh/06_pixel_canvas/#_6","title":"\u8981\u70b9","text":"<ol> <li> <p><code>create_tensor</code> \u662f\u53cc\u5411\u7684 \u2014\u2014 GPU \u53ef\u4ee5\u5f80\u91cc\u5199\uff08\u6a21\u62df\uff09\uff0c    Python \u4e5f\u53ef\u4ee5\u5f80\u91cc\u5199\uff08\u7528\u6237\u4ea4\u4e92\uff09\u3002Vultorch \u663e\u793a tensor \u91cc\u9762\u7684\u5185\u5bb9\uff0c\u4e0d\u7ba1\u662f\u8c01\u5199\u7684\u3002</p> </li> <li> <p>\u5c4f\u5e55\u2192tensor \u6620\u5c04 \u2014\u2014 <code>get_mouse_pos()</code> \u7ed9\u5c4f\u5e55\u50cf\u7d20\u5750\u6807\uff1b    \u51cf\u53bb\u9762\u677f\u539f\u70b9\u3001\u9664\u4ee5\u9762\u677f\u5927\u5c0f\u3001\u4e58\u4ee5 tensor \u5c3a\u5bf8\u3002    \u548c\u56fe\u5f62\u5b66\u91cc\u7684 UV \u5750\u6807\u662f\u4e00\u6837\u7684\u6570\u5b66\u3002</p> </li> <li> <p><code>is_item_hovered</code> + <code>is_mouse_dragging</code> \u2014\u2014 \u4ea4\u4e92\u5f0f\u63a7\u4ef6\u7684\u6807\u51c6\u6a21\u5f0f\u3002    \u5148\u68c0\u67e5\u60ac\u505c\uff0c\u518d\u68c0\u67e5\u6309\u94ae\u72b6\u6001\u3002</p> </li> <li> <p>\u5e95\u56fe\u6a21\u5f0f \u2014\u2014 \u5982\u679c\u8981\u5728\u7528\u6237\u6570\u636e\u4e0a\u53e0\u52a0\u88c5\u9970\uff08\u7f51\u683c\u3001\u6807\u8bb0\u3001\u5341\u5b57\u7ebf\uff09\uff0c    \u628a\u539f\u59cb\u6570\u636e\u5b58\u5728\u5355\u72ec\u7684 tensor \u91cc\uff0c\u6bcf\u5e27\u5408\u6210\u3002\u5426\u5219\u88c5\u9970\u4f1a\u4e0d\u65ad\u7d2f\u79ef\u3002</p> </li> <li> <p><code>filter=\"nearest\"</code> \u2014\u2014 \u50cf\u7d20\u753b\u7684\u5fc5\u9700\u54c1\u3002\u6ca1\u6709\u5b83 128\u00d7128 \u7684\u753b\u5e03    \u770b\u8d77\u6765\u50cf\u6a21\u7cca\u7684\u6c34\u5f69\u753b\uff0c\u800c\u4e0d\u662f\u6e05\u6670\u7684\u65b9\u5757\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u8fd9\u4e2a\u5c4f\u5e55\u2192\u50cf\u7d20\u6620\u5c04\u6280\u5de7\u53ef\u4ee5\u76f4\u63a5\u7528\u6765\u505a\u6807\u6ce8\u5de5\u5177 \u2014\u2014 \u5206\u5272 mask\u3001 \u5305\u56f4\u76d2\u3001\u5173\u952e\u70b9\u6807\u6ce8\u3002tensor \u672c\u8eab\u5c31\u662f\u6807\u7b7e\u56fe\u3002</p>"},{"location":"zh/07_multichannel/","title":"07 \u2014 \u591a\u901a\u9053\u67e5\u770b\u5668","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/07_multichannel.py</code></p> <p>\u5982\u679c\u4f60\u505a\u795e\u7ecf\u6e32\u67d3 \u2014\u2014 NeRF\u30013D Gaussian Splatting\u3001\u6216\u8005\u4e0b\u4e00\u4e2a\u7f29\u5199\u8bcd \u2014\u2014 \u4f60\u7684\u6a21\u578b\u4e0d\u53ea\u662f\u4ea7\u51fa\u4e00\u5f20\u6f02\u4eae\u7684\u56fe\u3002\u5b83\u540c\u65f6\u4ea7\u51fa RGB\u3001depth\u3001normal\u3001alpha\u3002 \u6bcf\u4e00\u4e2a\u50cf\u7d20\u90fd\u662f\u3002</p> <p>\u5f00\u53d1\u9636\u6bb5\u4f60\u9700\u8981\u540c\u65f6\u770b\u5230\u6240\u6709\u8fd9\u4e9b\u3002\u6807\u51c6\u6d41\u7a0b\uff1a\u5b58\u56db\u5f20 PNG\uff0c\u5728\u56db\u4e2a matplotlib \u7a97\u53e3\u91cc\u6253\u5f00\uff0c\u6b6a\u7740\u5934\u5de6\u53f3\u5bf9\u6bd4\uff0c\u53d1\u73b0\u6df1\u5ea6\u56fe\u4e0a\u4e0b\u98a0\u5012\uff0c\u91cd\u65b0\u5b58\u3001\u91cd\u65b0\u5f00\u3001 \u53cd\u590d\u76f4\u5230\u6000\u7591\u4eba\u751f\u3002</p> <p>\u672c\u7ae0\u7528\u56db\u4e2a\u96f6\u62f7\u8d1d\u9762\u677f\u66ff\u4ee3\u4e0a\u8ff0\u6d41\u7a0b\uff0c60 fps \u540c\u6b65\u5237\u65b0\u3002</p>"},{"location":"zh/07_multichannel/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u4e3a\u4ec0\u4e48\u91cd\u8981 \u56db\u6b21 <code>create_tensor</code> \u540c\u4e00\u4e2a\u7a97\u53e3\u91cc\u7528\u56db\u4e2a\u72ec\u7acb\u7684 GPU \u5171\u4eab\u7eb9\u7406 \u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u90fd\u6709\u81ea\u5df1\u7684\u5b9e\u65f6\u663e\u793a Turbo \u8272\u56fe \u628a <code>[0, 1]</code> \u6807\u91cf tensor \u6620\u5c04\u4e3a\u5f69\u8272 <code>(H, W, 3)</code> \u56fe\u50cf \u6df1\u5ea6\u7b49\u6807\u91cf\u5b57\u6bb5\u5728\u7070\u5ea6\u4e0b\u51e0\u4e4e\u4e0d\u53ef\u89c1\uff1bturbo \u8ba9\u7ed3\u6784\u4e00\u76ee\u4e86\u7136 \u6cd5\u7ebf\u2192RGB \u6620\u5c04 <code>normal * 0.5 + 0.5</code>\uff0c\u628a <code>[-1, 1]</code> \u6cd5\u7ebf\u53d8\u6210 <code>[0, 1]</code> \u989c\u8272 \u6807\u51c6\u60ef\u4f8b\uff1aX\u2192\u7ea2, Y\u2192\u7eff, Z\u2192\u84dd \u5149\u7ebf-\u7403\u4f53\u6c42\u4ea4 \u7528 ~30 \u884c PyTorch \u5b9e\u73b0\u7684\u5168 GPU \u7a0b\u5e8f\u5316\u6e32\u67d3 \u8bc1\u660e\u4efb\u4f55 GPU \u8ba1\u7b97\u90fd\u53ef\u4ee5\u76f4\u63a5\u63a5\u5165 Vultorch"},{"location":"zh/07_multichannel/#_2","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u7528\u7a0b\u5e8f\u5316\u7684\u5149\u7ebf-\u7403\u4f53\u6e32\u67d3\u5668\u4ea7\u51fa\u56db\u4e2a\u5b9e\u65f6\u8f93\u51fa\uff0c\u52a0\u4e00\u4e2a\u63a7\u5236\u4fa7\u8fb9\u680f\u3002 \u6240\u6709\u8ba1\u7b97 \u2014\u2014 \u5149\u7ebf\u3001\u6c42\u4ea4\u3001\u7740\u8272\u3001\u8272\u56fe \u2014\u2014 \u90fd\u8dd1\u5728 GPU \u4e0a\u3002 \u56db\u4e2a\u663e\u793a tensor \u5168\u90e8\u96f6\u62f7\u8d1d\uff0c\u4ec0\u4e48\u90fd\u4e0d\u7ecf\u8fc7 CPU\u3002</p>"},{"location":"zh/07_multichannel/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import math\n\nimport torch\nimport torch.nn.functional as F\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\nH, W = 256, 256\n\nview = vultorch.View(\"07 - Multi-Channel Viewer\", 512, 1024)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.20)\nrgb_panel = view.panel(\"RGB\")\ndepth_panel = view.panel(\"Depth\")\nnormal_panel = view.panel(\"Normal\")\nalpha_panel = view.panel(\"Alpha\")\n\n# \u56db\u4e2a\u96f6\u62f7\u8d1d\u663e\u793a tensor\nrgb_tensor = vultorch.create_tensor(H, W, 4, device, name=\"rgb\",\n                                     window=view.window)\ndepth_tensor = vultorch.create_tensor(H, W, 4, device, name=\"depth\",\n                                       window=view.window)\nnormal_tensor = vultorch.create_tensor(H, W, 4, device, name=\"normal\",\n                                        window=view.window)\nalpha_tensor = vultorch.create_tensor(H, W, 4, device, name=\"alpha\",\n                                       window=view.window)\n\nrgb_panel.canvas(\"rgb\").bind(rgb_tensor)\ndepth_panel.canvas(\"depth\").bind(depth_tensor)\nnormal_panel.canvas(\"normal\").bind(normal_tensor)\nalpha_panel.canvas(\"alpha\").bind(alpha_tensor)\n\n# --- Turbo \u8272\u56fe LUT\uff08256 \u4e2a\u6761\u76ee\uff0c\u53ea\u5efa\u4e00\u6b21\uff09---\n_turbo_data = [\n    (0.18995, 0.07176, 0.23217), (0.22500, 0.16354, 0.45096),\n    # ...\uff0832 \u4e2a\u5173\u952e\u8272\uff0c\u63d2\u503c\u5230 256 \u6761\uff09\n]\nTURBO_LUT = ...  # \u5b8c\u6574 LUT \u6784\u5efa\u89c1\u6e90\u6587\u4ef6\n\ndef apply_turbo(values):\n    \"\"\"\u628a [0,1] \u6d6e\u70b9 tensor (H,W) \u6620\u5c04\u4e3a (H,W,3) turbo \u8272\u5f69\u3002\"\"\"\n    idx = (values.clamp(0, 1) * 255).long()\n    return TURBO_LUT[idx]\n\n# \u9884\u8ba1\u7b97\u5149\u7ebf\u65b9\u5411\nys = torch.linspace(1, -1, H, device=device)\nxs = torch.linspace(-1, 1, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n\nstate = {\"sphere_r\": 0.6, \"light_az\": 0.5, \"light_el\": 0.8,\n         \"ambient\": 0.1, \"bg_r\": 0.12, \"bg_g\": 0.12, \"bg_b\": 0.14}\n\n\ndef render_sphere():\n    r = state[\"sphere_r\"]\n    ray_o = torch.tensor([0.0, 0.0, -2.0], device=device)\n    ray_d = torch.stack([xx, yy, torch.ones_like(xx)], dim=-1)\n    ray_d = ray_d / ray_d.norm(dim=-1, keepdim=True)\n\n    # \u5149\u7ebf-\u7403\u4f53\u6c42\u4ea4\u7684\u4e8c\u6b21\u65b9\u7a0b\n    b = 2.0 * (ray_d * ray_o).sum(-1)\n    c_val = (ray_o * ray_o).sum() - r * r\n    disc = b * b - 4.0 * c_val\n    hit = disc &gt; 0\n\n    t = (-b - torch.sqrt(disc.clamp(min=0))) / 2.0\n    t = t.clamp(min=0)\n    point = ray_o + t.unsqueeze(-1) * ray_d\n    normal = point / (point.norm(dim=-1, keepdim=True) + 1e-8)\n\n    # Lambert \u7740\u8272\n    az, el = state[\"light_az\"], state[\"light_el\"]\n    light_dir = torch.tensor([math.cos(el)*math.sin(az),\n                               math.sin(el),\n                               math.cos(el)*math.cos(az)], device=device)\n    light_dir = light_dir / light_dir.norm()\n    shade = state[\"ambient\"] + (1 - state[\"ambient\"]) * \\\n            (normal * light_dir).sum(-1).clamp(min=0)\n\n    bg = torch.tensor([state[\"bg_r\"], state[\"bg_g\"], state[\"bg_b\"]],\n                      device=device)\n\n    # RGB\n    rgb = torch.where(hit.unsqueeze(-1),\n                      shade.unsqueeze(-1) * torch.ones(1,1,3, device=device), bg)\n    rgb_tensor[:,:,:3] = rgb;  rgb_tensor[:,:,3] = 1.0\n\n    # Depth\uff08turbo \u8272\u56fe\uff09\n    depth_raw = t * hit.float()\n    d_min = depth_raw[hit].min() if hit.any() else torch.tensor(0.0)\n    d_max = depth_raw[hit].max() if hit.any() else torch.tensor(1.0)\n    depth_norm = ((depth_raw - d_min) / (d_max - d_min + 1e-8)).clamp(0,1)\n    depth_color = torch.where(hit.unsqueeze(-1), apply_turbo(depth_norm), bg)\n    depth_tensor[:,:,:3] = depth_color;  depth_tensor[:,:,3] = 1.0\n\n    # \u6cd5\u7ebf ([-1,1] \u2192 [0,1])\n    nc = torch.where(hit.unsqueeze(-1), normal * 0.5 + 0.5, bg)\n    normal_tensor[:,:,:3] = nc;  normal_tensor[:,:,3] = 1.0\n\n    # Alpha\n    a = hit.float()\n    alpha_tensor[:,:,0] = a; alpha_tensor[:,:,1] = a\n    alpha_tensor[:,:,2] = a; alpha_tensor[:,:,3] = 1.0\n\n\n@ctrl.on_frame\ndef draw_controls():\n    ctrl.text(f\"FPS: {view.fps:.1f}\")\n    ctrl.separator()\n    state[\"sphere_r\"] = ctrl.slider(\"Radius\", 0.1, 1.5, default=0.6)\n    ctrl.separator()\n    state[\"light_az\"] = ctrl.slider(\"Light Az\", -3.14, 3.14, default=0.5)\n    state[\"light_el\"] = ctrl.slider(\"Light El\", -1.5, 1.5, default=0.8)\n    state[\"ambient\"]  = ctrl.slider(\"Ambient\", 0.0, 1.0, default=0.1)\n    ctrl.separator()\n    bg = ctrl.color_picker(\"Background\", default=(0.12, 0.12, 0.14))\n    state[\"bg_r\"], state[\"bg_g\"], state[\"bg_b\"] = bg\n\n\n@view.on_frame\ndef update():\n    render_sphere()\n\n\nview.run()\n</code></pre> <p>\uff08\u4ee5\u4e0a\u4ee3\u7801\u6709\u5220\u8282 \u2014\u2014 \u5b8c\u6574\u7684 turbo \u8272\u56fe LUT \u89c1 <code>examples/07_multichannel.py</code>\u3002\uff09</p>"},{"location":"zh/07_multichannel/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/07_multichannel/#tensor","title":"\u56db\u4e2a\u9762\u677f\uff0c\u56db\u4e2a tensor\uff0c\u4e00\u4e2a\u7a97\u53e3","text":"<pre><code>rgb_tensor    = vultorch.create_tensor(H, W, 4, device, name=\"rgb\", ...)\ndepth_tensor  = vultorch.create_tensor(H, W, 4, device, name=\"depth\", ...)\nnormal_tensor = vultorch.create_tensor(H, W, 4, device, name=\"normal\", ...)\nalpha_tensor  = vultorch.create_tensor(H, W, 4, device, name=\"alpha\", ...)\n</code></pre> <p>\u6bcf\u6b21\u8c03\u7528\u5206\u914d\u4e00\u4e2a\u72ec\u7acb\u7684 Vulkan \u5171\u4eab tensor\u3002\u6bcf\u4e2a\u9762\u677f\u7ed1\u5b9a\u5176\u4e2d\u4e00\u4e2a\u3002 \u56db\u4e2a tensor \u6bcf\u5e27\u66f4\u65b0\uff0c\u4e0d\u7ecf\u8fc7 CPU \u2014\u2014 \u6570\u636e\u8def\u5f84\u662f CUDA \u2192 Vulkan \u2192 \u5c4f\u5e55\u3002</p> <p>\u8fd9\u5c31\u662f\u795e\u7ecf\u6e32\u67d3\u7684\u5de5\u4f5c\u6d41\uff1a\u4f60\u7684\u6a21\u578b\u505a\u4e00\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u586b\u56db\u4e2a tensor\uff0c \u67e5\u770b\u5668\u540c\u65f6\u628a\u5b83\u4eec\u5168\u90e8\u663e\u793a\u51fa\u6765\u3002</p>"},{"location":"zh/07_multichannel/#turbo","title":"Turbo \u8272\u56fe \u2014\u2014 \u8ba9\u6df1\u5ea6\u770b\u5f97\u89c1","text":"<p>\u539f\u59cb\u6df1\u5ea6\u503c\u662f\u4e2a\u6d6e\u70b9\u6570\uff0c\u503c\u57df\u4e0d\u786e\u5b9a\u3002\u76f4\u63a5\u663e\u793a\u7684\u8bdd\u4f60\u53ea\u80fd\u770b\u5230\u4e00\u5f20\u51e0\u4e4e\u5168\u9ed1\u7684\u56fe\uff0c \u6e10\u53d8\u5b8c\u5168\u4e0d\u53ef\u89c1\u3002Turbo \u8272\u56fe\u628a <code>[0, 1]</code> \u6807\u91cf\u6620\u5c04\u5230\u611f\u77e5\u5747\u5300\u7684\u5f69\u8679\u8272\uff0c \u8ba9\u4f60\u771f\u6b63\u770b\u5230\u6df1\u5ea6\u7ed3\u6784\uff1a</p> <pre><code>def apply_turbo(values):\n    idx = (values.clamp(0, 1) * 255).long()   # \u91cf\u5316\u5230 256 \u4e2a bin\n    return TURBO_LUT[idx]                       # \u67e5\u8868\uff0c\u8fd4\u56de (H, W, 3)\n</code></pre> <p>\u5168\u90e8\u5728 GPU \u4e0a\u8dd1 \u2014\u2014 \u4e0d\u9700\u8981 numpy\uff0c\u4e0d\u9700\u8981 matplotlib\u3002</p>"},{"location":"zh/07_multichannel/#rgb","title":"\u6cd5\u7ebf\u2192RGB \u60ef\u4f8b","text":"<p>\u53ef\u89c6\u5316\u8868\u9762\u6cd5\u7ebf\u7684\u6807\u51c6\u65b9\u6cd5\uff1a\u628a\u6bcf\u4e2a\u5206\u91cf\u4ece <code>[-1, 1]</code> \u6620\u5c04\u5230 <code>[0, 1]</code>\uff0c \u5206\u522b\u8d4b\u7ed9\u4e00\u4e2a\u989c\u8272\u901a\u9053\uff1a</p> <pre><code>normal_color = normal * 0.5 + 0.5   # X\u2192R, Y\u2192G, Z\u2192B\n</code></pre> <p>\u8868\u9762\u671d\u53f3\u662f\u7ea2\u8272\uff0c\u671d\u4e0a\u662f\u7eff\u8272\uff0c\u671d\u6444\u50cf\u673a\u662f\u84dd\u8272\u3002 \u6bcf\u7bc7\u795e\u7ecf\u6e32\u67d3\u8bba\u6587\u90fd\u7528\u8fd9\u4e2a\u60ef\u4f8b\uff0c\u6240\u4ee5\u4f60\u4e00\u773c\u5c31\u80fd\u8ba4\u51fa\u6765\u3002</p>"},{"location":"zh/07_multichannel/#-","title":"\u5149\u7ebf-\u7403\u4f53\u6c42\u4ea4","text":"<p>\u6574\u4e2a\u6e32\u67d3\u5668\u53ea\u6709 ~30 \u884c PyTorch \u4ee3\u7801\u3002\u6838\u5fc3\u662f\u6c42\u4ea4\u7684\u4e8c\u6b21\u516c\u5f0f\uff1a</p> <p>$$t = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}$$</p> <p>\u5176\u4e2d $a = |d|^2$\uff0c$b = 2 \\langle o, d \\rangle$\uff0c$c = |o|^2 - r^2$\u3002 \u5bf9\u6240\u6709 $H \\times W$ \u6761\u5149\u7ebf\u5e76\u884c\u8ba1\u7b97\uff0c\u4e00\u6b21 GPU kernel \u8c03\u7528\u5b8c\u6210\u3002 \u628a\u8fd9\u90e8\u5206\u66ff\u6362\u6210\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u524d\u5411\u4f20\u64ad\uff0c\u4f60\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a NeRF \u67e5\u770b\u5668\u3002</p>"},{"location":"zh/07_multichannel/#_5","title":"\u8981\u70b9","text":"<ol> <li> <p>\u591a\u4e2a\u96f6\u62f7\u8d1d tensor \u2014\u2014 \u6bcf\u4e2a\u8f93\u51fa\u901a\u9053\u8c03\u4e00\u6b21 <code>create_tensor</code>\uff0c    \u5404\u81ea\u7ed1\u5b9a\u5230\u81ea\u5df1\u7684\u9762\u677f\u3002\u6240\u6709\u66f4\u65b0\u90fd\u5728 GPU \u4e0a\u5b8c\u6210\u3002</p> </li> <li> <p>Turbo \u8272\u56fe \u2014\u2014 \u7528 <code>(values * 255).long()</code> \u7d22\u5f15 GPU \u4e0a\u7684 LUT\u3002    \u5bf9\u6df1\u5ea6\u3001\u89c6\u5dee\u3001attention \u6743\u91cd\u3001loss \u70ed\u529b\u56fe\u7b49\u6807\u91cf\u573a\u81f3\u5173\u91cd\u8981 \u2014\u2014    \u7070\u5ea6\u4e0b\u4f1a\u770b\u4e0d\u89c1\u3002</p> </li> <li> <p>\u6cd5\u7ebf\u2192RGB \u2014\u2014 <code>n * 0.5 + 0.5</code>\u3002\u4e09\u4e2a\u5b57\u7b26\u7684\u4ee3\u7801\uff0c    \u56fe\u5f62\u5b66/\u89c6\u89c9\u793e\u533a\u901a\u7528\u7684\u60ef\u4f8b\u3002</p> </li> <li> <p>\u7a0b\u5e8f\u5316\u2192\u795e\u7ecf \u2014\u2014 \u672c\u4f8b\u7528\u5149\u7ebf-\u7403\u4f53\u6c42\u4ea4\u4f5c\u4e3a\u66ff\u8eab\u3002    \u628a <code>render_sphere()</code> \u6362\u6210\u4f60\u6a21\u578b\u7684 forward pass\uff0c    \u4f60\u5c31\u5f97\u5230\u4e86\u4e00\u4e2a\u5b9e\u65f6\u591a\u901a\u9053\u795e\u7ecf\u6e32\u67d3\u67e5\u770b\u5668\u3002</p> </li> <li> <p>\u96f6\u62f7\u8d1d\u7684\u53ef\u6269\u5c55\u6027 \u2014\u2014 \u56db\u4e2a 256\u00d7256\u00d74 \u7eb9\u7406\u6bcf\u5e27\u66f4\u65b0\u3002    \u74f6\u9888\u662f\u4f60\u7684\u8ba1\u7b97\uff0c\u4e0d\u662f\u663e\u793a\u7ba1\u7ebf\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u62d6\u52a8\u9762\u677f\u8fb9\u6846\u53ef\u4ee5\u91cd\u65b0\u6392\u5217\u56db\u4e2a\u89c6\u56fe \u2014\u2014 \u628a\u6df1\u5ea6\u653e\u5728 RGB \u65c1\u8fb9\u505a\u5bf9\u6bd4\uff0c \u6216\u8005\u628a\u6cd5\u7ebf\u53e0\u5728 alpha \u4e0a\u9762\u3002\u5e03\u5c40\u5b8c\u5168\u53ef\u4ee5\u5728\u8fd0\u884c\u65f6\u7531\u7528\u6237\u914d\u7f6e\u3002</p>"},{"location":"zh/08_gt_vs_pred/","title":"08 \u2014 GT vs \u9884\u6d4b","text":"<p>\u793a\u4f8b\u6587\u4ef6\uff1a <code>examples/08_gt_vs_pred.py</code></p> <p>\u6bcf\u4e2a\u641e\u795e\u7ecf\u6e32\u67d3\u7684\u4eba\u6bcf\u5929\u90fd\u5728\u505a\u540c\u4e00\u4ef6\u4e8b\uff1a\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\uff0c\u770b\u7ed3\u679c\uff0c\u548c GT \u5bf9\u6bd4\uff0c \u76ef\u7740\u5dee\u5f02\u56fe\uff0c\u7422\u78e8\u8bef\u5dee\u85cf\u5728\u54ea\u91cc\u3002</p> <p>\u901a\u5e38\u7684\u6d41\u7a0b\uff1a\u5b58\u4e00\u5f20 PNG\uff0c\u5728 matplotlib \u91cc\u548c GT \u5e76\u6392\u6253\u5f00\uff0c\u5728\u53e6\u4e00\u4e2a cell \u7b97 PSNR\uff0c \u5728 TensorBoard \u91cc\u753b loss\uff0c\u5728\u4e09\u4e2a\u7a97\u53e3\u95f4 alt-tab \u5207\u6765\u5207\u53bb\uff0cGPU \u5e72\u7b49\u7740\u3002 \u7b49\u4f60\u62fc\u597d\u5bf9\u6bd4\u56fe\u7684\u65f6\u5019\uff0c\u5df2\u7ecf\u5fd8\u4e86\u521a\u624d\u6539\u4e86\u4ec0\u4e48\u8d85\u53c2\u6570\u3002</p> <p>\u672c\u7ae0\u628a\u8fd9\u4e9b\u5168\u90e8\u653e\u5728\u4e00\u4e2a\u7a97\u53e3\u91cc\uff1aGT\u3001\u9884\u6d4b\u3001\u8bef\u5dee\u70ed\u529b\u56fe\u3001loss \u66f2\u7ebf\u3001PSNR \u2014\u2014 \u5168\u90e8\u5b9e\u65f6\uff0c\u5168\u90e8 60 fps\uff0c\u5168\u90e8\u96f6\u62f7\u8d1d\u3002</p>"},{"location":"zh/08_gt_vs_pred/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5e72\u4ec0\u4e48\u7528 \u4e3a\u4ec0\u4e48\u91cd\u8981 \u8bef\u5dee\u70ed\u529b\u56fe <code>|GT - pred|</code> \u653e\u5927\u540e\u7528 turbo \u8272\u56fe\u7740\u8272 \u770b\u5230\u88f8\u773c\u5bf9\u6bd4\u6839\u672c\u770b\u4e0d\u89c1\u7684\u8bef\u5dee <code>panel.plot()</code> \u7528 Python \u5217\u8868\u753b\u6298\u7ebf\u56fe \u5b9e\u65f6 loss \u548c PSNR \u66f2\u7ebf\uff0c\u4e0d\u9700\u8981 TensorBoard <code>panel.progress()</code> \u4e00\u4e2a\u8fdb\u5ea6\u6761 \u5feb\u901f\u770b\u8bad\u7ec3\u8fdb\u5ea6 PSNR $-10 \\log_{10}(\\text{MSE})$\uff0c\u4ece\u5b9e\u65f6 loss \u8ba1\u7b97 \u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\u7684\u6807\u51c6\u6307\u6807 \u8bef\u5dee\u6a21\u5f0f combo \u5728 L1\u3001L2\u3001\u9010\u901a\u9053\u6700\u5927\u503c\u4e4b\u95f4\u5207\u6362 \u4e0d\u540c\u7684\u8bef\u5dee\u8303\u6570\u66b4\u9732\u4e0d\u540c\u7684\u95ee\u9898 \u8bef\u5dee\u589e\u76ca slider \u653e\u5927\u5fae\u5c0f\u8bef\u5dee\u4f7f\u5176\u53ef\u89c1 \u4f4e\u8bef\u5dee\u533a\u57df\u4e58\u4ee5 5\u201320\u00d7 \u540e\u624d\u770b\u5f97\u89c1"},{"location":"zh/08_gt_vs_pred/#_2","title":"\u8fd9\u6b21\u73a9\u4ec0\u4e48","text":"<p>\u4e00\u4e2a\u5750\u6807 MLP \u62df\u5408\u76ee\u6807\u56fe\u50cf\uff08\u548c\u4f8b\u5b50 03 \u4e00\u6837\uff0c\u4f46\u5347\u7ea7\u7248\uff09\u3002 \u4e09\u4e2a\u56fe\u50cf\u9762\u677f \u2014\u2014 GT\u3001\u9884\u6d4b\u3001\u8bef\u5dee\u70ed\u529b\u56fe \u2014\u2014 \u52a0\u4e0a\u4e00\u4e2a\u6307\u6807\u4fa7\u8fb9\u680f\uff0c \u5185\u542b\u5b9e\u65f6 loss/PSNR \u66f2\u7ebf\u548c\u63a7\u5236\u5668\u3002</p>"},{"location":"zh/08_gt_vs_pred/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\nif not torch.cuda.is_available():\n    raise RuntimeError(\"This example needs CUDA\")\n\ndevice = \"cuda\"\n\n# \u52a0\u8f7d\u76ee\u6807\u56fe\u50cf\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\nH, W = gt.shape[0], gt.shape[1]\n\n# MLP \u7684\u5750\u6807\u7f51\u683c\nys = torch.linspace(-1.0, 1.0, H, device=device)\nxs = torch.linspace(-1.0, 1.0, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\ncoords = torch.stack([xx, yy], dim=-1).reshape(-1, 2)\ntarget = gt.reshape(-1, 3)\n\n# \u7b80\u5355\u7684\u5750\u6807 MLP\nclass CoordMLP(nn.Module):\n    def __init__(self, hidden=64, layers=3):\n        super().__init__()\n        net = [nn.Linear(2, hidden), nn.ReLU(inplace=True)]\n        for _ in range(layers - 1):\n            net += [nn.Linear(hidden, hidden), nn.ReLU(inplace=True)]\n        net += [nn.Linear(hidden, 3), nn.Sigmoid()]\n        self.net = nn.Sequential(*net)\n    def forward(self, x):\n        return self.net(x)\n\nmodel = CoordMLP().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n\n# Turbo \u8272\u56fe LUT\uff08\u8be6\u89c1\u4f8b\u5b50 07\uff09\nTURBO_LUT = ...  # 256\u00d73, GPU \u4e0a\n\ndef apply_turbo(values):\n    idx = (values.clamp(0, 1) * 255).long()\n    return TURBO_LUT[idx]\n\n# \u89c6\u56fe + \u9762\u677f\nview = vultorch.View(\"08 - GT vs Prediction\", 1280, 1000)\nmetrics_panel = view.panel(\"Metrics\", side=\"right\", width=0.28)\ngt_panel = view.panel(\"Ground Truth\")\npred_panel = view.panel(\"Prediction\")\nerror_panel = view.panel(\"Error Map\")\n\ngt_panel.canvas(\"gt\").bind(gt)\n\npred_rgba = vultorch.create_tensor(H, W, 4, device, name=\"pred\",\n                                    window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\nerror_rgba = vultorch.create_tensor(H, W, 4, device, name=\"error\",\n                                     window=view.window)\nerror_rgba[:, :, 3] = 1.0\nerror_panel.canvas(\"error\").bind(error_rgba)\n\nERROR_MODES = [\"L1\", \"L2\", \"Per-Channel Max\"]\nstate = {\"iter\": 0, \"loss\": 1.0, \"psnr\": 0.0, \"steps_per_frame\": 6,\n         \"error_mode\": 0, \"error_gain\": 5.0,\n         \"loss_history\": [], \"psnr_history\": []}\n\n\ndef compute_error_map(gt_img, pred_img, mode, gain):\n    if mode == 0:    err = (gt_img - pred_img).abs().mean(dim=-1)\n    elif mode == 1:  err = ((gt_img - pred_img)**2).mean(dim=-1).sqrt()\n    else:            err = (gt_img - pred_img).abs().max(dim=-1).values\n    return apply_turbo((err * gain).clamp(0, 1))\n\n\ndef compute_psnr(mse):\n    return -10.0 * math.log10(mse) if mse &gt; 0 else 50.0\n\n\n@view.on_frame\ndef train():\n    for _ in range(state[\"steps_per_frame\"]):\n        optimizer.zero_grad(set_to_none=True)\n        out = model(coords)\n        loss = F.mse_loss(out, target)\n        loss.backward()\n        optimizer.step()\n        state[\"iter\"] += 1\n        state[\"loss\"] = loss.item()\n\n    with torch.no_grad():\n        pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n        pred_rgba[:, :, :3] = pred\n\n        error_color = compute_error_map(gt, pred, state[\"error_mode\"],\n                                         state[\"error_gain\"])\n        error_rgba[:, :, :3] = error_color\n        state[\"psnr\"] = compute_psnr(state[\"loss\"])\n\n    state[\"loss_history\"].append(state[\"loss\"])\n    state[\"psnr_history\"].append(state[\"psnr\"])\n    if len(state[\"loss_history\"]) &gt; 300:\n        state[\"loss_history\"] = state[\"loss_history\"][-300:]\n        state[\"psnr_history\"] = state[\"psnr_history\"][-300:]\n\n\n@metrics_panel.on_frame\ndef draw_metrics():\n    metrics_panel.text(f\"FPS: {view.fps:.1f}\")\n    metrics_panel.text(f\"Iteration: {state['iter']}\")\n    metrics_panel.separator()\n    metrics_panel.text(f\"MSE Loss: {state['loss']:.6f}\")\n    metrics_panel.text(f\"PSNR: {state['psnr']:.2f} dB\")\n    metrics_panel.separator()\n\n    metrics_panel.text(\"Loss \u66f2\u7ebf\")\n    if state[\"loss_history\"]:\n        metrics_panel.plot(state[\"loss_history\"], label=\"##loss\",\n                           overlay=f\"{state['loss']:.5f}\", height=80)\n\n    metrics_panel.text(\"PSNR \u66f2\u7ebf\")\n    if state[\"psnr_history\"]:\n        metrics_panel.plot(state[\"psnr_history\"], label=\"##psnr\",\n                           overlay=f\"{state['psnr']:.1f} dB\", height=80)\n\n    metrics_panel.separator()\n    state[\"steps_per_frame\"] = metrics_panel.slider_int(\n        \"Steps/Frame\", 1, 32, default=6)\n    state[\"error_mode\"] = metrics_panel.combo(\"Error Mode\", ERROR_MODES)\n    state[\"error_gain\"] = metrics_panel.slider(\"Error Gain\", 1.0, 20.0,\n                                                default=5.0)\n\n    progress = min(1.0, state[\"iter\"] / 5000.0)\n    metrics_panel.progress(progress, overlay=f\"{progress*100:.0f}%\")\n\n\nview.run()\n</code></pre> <p>\uff08\u6709\u5220\u8282 \u2014\u2014 \u5b8c\u6574\u4ee3\u7801\u542b turbo LUT \u89c1 <code>examples/08_gt_vs_pred.py</code>\u3002\uff09</p>"},{"location":"zh/08_gt_vs_pred/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/08_gt_vs_pred/#_5","title":"\u8bef\u5dee\u70ed\u529b\u56fe \u2014\u2014 \u770b\u89c1\u770b\u4e0d\u89c1\u7684\u4e1c\u897f","text":"<p>\u539f\u59cb\u7684 <code>|GT - pred|</code> \u5dee\u503c\u901a\u5e38\u662f\u63a5\u8fd1\u96f6\u7684\u5c0f\u6d6e\u70b9\u6570\u3002\u76f4\u63a5\u663e\u793a\u7684\u8bdd\u4f60\u53ea\u80fd\u770b\u5230 \u4e00\u5f20\u51e0\u4e4e\u5168\u9ed1\u7684\u56fe\uff0c\u7136\u540e\u5f97\u51fa\"\u4e00\u5207\u6b63\u5e38\"\u7684\u7ed3\u8bba\u3002\u8fd9\u662f\u9519\u8bef\u7684\u3002</p> <pre><code>err = (gt_img - pred_img).abs().mean(dim=-1)   # \u6bcf\u50cf\u7d20 L1 \u8bef\u5dee\nerr = (err * gain).clamp(0, 1)                  # \u653e\u5927 5\u201320\u00d7\nheatmap = apply_turbo(err)                       # turbo \u8272\u56fe\n</code></pre> <p>\u589e\u76ca\u6ed1\u6761\u8ba9\u4f60\u628a\u5fae\u5c0f\u8bef\u5dee\u653e\u5927\u5230\u53ef\u89c1\u30025\u00d7 \u589e\u76ca\u4e0b\u4f60\u5c31\u80fd\u770b\u5230\u7f51\u7edc\u5728\u54ea\u91cc\u5403\u529b \u2014\u2014 \u8fb9\u7f18\u3001\u7cbe\u7ec6\u7eb9\u7406\u3001\u9ad8\u9891\u533a\u57df\u3002\u8fd9\u79cd\u6d1e\u5bdf\u529b\u662f\u5355\u4e2a PSNR \u6570\u5b57\u6c38\u8fdc\u7ed9\u4e0d\u4e86\u7684\u3002</p>"},{"location":"zh/08_gt_vs_pred/#psnr","title":"PSNR \u2014\u2014 \u795e\u7ecf\u6e32\u67d3\u7684\u6807\u51c6\u6307\u6807","text":"<p>$$\\text{PSNR} = -10 \\log_{10}(\\text{MSE})$$</p> <p>\u6bcf\u5e27\u4ece\u5b9e\u65f6 loss \u66f4\u65b0\u3002\u4e00\u4e2a\u6570\u5b57\u5c31\u77e5\u9053\u4f60\u7684\u6a21\u578b\u5904\u4e8e\u4ec0\u4e48\u6c34\u5e73\uff1a 20 dB\uff08\u6a21\u7cca\u4e00\u7247\uff09\u300130 dB\uff08\u8fd8\u884c\uff09\u3001\u8fd8\u662f 40 dB\uff08\u5f88\u9510\u5229\uff09\u3002 \u5b9e\u65f6\u66f2\u7ebf\u544a\u8bc9\u4f60\u8bad\u7ec3\u4ec0\u4e48\u65f6\u5019\u5f00\u59cb\u505c\u6ede\u3002</p> <pre><code>def compute_psnr(mse):\n    return -10.0 * math.log10(mse) if mse &gt; 0 else 50.0\n</code></pre>"},{"location":"zh/08_gt_vs_pred/#panelplot-tensorboard-loss","title":"panel.plot() \u2014\u2014 \u4e0d\u7528 TensorBoard \u7684 loss \u66f2\u7ebf","text":"<pre><code>metrics_panel.plot(state[\"loss_history\"],\n                   label=\"##loss\",\n                   overlay=f\"{state['loss']:.5f}\",\n                   height=80)\n</code></pre> <p>\u4f20\u4e00\u4e2a Python float \u5217\u8868\u8fdb\u53bb\uff0c\u753b\u4e00\u6761\u6298\u7ebf\u3002<code>overlay</code> \u6587\u5b57\u663e\u793a\u5728\u56fe\u8868\u4e0a\u65b9\u3002 \u4fdd\u7559\u6700\u8fd1 300 \u4e2a\u503c\u4f5c\u4e3a\u6eda\u52a8\u7a97\u53e3\u3002\u4e0d\u9700\u8981\u4efb\u4f55\u5916\u90e8\u65e5\u5fd7\u5e93\u3002</p>"},{"location":"zh/08_gt_vs_pred/#_6","title":"\u8bef\u5dee\u6a21\u5f0f \u2014\u2014 \u4e0d\u540c\u8303\u6570\u66b4\u9732\u4e0d\u540c\u95ee\u9898","text":"<ul> <li>L1\uff08<code>abs().mean(dim=-1)</code>\uff09\u2014\u2014 \u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u3002\u663e\u793a\u9884\u6d4b\u6574\u4f53\u54ea\u91cc\u504f\u4e86\u3002</li> <li>L2\uff08<code>square().mean(dim=-1).sqrt()</code>\uff09\u2014\u2014 \u5747\u65b9\u6839\u8bef\u5dee\u3002\u6bd4 L1 \u66f4\u653e\u5927\u79bb\u7fa4\u503c\u3002</li> <li>\u9010\u901a\u9053\u6700\u5927\u503c\uff08<code>abs().max(dim=-1)</code>\uff09\u2014\u2014 \u6700\u5dee\u7684\u901a\u9053\u3002   \u66b4\u9732\u989c\u8272\u901a\u9053\u4e0d\u5339\u914d\uff08\u6bd4\u5982\u84dd\u901a\u9053\u9519\u4e86\u4f46\u7ea2\u7eff\u6ca1\u95ee\u9898\uff09\u3002</li> </ul> <p>\u8fd0\u884c\u65f6\u7528 combo \u5207\u6362\u3002\u4e0d\u540c\u7684\u8bef\u5dee\u8303\u6570\u8ba9\u4e0d\u540c\u7684\u95ee\u9898\u53d8\u5f97\u53ef\u89c1\u3002</p>"},{"location":"zh/08_gt_vs_pred/#03","title":"\u76f8\u6bd4\u4f8b\u5b50 03 \u7684\u5347\u7ea7","text":"<p>\u4f8b\u5b50 03 \u53ea\u6709 GT \u548c\u9884\u6d4b\u5e76\u6392\u3002\u672c\u4f8b\u589e\u52a0\u4e86\uff1a</p> <ul> <li>\u8bef\u5dee\u70ed\u529b\u56fe \u2014\u2014 \u770b\u5230\u6a21\u578b\u54ea\u91cc\u9519\u4e86\uff0c\u4e0d\u53ea\u662f\u9519\u4e86\u591a\u5c11\u3002</li> <li>PSNR + loss \u66f2\u7ebf \u2014\u2014 \u5b9e\u65f6\u6307\u6807\uff0c\u4e0d\u53ea\u662f\u6587\u672c\u8ba1\u6570\u5668\u3002</li> <li>\u8bef\u5dee\u589e\u76ca \u2014\u2014 \u653e\u5927\u5fae\u5c0f\u8bef\u5dee\u4f7f\u5176\u53ef\u89c1\u3002</li> <li>\u8bef\u5dee\u6a21\u5f0f\u5207\u6362 \u2014\u2014 \u8fd0\u884c\u65f6 L1/L2/\u9010\u901a\u9053\u3002</li> </ul> <p>\u8fd9\u5c31\u662f\"\u6211\u7684\u6a21\u578b\u5728\u8bad\u7ec3\"\u548c\"\u6211\u80fd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c03\u8bd5\u6a21\u578b\"\u7684\u533a\u522b\u3002</p>"},{"location":"zh/08_gt_vs_pred/#_7","title":"\u8981\u70b9","text":"<ol> <li> <p>\u4e09\u9762\u677f\u5bf9\u6bd4 \u2014\u2014 GT\u3001\u9884\u6d4b\u3001\u8bef\u5dee\u70ed\u529b\u56fe\u3002\u8fd9\u662f\u6240\u6709\u91cd\u5efa\u4efb\u52a1    \u6700\u57fa\u672c\u7684\u89c6\u89c9\u8c03\u8bd5\u5e03\u5c40\u3002</p> </li> <li> <p>\u8bef\u5dee\u70ed\u529b\u56fe = \u653e\u5927\u5dee\u503c + \u8272\u56fe \u2014\u2014 \u589e\u76ca\u6ed1\u6761\u662f\u5173\u952e\u3002    \u4e0d\u653e\u5927\u7684\u8bdd\u4f4e\u8bef\u5dee\u6839\u672c\u770b\u4e0d\u89c1\u3002</p> </li> <li> <p>\u5b9e\u65f6 PSNR \u2014\u2014 \u603b\u7ed3\u91cd\u5efa\u8d28\u91cf\u7684\u4e00\u4e2a\u6570\u5b57\u3002    $-10 \\log_{10}(\\text{MSE})$\uff0c\u76f4\u63a5\u4ece\u5df2\u6709\u7684 loss \u7b97\u3002</p> </li> <li> <p>panel.plot() \u2014\u2014 \u540c\u4e00\u4e2a\u7a97\u53e3\u91cc\u7684\u5373\u65f6 loss/\u6307\u6807\u66f2\u7ebf\u3002    \u4e0d\u9700\u8981 TensorBoard\uff0c\u4e0d\u9700\u8981 wandb\uff0c\u4e0d\u9700\u8981 alt-tab\u3002</p> </li> <li> <p>\u8fd0\u884c\u65f6\u5207\u6362\u8bef\u5dee\u6a21\u5f0f \u2014\u2014 \u4e0d\u540c\u8303\u6570\u66b4\u9732\u4e0d\u540c\u95ee\u9898\u3002    L1 \u770b\u6574\u4f53\u8bef\u5dee\uff0cL2 \u770b\u79bb\u7fa4\u503c\uff0c\u9010\u901a\u9053\u770b\u989c\u8272 bug\u3002</p> </li> </ol> <p>\u63d0\u793a</p> <p>\u8bad\u7ec3\u6536\u655b\u540e\u628a\"Error Gain\"\u62c9\u5230 15\u201320\u00d7\u3002\u4f60\u4f1a\u770b\u5230\u6b8b\u4f59\u8bef\u5dee\u7684\u5206\u5e03 \u2014\u2014 \u901a\u5e38\u96c6\u4e2d\u5728\u8fb9\u7f18\u548c\u9ad8\u9891\u7eb9\u7406\u4e0a\uff0c\u8fd9\u4f1a\u544a\u8bc9\u4f60\u662f\u5426\u9700\u8981\u4f4d\u7f6e\u7f16\u7801\u6216\u66f4\u6df1\u7684\u7f51\u7edc\u3002</p> <p>\u8bf4\u660e</p> <p>\u672c\u4f8b\u7528\u7684\u662f\u4e00\u4e2a\u7b80\u5355\u7684 3 \u5c42 MLP\uff0c\u4e3a\u4e86\u8dd1\u5f97\u5feb\u3002 \u628a\u5b83\u6362\u6210\u4f60\u81ea\u5df1\u7684\u6a21\u578b\u548c\u8bad\u7ec3\u5faa\u73af \u2014\u2014 \u53ef\u89c6\u5316\u4ee3\u7801\u5b8c\u5168\u4e0d\u7528\u6539\u3002</p>"},{"location":"zh/09_live_tuning/","title":"09 \u2014 \u5b9e\u65f6\u8d85\u53c2\u6570\u8c03\u4f18","text":"<p>\u793a\u4f8b\u6587\u4ef6: <code>examples/09_live_tuning.py</code></p> <p>\u4f60\u6b63\u5728\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b\u3002Loss \u5361\u4f4f\u4e86\u3002\u4f60\u60f3\u6362\u4e2a\u5b66\u4e60\u7387\u8bd5\u8bd5\u3002 \u4e8e\u662f\u4f60\u6740\u6389\u8fdb\u7a0b\uff0c\u6539\u811a\u672c\uff0c\u91cd\u542f\uff0c\u7b49\u521d\u59cb\u5316\uff0c\u7b49\u524d 500 \u8f6e \u91cd\u65b0\u8dd1\u5b8c\u4e4b\u524d\u7684\u8fdb\u5ea6\uff0c\u7136\u540e\u624d\u80fd\u770b\u5230\u65b0 LR \u6709\u6ca1\u6709\u7528\u3002</p> <p>\u6216\u8005\u2014\u2014\u4f60\u53ef\u4ee5\u76f4\u63a5\u62d6\u4e00\u4e2a\u6ed1\u5757\u3002</p> <p>\u8fd9\u4e2a\u4f8b\u5b50\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u4fee\u6539\uff1a\u5b66\u4e60\u7387\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570\u3001 \u6743\u91cd\u8870\u51cf\u3002\u4e0d\u7528\u91cd\u542f\uff0c\u4e0d\u7528\u5b58 checkpoint\uff0c\u4e0d\u7528\u5199\u4efb\u4f55\u989d\u5916\u4ee3\u7801\u3002</p> <p>\u79d8\u5bc6\u6b66\u5668\u662f <code>step()</code> / <code>end_step()</code> \u2014\u2014\u5b83\u4eec\u628a\u4e3b\u5faa\u73af\u7684\u63a7\u5236\u6743 \u4ea4\u7ed9\u4f60\u7684\u8bad\u7ec3\u4ee3\u7801\uff0c\u800c Vultorch \u5728\u6bcf\u4e00\u6b65\u8d1f\u8d23\u6e32\u67d3\u3002</p>"},{"location":"zh/09_live_tuning/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5b83\u505a\u4ec0\u4e48 \u4e3a\u4ec0\u4e48\u91cd\u8981 <code>view.step()</code> \u5904\u7406\u4e00\u5e27\uff0c\u7a97\u53e3\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code> \u4f60\u7684\u8bad\u7ec3 <code>while</code> \u5faa\u73af\u62e5\u6709\u5916\u5c42\u8fed\u4ee3 <code>view.end_step()</code> \u7ed3\u675f\u5f53\u524d\u5e27 \u4e0e <code>step()</code> \u914d\u5bf9\u4f7f\u7528\uff0c\u66ff\u4ee3 <code>run()</code> <code>view.close()</code> \u663e\u5f0f\u9500\u6bc1\u7a97\u53e3 \u5728 <code>finally</code> \u4e2d\u8c03\u7528\uff0c\u786e\u4fdd\u5e72\u51c0\u5173\u95ed \u5bf9\u6570 LR \u6ed1\u5757 slider \u8303\u56f4 -5 \u5230 -1\uff0c\u7136\u540e <code>10 ** value</code> \u7ebf\u6027\u6ed1\u5757\u5bf9\u5b66\u4e60\u7387\u5b8c\u5168\u6ca1\u7528\u2014\u2014\u5fc5\u987b\u7528\u5bf9\u6570\u5c3a\u5ea6 \u4f18\u5316\u5668\u70ed\u5207\u6362 \u68c0\u6d4b combo \u53d8\u5316\uff0c\u91cd\u5efa optimizer \u4e0d\u91cd\u542f\u5c31\u80fd\u5728 Adam \u2194 SGD \u2194 AdamW \u4e4b\u95f4\u5207\u6362 <code>compute_loss()</code> MSE / L1 / Huber \u7531 combo \u9009\u62e9 \u4e0d\u540c\u635f\u5931\u51fd\u6570\u5bf9\u4e0d\u540c\u95ee\u9898\uff0c\u8fd0\u884c\u65f6\u53ef\u5207\u6362"},{"location":"zh/09_live_tuning/#_2","title":"\u6211\u4eec\u8981\u505a\u4ec0\u4e48","text":"<p>\u548c\u793a\u4f8b 08 \u4e00\u6837\u7684\u5750\u6807 MLP\uff0c\u4f46\u73b0\u5728\u6709\u4e00\u4e2a\u63a7\u5236\u4fa7\u8fb9\u680f\u53ef\u4ee5\u8ba9\u4f60\uff1a</p> <ul> <li>\u62d6\u52a8 LR \u2014 \u5bf9\u6570\u5c3a\u5ea6\u6ed1\u5757\uff081e-5 \u5230 0.1\uff09</li> <li>\u5207\u6362\u4f18\u5316\u5668 \u2014 Adam\u3001SGD\uff08\u5e26\u52a8\u91cf\uff09\u3001AdamW</li> <li>\u5207\u6362\u635f\u5931\u51fd\u6570 \u2014 MSE\u3001L1\u3001Huber</li> <li>\u8c03\u6574\u6743\u91cd\u8870\u51cf \u2014 \u5b9e\u65f6\u751f\u6548</li> <li>\u91cd\u7f6e\u6a21\u578b \u2014 \u4e00\u952e\u6062\u590d\u968f\u673a\u6743\u91cd</li> <li>\u5373\u65f6\u770b\u5230\u6548\u679c \u2014 \u9884\u6d4b\u56fe\u50cf\u548c loss \u66f2\u7ebf\u7acb\u523b\u53cd\u6620\u53d8\u5316</li> </ul>"},{"location":"zh/09_live_tuning/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>from pathlib import Path\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport vultorch\n\ndevice = \"cuda\"\n\n# \u52a0\u8f7d\u76ee\u6807\u56fe\u50cf\nimg_path = Path(__file__).resolve().parents[1] / \"docs\" / \"images\" / \"pytorch_logo.png\"\ngt = vultorch.imread(img_path, channels=3, size=(256, 256), device=device)\nH, W = gt.shape[0], gt.shape[1]\n\n# \u5750\u6807\u7f51\u683c\ncoords = ...  # (H*W, 2) in [-1, 1]\ntarget = gt.reshape(-1, 3)\n\n\nclass CoordMLP(nn.Module):\n    def __init__(self, hidden=64, layers=3):\n        super().__init__()\n        net = [nn.Linear(2, hidden), nn.ReLU(inplace=True)]\n        for _ in range(layers - 1):\n            net += [nn.Linear(hidden, hidden), nn.ReLU(inplace=True)]\n        net += [nn.Linear(hidden, 3), nn.Sigmoid()]\n        self.net = nn.Sequential(*net)\n    def forward(self, x):\n        return self.net(x)\n\n\nOPTIMIZER_NAMES = [\"Adam\", \"SGD\", \"AdamW\"]\nLOSS_NAMES = [\"MSE\", \"L1\", \"Huber\"]\n\n\ndef make_optimizer(model, name, lr, weight_decay):\n    if name == \"Adam\":\n        return torch.optim.Adam(model.parameters(), lr=lr,\n                                weight_decay=weight_decay)\n    elif name == \"SGD\":\n        return torch.optim.SGD(model.parameters(), lr=lr,\n                               momentum=0.9, weight_decay=weight_decay)\n    elif name == \"AdamW\":\n        return torch.optim.AdamW(model.parameters(), lr=lr,\n                                 weight_decay=weight_decay)\n\n\ndef compute_loss(pred, target, loss_name):\n    if loss_name == \"MSE\":  return F.mse_loss(pred, target)\n    elif loss_name == \"L1\": return F.l1_loss(pred, target)\n    elif loss_name == \"Huber\": return F.smooth_l1_loss(pred, target)\n\n\nmodel = CoordMLP().to(device)\noptimizer = make_optimizer(model, \"Adam\", 2e-3, 0.0)\n\n# View + \u9762\u677f\nview = vultorch.View(\"09 - Live Hyperparameter Tuning\", 1100, 900)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.30)\npred_panel = view.panel(\"Prediction\")\nmetrics_panel = view.panel(\"Metrics\")\n\npred_rgba = vultorch.create_tensor(H, W, 4, device, name=\"pred\",\n                                    window=view.window)\npred_rgba[:, :, 3] = 1.0\npred_panel.canvas(\"pred\").bind(pred_rgba)\n\n\n@ctrl.on_frame\ndef draw_controls():\n    log_lr = ctrl.slider(\"log10(LR)\", -5.0, -1.0, default=-2.7)\n    state[\"lr\"] = 10.0 ** log_lr\n    for pg in optimizer.param_groups:\n        pg[\"lr\"] = state[\"lr\"]\n\n    state[\"optimizer_idx\"] = ctrl.combo(\"Optimizer\", OPTIMIZER_NAMES)\n    state[\"loss_idx\"] = ctrl.combo(\"Loss\", LOSS_NAMES)\n\n    if ctrl.button(\"Reset Model\"):\n        state[\"needs_reset\"] = True\n\n\n# \u8bad\u7ec3\u5faa\u73af \u2014 \u7528 step()/end_step() \u800c\u4e0d\u662f run()\ntry:\n    while view.step():\n        if state[\"needs_reset\"]:\n            model = CoordMLP().to(device)\n            optimizer = make_optimizer(...)\n            state[\"needs_reset\"] = False\n\n        if state[\"optimizer_idx\"] != state[\"prev_optimizer_idx\"]:\n            optimizer = make_optimizer(...)\n            state[\"prev_optimizer_idx\"] = state[\"optimizer_idx\"]\n\n        for _ in range(state[\"steps_per_frame\"]):\n            optimizer.zero_grad(set_to_none=True)\n            out = model(coords)\n            loss = compute_loss(out, target, LOSS_NAMES[state[\"loss_idx\"]])\n            loss.backward()\n            optimizer.step()\n\n        with torch.no_grad():\n            pred = model(coords).reshape(H, W, 3).clamp_(0, 1)\n            pred_rgba[:, :, :3] = pred\n\n        view.end_step()\nfinally:\n    view.close()\n</code></pre> <p>(\u7cbe\u7b80\u7248\u2014\u2014\u5b8c\u6574\u4ee3\u7801\u89c1 <code>examples/09_live_tuning.py</code>\u3002)</p>"},{"location":"zh/09_live_tuning/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/09_live_tuning/#step-end_step","title":"step() / end_step() \u2014 \u4f60\u7684\u8bad\u7ec3\u5faa\u73af\u505a\u4e3b","text":"<p>\u5728\u793a\u4f8b 01\u201308 \u4e2d\u6211\u4eec\u90fd\u7528 <code>view.run()</code>\u3002\u5f88\u65b9\u4fbf\u2014\u2014Vultorch \u62e5\u6709\u4e3b\u5faa\u73af\u7136\u540e\u8c03\u4f60\u7684\u56de\u8c03\u3002 \u4f46\u5728\u8bad\u7ec3\u573a\u666f\u91cc\uff0c\u4f60\u624d\u5e94\u8be5\u62e5\u6709\u4e3b\u5faa\u73af\uff1a</p> <pre><code>try:\n    while view.step():\n        # ... \u4f60\u7684\u8bad\u7ec3\u4ee3\u7801 ...\n        view.end_step()\nfinally:\n    view.close()\n</code></pre> <p><code>step()</code> \u5904\u7406\u4e00\u5e27\u7684\u8f93\u5165\u548c\u6e32\u67d3\uff0c\u7a97\u53e3\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>end_step()</code> \u7ed3\u675f\u8fd9\u4e00\u5e27\u3002\u4f60\u7684\u8bad\u7ec3\u4ee3\u7801\u653e\u5728\u5b83\u4eec\u4e2d\u95f4\u3002</p> <p>\u53ef\u4ee5\u628a\u5b83\u60f3\u8c61\u6210 matplotlib \u7684 <code>ion()</code> \u6a21\u5f0f\u2014\u2014\u56fe\u5728\u66f4\u65b0\uff0c\u4f46\u4f60\u7684\u811a\u672c \u8fd8\u5728\u7ee7\u7eed\u8dd1\u3002\u53ea\u4e0d\u8fc7\u8fd9\u91cc\u662f 60fps\u3001\u96f6\u62f7\u8d1d\u3001\u8fd8\u6709\u6ed1\u5757\u53ef\u4ee5\u62d6\u3002</p>"},{"location":"zh/09_live_tuning/#lr","title":"\u5bf9\u6570\u5c3a\u5ea6 LR \u6ed1\u5757","text":"<p>\u7ebf\u6027\u6ed1\u5757\u5bf9\u5b66\u4e60\u7387\u6765\u8bf4\u662f\u707e\u96be\u30021e-4 \u548c 2e-4 \u7684\u5dee\u522b\u5f88\u91cd\u8981\uff0c\u4f46\u5728 0 \u5230 0.1 \u7684\u7ebf\u6027\u6ed1\u5757\u4e0a\uff0c\u90a3\u53ea\u662f\u4e00\u4e2a\u770b\u4e0d\u89c1\u7684\u5c0f\u683c\u5b50\u3002\u5bf9\u6570\u5c3a\u5ea6\u89e3\u51b3\u95ee\u9898\uff1a</p> <pre><code>log_lr = ctrl.slider(\"log10(LR)\", -5.0, -1.0, default=-2.7)\nstate[\"lr\"] = 10.0 ** log_lr\n</code></pre> <p>\u6ed1\u5757\u4f4d\u7f6e -5.0 = LR 1e-5\uff0c\u4f4d\u7f6e -1.0 = LR 0.1\u3002\u73b0\u5728\u4f60\u53ef\u4ee5\u7cbe\u7ec6\u63a7\u5236 \u4ece\u5fae\u8c03\u5230\u6fc0\u8fdb\u9884\u70ed\u7684\u6240\u6709\u5b66\u4e60\u7387\u8303\u56f4\u3002</p> <p>\u5e94\u7528\u5230\u5f53\u524d\u4f18\u5316\u5668\uff1a</p> <pre><code>for pg in optimizer.param_groups:\n    pg[\"lr\"] = state[\"lr\"]\n</code></pre> <p>PyTorch \u7684\u4f18\u5316\u5668\u5728\u6bcf\u6b21 <code>step()</code> \u65f6\u4ece <code>param_groups</code> \u8bfb\u53d6 <code>lr</code>\u3002 \u6539\u4e86\u90a3\u91cc\uff0c\u4e0b\u4e00\u6b21 <code>optimizer.step()</code> \u5c31\u7528\u65b0\u503c\u3002\u4e0d\u9700\u8981\u91cd\u5efa\u3002</p>"},{"location":"zh/09_live_tuning/#_5","title":"\u4f18\u5316\u5668\u70ed\u5207\u6362","text":"<p>\u5f53\u4f60\u628a combo \u4ece Adam \u5207\u5230 SGD \u65f6\uff0c\u6211\u4eec\u5fc5\u987b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 optimizer \u5bf9\u8c61\u2014\u2014\u6ca1\u6709\u529e\u6cd5\u628a\u4e00\u79cd\u53d8\u5f62\u6210\u53e6\u4e00\u79cd\uff1a</p> <pre><code>if state[\"optimizer_idx\"] != state[\"prev_optimizer_idx\"]:\n    optimizer = make_optimizer(\n        model, OPTIMIZER_NAMES[state[\"optimizer_idx\"]],\n        state[\"lr\"], state[\"weight_decay\"])\n    state[\"prev_optimizer_idx\"] = state[\"optimizer_idx\"]\n</code></pre> <p>\u6a21\u578b\u7684\u53c2\u6570\u4e0d\u53d8\u2014\u2014\u53ea\u6709\u4f18\u5316\u5668\u72b6\u6001\uff08\u52a8\u91cf\u7f13\u51b2\u533a\u3001Adam \u7684\u6ed1\u52a8\u5e73\u5747\uff09 \u88ab\u91cd\u7f6e\u3002\u8fd9\u5176\u5b9e\u5f88\u6709\u7528\uff1a\u6709\u65f6\u5019\u5207\u5230 SGD \u8dd1\u51e0\u8f6e\u53ef\u4ee5\u5e2e\u6a21\u578b\u8df3\u51fa Adam \u9677\u5165\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002</p>"},{"location":"zh/09_live_tuning/#_6","title":"\u635f\u5931\u51fd\u6570\u5207\u6362","text":"<p>\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u5f3a\u8c03\u4e0d\u540c\u7684\u8bef\u5dee\u6a21\u5f0f\uff1a</p> <ul> <li>MSE \u2014 \u4e8c\u6b21\u60e9\u7f5a\u5927\u8bef\u5dee\u3002PSNR \u7684\u6807\u51c6\u3002</li> <li>L1 \u2014 \u5e73\u7b49\u60e9\u7f5a\u6240\u6709\u8bef\u5dee\u3002\u5bf9\u5f02\u5e38\u503c\u66f4\u9c81\u68d2\uff0c\u4f46\u96f6\u9644\u8fd1\u68af\u5ea6\u6052\u5b9a   \uff08\u53ef\u80fd\u5bfc\u81f4\u9707\u8361\uff09\u3002</li> <li>Huber \u2014 \u96f6\u9644\u8fd1\u7528 MSE\uff0c\u8fdc\u5904\u7528 L1\u3002\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u4eba\u4eba\u90fd\u7528\u7684   \"\u4e24\u5168\u5176\u7f8e\"\u9009\u62e9\u3002</li> </ul> <p>\u8fd0\u884c\u65f6\u5207\u6362\u53ef\u4ee5\u8ba9\u4f60\u76f4\u89c2\u770b\u5230\u4e0d\u540c\u635f\u5931\u51fd\u6570\u5bf9\u6536\u655b\u7684\u5f71\u54cd\u3002 \u8bd5\u8bd5\u5148\u7528 MSE \u8dd1 1000 \u8f6e\uff0c\u7136\u540e\u5207\u5230 Huber\u2014\u2014\u4f60\u53ef\u80fd\u4f1a\u770b\u5230 loss \u8fdb\u4e00\u6b65\u4e0b\u964d\uff0c\u56e0\u4e3a Huber \u66f4\u597d\u5730\u5904\u7406\u4e86\u79bb\u7fa4\u50cf\u7d20\u3002</p>"},{"location":"zh/09_live_tuning/#_7","title":"\u6a21\u578b\u91cd\u7f6e","text":"<pre><code>if ctrl.button(\"Reset Model\"):\n    state[\"needs_reset\"] = True\n</code></pre> <p>\u5ef6\u8fdf\u5230\u8bad\u7ec3\u5faa\u73af\u4e2d\u5904\u7406\uff08\u4e0d\u5728\u56de\u8c03\u5185\u90e8\uff09\uff0c\u8fd9\u6837\u6a21\u578b\u91cd\u5efa\u53d1\u751f\u5728\u6b63\u786e\u7684\u65f6\u673a\u3002 \u65b0\u6a21\u578b\u5f97\u5230\u5168\u65b0\u7684\u968f\u673a\u6743\u91cd\u3001\u5168\u65b0\u7684\u4f18\u5316\u5668\uff0c\u8ba1\u6570\u5668\u5f52\u96f6\u3002</p>"},{"location":"zh/09_live_tuning/#_8","title":"\u6838\u5fc3\u8981\u70b9","text":"<ol> <li> <p><code>step()</code> / <code>end_step()</code> \u2014 \u5f53\u4f60\u9700\u8981\u81ea\u5b9a\u4e49\u8bad\u7ec3\u5faa\u73af\u65f6\u7528\u8fd9\u5bf9\uff0c    \u66ff\u4ee3 <code>run()</code>\u3002\u4f60\u7684 <code>while</code> \u5faa\u73af\u638c\u63a7\u8fed\u4ee3\u3002</p> </li> <li> <p>\u5bf9\u6570 LR \u6ed1\u5757 \u2014 \u5b66\u4e60\u7387\u8de8\u8d8a\u591a\u4e2a\u6570\u91cf\u7ea7\u3002\u7ebf\u6027\u6ed1\u5757\u6ca1\u7528\uff0c    \u7528 <code>10 ** slider_value</code>\u3002</p> </li> <li> <p>\u4f18\u5316\u5668\u70ed\u5207\u6362 \u2014 \u8fd0\u884c\u65f6\u4ece Adam \u5207\u5230 SGD \u518d\u5207\u5230 AdamW\u3002    \u6a21\u578b\u53c2\u6570\u4fdd\u7559\uff0c\u53ea\u6709\u4f18\u5316\u5668\u72b6\u6001\u91cd\u7f6e\u3002</p> </li> <li> <p>\u635f\u5931\u51fd\u6570\u5207\u6362 \u2014 MSE\u3001L1\u3001Huber \u8fd0\u884c\u65f6\u53ef\u5207\u6362\u3002\u4e0d\u540c\u635f\u5931    \u63ed\u793a\u4e0d\u540c\u7684\u8bad\u7ec3\u52a8\u6001\u3002</p> </li> <li> <p>\u5373\u65f6\u53cd\u9988 \u2014 \u6bcf\u6b21\u6ed1\u5757\u53d8\u5316\u5728\u4e0b\u4e00\u4e2a\u8bad\u7ec3\u6b65\u5c31\u751f\u6548\u3002\u4e0d\u7528\u91cd\u542f\uff0c    \u4e0d\u7528\u5b58\u6863\uff0c\u4e0d\u7528\u4efb\u4f55\u989d\u5916\u4ee3\u7801\u3002</p> </li> </ol> <p>\u5c0f\u8d34\u58eb</p> <p>\u5148\u7528 Adam + LR 2e-3 \u5f00\u59cb\u8bad\u7ec3\uff0c\u7b49 loss \u8d8b\u4e8e\u5e73\u7a33\u540e\uff0c \u8bd5\u8bd5\u5207\u5230 SGD with momentum\u3002\u4e0d\u540c\u7684\u4f18\u5316\u5730\u5f62\u6709\u65f6\u5019 \u80fd\u6539\u5584\u6700\u7ec8\u8d28\u91cf\u3002</p> <p>\u6ce8\u610f</p> <p>PSNR \u59cb\u7ec8\u7528 MSE \u8ba1\u7b97\uff0c\u4e0e\u5f53\u524d\u9009\u62e9\u7684\u635f\u5931\u51fd\u6570\u65e0\u5173\uff0c \u8fd9\u6837 PSNR \u6570\u503c\u5728\u4e0d\u540c\u635f\u5931\u6a21\u5f0f\u4e0b\u4fdd\u6301\u53ef\u6bd4\u6027\u3002</p>"},{"location":"zh/10_gaussian2d/","title":"10 \u2014 \u4e8c\u7ef4\u9ad8\u65af\u6cfc\u6e85","text":"<p>\u793a\u4f8b\u6587\u4ef6: <code>examples/10_gaussian2d.py</code></p> <p>3D Gaussian Splatting\uff083DGS\uff09\u5e2d\u5377\u4e86\u795e\u7ecf\u6e32\u67d3\u9886\u57df\u3002\u4f46\u5b83\u7684\u6838\u5fc3\u601d\u60f3 \u51fa\u5947\u5730\u7b80\u5355\uff1a\u6492\u4e00\u5806\u5f69\u8272\u7684\u692d\u5706\u6591\u70b9\uff0calpha \u6df7\u5408\u5230\u4e00\u8d77\uff0c\u548c\u76ee\u6807\u56fe\u6bd4\u8f83\uff0c \u7136\u540e\u53cd\u5411\u4f20\u64ad\u3002</p> <p>\u8fd9\u4e2a\u4f8b\u5b50\u628a 3DGS \u84b8\u998f\u5230\u4e8c\u7ef4\u672c\u8d28\u3002\u6ca1\u6709\u76f8\u673a\u6295\u5f71\uff0c\u6ca1\u6709\u7403\u8c10\u51fd\u6570\uff0c \u6ca1\u6709\u5206\u5757\u5149\u6805\u5316\u3002\u5c31\u662f N \u4e2a\u53ef\u5b66\u4e60\u7684\u4e8c\u7ef4\u9ad8\u65af\u2014\u2014\u6bcf\u4e2a\u6709\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001 \u65cb\u8f6c\u3001\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\u2014\u2014\u901a\u8fc7\u53ef\u5fae\u5206 alpha \u5408\u6210\u6e32\u67d3\uff0c\u7528\u666e\u901a\u7684 PyTorch autograd \u4f18\u5316\u3002</p> <p>\u770b\u7740\u9ad8\u65af\u4eec\u5728\u753b\u5e03\u4e0a\u6e38\u8d70\u3001\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u53d8\u8272\uff0c\u6700\u7ec8\u91cd\u65b0\u7ec4\u88c5\u51fa\u76ee\u6807\u56fe\u50cf\u3002 \u8fd9\u5c31\u662f\u90a3\u4e2a\u80fd\u4ee5 100+ fps \u6e32\u67d3\u7167\u7247\u7ea7\u771f\u5b9e\u573a\u666f\u7684\u7b97\u6cd5\u3002</p>"},{"location":"zh/10_gaussian2d/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5b83\u505a\u4ec0\u4e48 \u4e3a\u4ec0\u4e48\u91cd\u8981 \u9ad8\u65af\u7684 <code>nn.Parameter</code> \u4f4d\u7f6e\u3001\u5bf9\u6570\u5c3a\u5ea6\u3001\u65cb\u8f6c\u3001\u539f\u59cb\u989c\u8272\u3001\u539f\u59cb\u4e0d\u900f\u660e\u5ea6 \u6bcf\u4e2a\u9ad8\u65af\u5c5e\u6027\u90fd\u53ef\u5b66\u4e60\u2014\u2014autograd \u641e\u5b9a\u5269\u4e0b\u7684 \u9006\u534f\u65b9\u5dee \u4ece\u5c3a\u5ea6 + \u65cb\u8f6c\u5f97\u5230 $\\Sigma^{-1}$ \u8ba9\u6bcf\u4e2a\u9ad8\u65af\u6709\u81ea\u5df1\u7684\u692d\u5706\u5f62\u72b6\u7684\u6570\u5b66 Alpha \u5408\u6210 $C = \\sum_i \\alpha_i T_i c_i$\uff0c\u5176\u4e2d $T_i = \\prod_{j"},{"location":"zh/11_3d_inspector/","title":"11 \u2014 3D \u8868\u9762\u68c0\u67e5\u5668","text":"<p>\u793a\u4f8b\u6587\u4ef6: <code>examples/11_3d_inspector.py</code></p> <p>\u4f60\u6e32\u67d3\u4e86\u4e00\u5f20\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u6216\u7eb9\u7406\u3002\u73b0\u5728\u4f60\u60f3\u4ece\u53e6\u4e00\u4e2a\u89d2\u5ea6\u770b\u770b\uff0c \u8ba9\u5149\u6253\u4e0a\u53bb\uff0c\u68c0\u67e5\u63a0\u5c04\u89d2\u5ea6\u4e0b\u6709\u6ca1\u6709\u7455\u75b5\u3002\u5728 matplotlib \u91cc\u4f60\u5f97\u8ddf <code>plot_surface()</code> \u548c <code>set_azim()</code> \u640f\u6597\u3002\u5728\u8fd9\u91cc\uff0c\u4f60\u53ea\u8981\u62d6\u9f20\u6807\u3002</p> <p>SceneView \u662f Vultorch \u7684 3D \u67e5\u770b\u5668\u7ec4\u4ef6\u3002\u5b83\u63a5\u6536\u4efb\u4f55 tensor\uff0c \u6620\u5c04\u5230 3D \u7a7a\u95f4\u4e2d\u7684\u4e00\u4e2a\u5e73\u9762\u4e0a\uff0c\u52a0\u4e0a Blinn-Phong \u5149\u7167\uff0c \u7136\u540e\u8ba9\u4f60\u7528\u9f20\u6807\u62d6\u62fd\u6765\u7ed5\u8f68\u9053\u65cb\u8f6c\u3002MSAA \u6297\u952f\u9f7f\u3001\u53ef\u8c03 FOV\u3001 \u5149\u7167\u65b9\u5411\u3001\u5149\u6cfd\u5ea6\u2014\u2014\u5168\u90e8\u5b9e\u65f6\u3002</p>"},{"location":"zh/11_3d_inspector/#_1","title":"\u65b0\u670b\u53cb","text":"\u65b0\u4e1c\u897f \u5b83\u505a\u4ec0\u4e48 \u4e3a\u4ec0\u4e48\u91cd\u8981 <code>SceneView</code> \u5e26\u8f68\u9053\u76f8\u673a\u7684 3D \u5e73\u9762\u67e5\u770b\u5668 \u7528\u9f20\u6807\u4ea4\u4e92\u5728 3D \u4e2d\u68c0\u67e5\u7eb9\u7406/\u8f93\u51fa <code>Camera</code> \u65b9\u4f4d\u89d2\u3001\u4ef0\u89d2\u3001\u8ddd\u79bb\u3001FOV \u7ed5\u573a\u666f\u65cb\u8f6c\uff1b\u53ef\u9f20\u6807\u62d6\u62fd\u6216\u7a0b\u5e8f\u63a7\u5236 <code>Light</code> \u65b9\u5411\u3001\u5f3a\u5ea6\u3001\u73af\u5883\u5149\u3001\u955c\u9762\u53cd\u5c04\u3001\u5149\u6cfd\u5ea6 \u5b8c\u5168\u53ef\u63a7\u7684 Blinn-Phong \u7740\u8272 <code>.set_tensor()</code> \u4e0a\u4f20\u4efb\u610f tensor \u5230 3D \u573a\u666f RGB\u3001RGBA\u3001\u7070\u5ea6\u2014\u2014\u81ea\u52a8\u6269\u5c55\u4e3a RGBA <code>.render()</code> \u5c06\u573a\u666f\u7ed8\u5236\u4e3a ImGui \u56fe\u50cf \u81ea\u52a8\u5904\u7406\u9f20\u6807\u62d6\u62fd\u3001\u76f8\u673a\u540c\u6b65\u3001\u7f29\u653e <code>.msaa</code> \u591a\u91cd\u91c7\u6837\u6297\u952f\u9f7f\uff081/2/4/8\uff09 \u4e0d\u540c\u8d28\u91cf/\u6027\u80fd\u6743\u8861\u4e0b\u7684\u5e73\u6ed1\u8fb9\u7f18 <code>.background</code> \u80cc\u666f\u989c\u8272\u5143\u7ec4 \u8bbe\u7f6e 3D \u5e73\u9762\u540e\u9762\u7684\u6e05\u9664\u8272"},{"location":"zh/11_3d_inspector/#_2","title":"\u6211\u4eec\u8981\u505a\u4ec0\u4e48","text":"<p>\u5728 GPU \u4e0a\u751f\u6210\u56db\u79cd\u7a0b\u5e8f\u5316\u7eb9\u7406\u2014\u2014\u68cb\u76d8\u683c\u3001\u5f84\u5411\u6e10\u53d8\u3001\u6b63\u5f26\u56fe\u6848\u3001\u6cd5\u7ebf\u56fe\u2014\u2014 \u6bcf\u6b21\u663e\u793a\u4e00\u79cd\u5728 3D \u5e73\u9762\u4e0a\u3002\u9f20\u6807\u62d6\u62fd\u65cb\u8f6c\uff0c\u4fa7\u8fb9\u680f\u63a7\u5236\u76f8\u673a\u3001\u5149\u7167\u3001 MSAA \u548c\u80cc\u666f\u989c\u8272\u3002</p>"},{"location":"zh/11_3d_inspector/#_3","title":"\u5b8c\u6574\u4ee3\u7801","text":"<pre><code>import math\n\nimport torch\nimport vultorch\nfrom vultorch import ui\n\ndevice = \"cuda\"\nH, W = 256, 256\n\nys = torch.linspace(-1, 1, H, device=device)\nxs = torch.linspace(-1, 1, W, device=device)\nyy, xx = torch.meshgrid(ys, xs, indexing=\"ij\")\n\n\ndef make_checkerboard(freq=8.0):\n    check = ((xx * freq).floor() + (yy * freq).floor()) % 2\n    return torch.stack([check*0.9+0.1, check*0.1+0.2, check*0.3+0.4], dim=-1)\n\n\ndef make_radial_gradient():\n    r = (xx**2 + yy**2).sqrt()\n    val = (1.0 - r).clamp(0, 1)\n    return torch.stack([val, val*0.5, val*0.2], dim=-1)\n\n\ndef make_sine_pattern(freq=6.0):\n    s1 = (torch.sin(xx * freq * math.pi) * 0.5 + 0.5)\n    s2 = (torch.sin(yy * freq * math.pi) * 0.5 + 0.5)\n    val = s1 * s2\n    return torch.stack([val*0.2+0.1, val*0.8+0.1, val*0.5+0.3], dim=-1)\n\n\ndef make_normal_map():\n    nx = xx * 0.5 + 0.5\n    ny = yy * 0.5 + 0.5\n    nz = (1.0 - xx**2 - yy**2).clamp(0, 1).sqrt() * 0.5 + 0.5\n    return torch.stack([nx, ny, nz], dim=-1)\n\n\nTEXTURE_NAMES = [\"Checkerboard\", \"Radial Gradient\", \"Sine Pattern\", \"Normal Map\"]\nTEXTURE_FNS = [make_checkerboard, make_radial_gradient,\n               make_sine_pattern, make_normal_map]\n\n# View + \u9762\u677f\nview = vultorch.View(\"11 - 3D Surface Inspector\", 1200, 800)\nctrl = view.panel(\"Controls\", side=\"left\", width=0.28)\nscene_panel = view.panel(\"3D View\")\n\n# SceneView \u653e\u5728\u9762\u677f\u91cc\nscene = vultorch.SceneView(\"Inspector\", 800, 600, msaa=4)\ncurrent_texture = make_checkerboard()\n\n\n@ctrl.on_frame\ndef draw_controls():\n    state[\"texture_idx\"] = ctrl.combo(\"Texture\", TEXTURE_NAMES)\n    state[\"fov\"] = ctrl.slider(\"FOV\", 10.0, 120.0, default=45.0)\n    state[\"distance\"] = ctrl.slider(\"Distance\", 1.0, 10.0, default=3.0)\n    state[\"auto_rotate\"] = ctrl.checkbox(\"Auto Rotate\")\n\n    # \u5149\u7167\u63a7\u5236\n    state[\"light_az\"] = ctrl.slider(\"Light Az\", -3.14, 3.14)\n    state[\"light_el\"] = ctrl.slider(\"Light El\", -3.14, 3.14)\n    state[\"ambient\"] = ctrl.slider(\"Ambient\", 0.0, 1.0, default=0.15)\n    state[\"specular\"] = ctrl.slider(\"Specular\", 0.0, 2.0, default=0.5)\n    state[\"shininess\"] = ctrl.slider(\"Shininess\", 1.0, 128.0, default=32.0)\n\n    state[\"msaa_idx\"] = ctrl.combo(\"MSAA\", [\"1\", \"2\", \"4\", \"8\"])\n    state[\"bg_color\"] = ctrl.color_picker(\"Background\")\n\n    if ctrl.button(\"Reset Camera\"):\n        scene.camera.reset()\n\n\n@scene_panel.on_frame\ndef draw_scene():\n    # \u5e94\u7528\u8bbe\u7f6e\u5230\u76f8\u673a\u3001\u5149\u7167\u3001\u80cc\u666f\n    scene.camera.fov = state[\"fov\"]\n    scene.camera.distance = state[\"distance\"]\n    if state[\"auto_rotate\"]:\n        scene.camera.azimuth += 0.02\n\n    scene.light.direction = (cos(el)*sin(az), sin(el), cos(el)*cos(az))\n    scene.light.ambient = state[\"ambient\"]\n    scene.light.specular = state[\"specular\"]\n    scene.light.shininess = state[\"shininess\"]\n    scene.msaa = msaa_val\n    scene.background = state[\"bg_color\"]\n\n    scene.set_tensor(current_texture)\n    scene.render()  # \u5728\u8fd9\u91cc\u7ed8\u5236 3D \u89c6\u56fe\n\n\nview.run()\n</code></pre> <p>(\u7cbe\u7b80\u7248\u2014\u2014\u5b8c\u6574\u4ee3\u7801\u89c1 <code>examples/11_3d_inspector.py</code>\u3002)</p>"},{"location":"zh/11_3d_inspector/#_4","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":""},{"location":"zh/11_3d_inspector/#sceneview-3d","title":"SceneView \u2014 \u9762\u677f\u91cc\u7684 3D","text":"<p>SceneView \u662f\u4e00\u4e2a\u81ea\u5305\u542b\u7684 3D \u67e5\u770b\u5668\u7ec4\u4ef6\u3002\u521b\u5efa\u4e00\u6b21\uff0c\u7136\u540e\u6bcf\u5e27\u8c03\u7528 <code>set_tensor()</code> \u548c <code>render()</code>\uff1a</p> <pre><code>scene = vultorch.SceneView(\"Inspector\", 800, 600, msaa=4)\n\n# \u5728\u9762\u677f\u56de\u8c03\u4e2d\uff1a\nscene.set_tensor(my_tensor)  # \u4e0a\u4f20\u7eb9\u7406\nscene.render()               # \u6e32\u67d3 + \u663e\u793a + \u5904\u7406\u9f20\u6807\n</code></pre> <p><code>render()</code> \u505a\u6240\u6709\u4e8b\u60c5\uff1a\u628a\u76f8\u673a/\u5149\u7167\u8bbe\u7f6e\u63a8\u9001\u5230 GPU\uff0c\u79bb\u5c4f\u6e32\u67d3\u573a\u666f\uff0c \u4f5c\u4e3a ImGui \u56fe\u50cf\u663e\u793a\uff0c\u5904\u7406\u9f20\u6807\u62d6\u62fd\u7684\u8f68\u9053/\u5e73\u79fb/\u7f29\u653e\uff0c \u7136\u540e\u628a\u76f8\u673a\u72b6\u6001\u62c9\u56de\u6765\u8ba9 Python \u770b\u5230\u66f4\u65b0\u540e\u7684\u65b9\u4f4d\u89d2/\u4ef0\u89d2\u3002</p>"},{"location":"zh/11_3d_inspector/#camera","title":"Camera \u2014 \u7528\u9f20\u6807\u6570\u5b66\u65cb\u8f6c","text":"<p>\u76f8\u673a\u7531 5 \u4e2a\u503c\u5b9a\u4e49\uff1a</p> \u5c5e\u6027 \u9ed8\u8ba4\u503c \u63a7\u5236\u4ec0\u4e48 <code>azimuth</code> 0.0 \u6c34\u5e73\u65cb\u8f6c\uff08\u5f27\u5ea6\uff09 <code>elevation</code> 0.6 \u5782\u76f4\u65cb\u8f6c\uff08\u5f27\u5ea6\uff09 <code>distance</code> 3.0 \u5230\u76ee\u6807\u70b9\u7684\u8ddd\u79bb <code>target</code> (0,0,0) \u6ce8\u89c6\u70b9 <code>fov</code> 45.0 \u89c6\u573a\u89d2\uff08\u5ea6\uff09 <p>\u5de6\u952e\u62d6\u62fd\u65cb\u8f6c\uff08\u65b9\u4f4d\u89d2 + \u4ef0\u89d2\uff09\uff0c\u53f3\u952e\u62d6\u62fd\u5e73\u79fb\uff08\u76ee\u6807\u70b9\uff09\uff0c \u4e2d\u952e\u62d6\u62fd/\u6eda\u8f6e\u7f29\u653e\uff08\u8ddd\u79bb\uff09\u3002\u5168\u90e8\u5185\u7f6e\u2014\u2014\u4e0d\u9700\u8981\u5199\u4ee3\u7801\u3002</p> <p>\u4e5f\u53ef\u4ee5\u7a0b\u5e8f\u5316\u8bbe\u7f6e\uff1a</p> <pre><code>scene.camera.fov = 90.0       # \u5e7f\u89d2\u955c\u5934\nscene.camera.distance = 5.0   # \u8fdc\u79bb\nscene.camera.azimuth += 0.02  # \u81ea\u52a8\u65cb\u8f6c\n</code></pre>"},{"location":"zh/11_3d_inspector/#light-blinn-phong","title":"Light \u2014 Blinn-Phong \u7740\u8272","text":"<p>\u5149\u6e90\u662f\u5e26 Blinn-Phong \u7740\u8272\u7684\u65b9\u5411\u5149\uff1a</p> <pre><code>scene.light.direction = (0.3, -1.0, 0.5)  # \u65b9\u5411\u5411\u91cf\nscene.light.intensity = 1.0                # \u6574\u4f53\u4eae\u5ea6\nscene.light.ambient = 0.15                 # \u586b\u5145\u5149\nscene.light.specular = 0.5                 # \u9ad8\u5149\u5f3a\u5ea6\nscene.light.shininess = 32.0               # \u9ad8\u5149\u9510\u5ea6\n</code></pre> <p>\u4f4e\u73af\u5883\u5149 + \u9ad8\u955c\u9762\u53cd\u5c04 = \u620f\u5267\u6027\u7684\u3001\u9ad8\u5bf9\u6bd4\u5ea6\u7684\u5916\u89c2\u3002 \u9ad8\u73af\u5883\u5149 + \u4f4e\u955c\u9762\u53cd\u5c04 = \u5e73\u5766\u7684\u3001\u5747\u5300\u7167\u660e\u7684\u5916\u89c2\u3002 \u4ea4\u4e92\u5f0f\u8c03\u6574\u8fd9\u4e9b\u53c2\u6570\u53ef\u4ee5\u5e2e\u4f60\u53d1\u73b0\u53ea\u5728\u7279\u5b9a\u5149\u7167\u89d2\u5ea6\u624d\u663e\u73b0\u7684\u8868\u9762\u7455\u75b5\u3002</p>"},{"location":"zh/11_3d_inspector/#msaa","title":"MSAA \u2014 \u6297\u952f\u9f7f\u8d28\u91cf","text":"<p>MSAA\uff08\u591a\u91cd\u91c7\u6837\u6297\u952f\u9f7f\uff09\u8ba9\u952f\u9f7f\u8fb9\u7f18\u53d8\u5e73\u6ed1\uff1a</p> MSAA \u6bcf\u50cf\u7d20\u91c7\u6837 \u8d28\u91cf \u6027\u80fd 1 1 \u6709\u952f\u9f7f \u6700\u5feb 2 2 \u7565\u5fae\u5e73\u6ed1 \u5feb 4 4 \u5e73\u6ed1 \u4e2d\u7b49 8 8 \u975e\u5e38\u5e73\u6ed1 \u6700\u6162 <pre><code>scene.msaa = 4  # \u597d\u7684\u9ed8\u8ba4\u503c\n</code></pre> <p>\u5927\u591a\u6570\u53ef\u89c6\u5316\u5de5\u4f5c\u4e2d\uff0c4\u00d7 MSAA \u662f\u6700\u4f73\u5e73\u8861\u70b9\u3002\u9700\u8981\u6700\u5927\u5e27\u7387\u65f6\u964d\u5230 1\uff0c \u622a\u56fe\u65f6\u4e0a\u5230 8\u3002</p>"},{"location":"zh/11_3d_inspector/#gpu","title":"GPU \u4e0a\u7684\u7a0b\u5e8f\u5316\u7eb9\u7406","text":"<p>\u56db\u79cd\u7eb9\u7406\u5168\u90e8\u662f\u7eaf PyTorch tensor \u8fd0\u7b97\u2014\u2014\u6ca1\u6709 CPU\uff0c\u6ca1\u6709 PIL\uff0c \u6ca1\u6709\u6587\u4ef6 I/O\uff1a</p> <pre><code>def make_checkerboard(freq=8.0):\n    check = ((xx * freq).floor() + (yy * freq).floor()) % 2\n    return torch.stack([check*0.9+0.1, check*0.1+0.2, check*0.3+0.4], dim=-1)\n</code></pre> <p>\u8fd9\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u5728\u771f\u5b9e\u7684\u795e\u7ecf\u6e32\u67d3\u7ba1\u7ebf\u4e2d\uff0c\u4f60\u5728\u8fd9\u91cc\u663e\u793a\u7684 tensor \u4f1a\u662f\u4f60\u7684 NeRF \u6e32\u67d3\u8f93\u51fa\u30013DGS \u7684\u6df1\u5ea6\u56fe\u3001\u6216\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u7eb9\u7406\u3002 SceneView \u4e0d\u5173\u5fc3 tensor \u4ece\u54ea\u6765\u2014\u2014\u5b83\u53ea\u8d1f\u8d23\u663e\u793a\u3002</p>"},{"location":"zh/11_3d_inspector/#_5","title":"\u6838\u5fc3\u8981\u70b9","text":"<ol> <li> <p>SceneView = 3D \u5e73\u9762\u67e5\u770b\u5668\u3002\u4e0a\u4f20 tensor\uff0c\u6e32\u67d3\uff0c\u9f20\u6807\u62d6\u62fd\u65cb\u8f6c\u3002    \u5c31\u8fd9\u4e48\u7b80\u5355\u3002</p> </li> <li> <p>Camera \u6709\u65b9\u4f4d\u89d2/\u4ef0\u89d2/\u8ddd\u79bb/FOV\u2014\u2014\u9f20\u6807\u62d6\u62fd\u6216 Python \u4ee3\u7801\u8bbe\u7f6e\u3002    <code>camera.reset()</code> \u56de\u5230\u9ed8\u8ba4\u503c\u3002</p> </li> <li> <p>Light \u662f Blinn-Phong\uff1a\u65b9\u5411\u3001\u5f3a\u5ea6\u3001\u73af\u5883\u5149\u3001\u955c\u9762\u53cd\u5c04\u3001\u5149\u6cfd\u5ea6\u3002    \u4ea4\u4e92\u5f0f\u5149\u7167\u63ed\u793a\u8868\u9762\u7455\u75b5\u3002</p> </li> <li> <p>MSAA \u4ece 1\uff08\u5feb\uff0c\u6709\u952f\u9f7f\uff09\u5230 8\uff08\u6162\uff0c\u5e73\u6ed1\uff09\u3002\u65e5\u5e38\u5de5\u4f5c\u7528 4\u3002</p> </li> <li> <p>\u4efb\u4f55 tensor \u90fd\u884c \u2014 RGB\u3001RGBA\u3001\u7070\u5ea6\u3002SceneView \u5185\u90e8\u81ea\u52a8\u6269\u5c55\u4e3a RGBA\u3002</p> </li> </ol> <p>\u5c0f\u8d34\u58eb</p> <p>\u7528\u81ea\u52a8\u65cb\u8f6c\uff08<code>scene.camera.azimuth += 0.02</code>\uff09\u5feb\u901f\u626b\u63cf\u8868\u9762 \u627e\u7455\u75b5\u2014\u2014\u4ece\u9ed8\u8ba4\u89d2\u5ea6\u770b\u4e0d\u89c1\u7684\u95ee\u9898\uff0c\u65cb\u8f6c\u8d77\u6765\u5c31\u4e00\u76ee\u4e86\u7136\u3002</p> <p>\u6ce8\u610f</p> <p>SceneView \u6e32\u67d3\u7684\u662f\u4e00\u4e2a\u5e73\u9762\u3002\u8981\u770b\u771f\u6b63\u7684 3D \u51e0\u4f55\u4f53\uff08\u7f51\u683c\u3001\u70b9\u4e91\uff09\uff0c \u9700\u8981\u6269\u5c55 C++ \u6e32\u67d3\u5668\u3002\u4f46\u5bf9\u4e8e\u68c0\u67e5\u9010\u50cf\u7d20\u8f93\u51fa\uff08\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u3001\u7eb9\u7406\uff09\uff0c \u4e00\u4e2a\u5e26\u5149\u7167\u548c\u8f68\u9053\u76f8\u673a\u7684 3D \u5e73\u9762\u6b63\u597d\u591f\u7528\u3002</p>"},{"location":"zh/12_neural_workstation/","title":"12 \u2014 \u795e\u7ecf\u6e32\u67d3\u5de5\u4f5c\u7ad9","text":"<p>\u538b\u8f74\u5927\u620f</p> <p>\u8fd9\u662f\u6700\u540e\u4e00\u4e2a\u793a\u4f8b\u2014\u2014\u4e00\u4e2a\u529f\u80fd\u5b8c\u5907\u7684\u795e\u7ecf\u6e32\u67d3 IDE\uff0c\u53ea\u7528\u4e00\u4e2a Python \u811a\u672c\u3002\u7528\u5b83\u4f5c\u4e3a\u6a21\u677f\uff0c\u53d1\u5e03\u4f60\u81ea\u5df1\u7814\u7a76\u7684\u7cbe\u7f8e\u4ea4\u4e92\u5f0f demo\u3002</p>"},{"location":"zh/12_neural_workstation/#_1","title":"\u672c\u7ae0\u65b0\u670b\u53cb","text":"\u540d\u79f0 \u5e72\u4ec0\u4e48\u7684 \u7c7b\u6bd4 \u6253\u5f00\u56fe\u50cf \u64cd\u4f5c\u7cfb\u7edf\u6587\u4ef6\u5bf9\u8bdd\u6846\u52a0\u8f7d\u4efb\u610f PNG/JPEG/BMP \u8bad\u7ec3 <code>plt.imread()</code> + \u91cd\u65b0\u8bad\u7ec3\uff0c\u4f46\u53ea\u9700\u4e00\u4e2a\u6309\u94ae \u4f4d\u7f6e\u7f16\u7801 \u5085\u91cc\u53f6\u7279\u5f81 $[\\sin(2^l \\pi x), \\cos(2^l \\pi x)]$ NeRF \u5b66\u4e60\u9ad8\u9891\u7ec6\u8282\u7684\u5173\u952e\u6280\u5de7 \u67b6\u6784\u6ed1\u5757 \u8fd0\u884c\u65f6\u6539\u53d8\u9690\u85cf\u5c42\u5bbd\u5ea6\u3001\u6df1\u5ea6\u3001PE \u7ea7\u522b \u7f16\u8f91\u6a21\u578b\u914d\u7f6e\u4e0d\u7528\u91cd\u542f \u8bef\u5dee\u589e\u76ca \u653e\u5927\u8bef\u5dee\u70ed\u529b\u56fe\u4ee5\u770b\u5230\u7ec6\u5fae\u5dee\u5f02 \u7ed9\u56fe\u7247\u62c9\u5bf9\u6bd4\u5ea6 \u6682\u505c/\u6062\u590d \u505c\u6b62\u8bad\u7ec3\u4f46 UI \u4fdd\u6301\u54cd\u5e94 <code>Ctrl-C</code> \u4f46\u4e0d\u6740\u8fdb\u7a0b \u4fdd\u5b58\u5feb\u7167 <code>Canvas.save()</code> \u628a\u5f53\u524d GPU tensor \u5199\u6210 PNG \u5b9e\u65f6 GPU \u6570\u636e\u7684 <code>plt.savefig()</code> \u8bad\u7ec3\u901f\u5ea6 \u8fed\u4ee3/\u79d2 \u8ba1\u6570\u5668 \u5185\u7f6e\u7684 <code>tqdm</code>"},{"location":"zh/12_neural_workstation/#depth","title":"\u4e3a\u4ec0\u4e48\u6ca1\u6709 depth \u5934\uff1f","text":"<p>\u4e4b\u524d\u7684\u7248\u672c\u6709 RGB + depth \u53cc\u5934 MLP\u3002\u4f46\u8fd9\u4e2a\u793a\u4f8b\u91cd\u5efa\u7684\u662f 2D \u56fe\u50cf\u2014\u2014 \u6ca1\u6709\u6709\u610f\u4e49\u7684\u6df1\u5ea6\u53ef\u4ee5\u9884\u6d4b\u3002\u6240\u4ee5\u6211\u4eec\u4fdd\u6301\u7b80\u6d01\uff1a\u4e00\u4e2a\u5934\uff0c\u4e09\u4e2a\u8f93\u51fa\uff08RGB\uff09\uff0c \u76f4\u63a5\u7528 MSE \u5bf9\u6bd4\u76ee\u6807\u56fe\u50cf\u3002\u5f53\u4f60\u505a\u771f\u6b63\u7684 NeRF \u65f6\uff0c\u518d\u628a\u5bc6\u5ea6/\u6df1\u5ea6\u5934\u52a0\u56de \u4f60\u81ea\u5df1\u7684\u6a21\u578b\u91cc\u3002</p>"},{"location":"zh/12_neural_workstation/#nerf","title":"\u4f4d\u7f6e\u7f16\u7801 \u2014\u2014 NeRF \u6838\u5fc3\u6280\u5de7","text":"<p>\u539f\u59cb <code>(x, y)</code> \u5750\u6807\u53ea\u80fd\u8868\u793a\u4f4e\u9891\u51fd\u6570\u3002\u4f4d\u7f6e\u7f16\u7801\u628a\u5b83\u4eec\u63d0\u5347\u5230\u9ad8\u7ef4\u7a7a\u95f4\uff1a</p> <pre><code>def positional_encoding(x, L):\n    if L == 0:\n        return x\n    freqs = 2.0 ** torch.arange(L, device=x.device)\n    xf = (x.unsqueeze(-1) * freqs * math.pi).reshape(*x.shape[:-1], -1)\n    return torch.cat([x, xf.sin(), xf.cos()], dim=-1)\n</code></pre> <p>\u5f53 $L = 6$ \u65f6\uff0c2D \u8f93\u5165\u53d8\u6210 $2 + 2 \\times 6 \\times 2 = 26$ \u7ef4\u3002 \u8fd9\u8ba9 MLP \u53ef\u4ee5\u5b66\u5230\u9510\u5229\u7684\u8fb9\u7f18\u548c\u7cbe\u7ec6\u7684\u7eb9\u7406\u3002PE Levels \u6ed1\u5757\u8ba9\u4f60 \u5b9e\u65f6\u770b\u5230\u533a\u522b\u2014\u2014\u8bbe\u4e3a 0\uff0c\u89c2\u5bdf\u9884\u6d4b\u53d8\u6a21\u7cca\u3002</p>"},{"location":"zh/12_neural_workstation/#_2","title":"\u4ece\u64cd\u4f5c\u7cfb\u7edf\u6253\u5f00\u56fe\u50cf","text":"<p>\u70b9\u51fb Open Image...\uff0c\u5f39\u51fa\u7cfb\u7edf\u539f\u751f\u6587\u4ef6\u5bf9\u8bdd\u6846\uff08\u901a\u8fc7 <code>tkinter.filedialog</code>\uff09\u3002\u5bf9\u8bdd\u6846\u5728\u540e\u53f0\u7ebf\u7a0b\u8fd0\u884c\uff0c\u6e32\u67d3\u5faa\u73af\u4e0d\u4f1a\u963b\u585e\uff1a</p> <pre><code>def open_file_dialog():\n    import tkinter as tk\n    from tkinter import filedialog\n    root = tk.Tk()\n    root.withdraw()\n    root.attributes(\"-topmost\", True)\n    path = filedialog.askopenfilename(\n        title=\"Select an image\",\n        filetypes=[(\"Images\", \"*.png *.jpg *.jpeg *.bmp *.tga *.hdr\"),\n                   (\"All files\", \"*.*\")])\n    root.destroy()\n    return path if path else None\n</code></pre> <p>\u5f53\u5bf9\u8bdd\u6846\u8fd4\u56de\u8def\u5f84\u65f6\uff0c\u4e3b\u5faa\u73af\u63a5\u6536\uff1a</p> <pre><code>if S[\"pending_image\"]:\n    gt, coords, target, H, W = load_image(S[\"pending_image\"], RES)\n    S[\"img_name\"] = Path(S[\"pending_image\"]).name\n    # \u91cd\u5efa\u663e\u793a tensor\uff0c\u91cd\u7f6e\u6a21\u578b...\n</code></pre> <p>\u8fd9\u4e2a\u6a21\u5f0f\u53ef\u4ee5\u5728\u4efb\u4f55 Vultorch demo \u4e2d\u590d\u7528\uff1a\u628a\u963b\u585e\u7684 OS \u8c03\u7528\u653e\u5728\u7ebf\u7a0b\u91cc\uff0c \u901a\u8fc7\u5171\u4eab\u53d8\u91cf\u4f20\u56de\u7ed3\u679c\u3002</p>"},{"location":"zh/12_neural_workstation/#_3","title":"\u5b9e\u65f6\u67b6\u6784\u8c03\u4f18","text":"<p>\u4e09\u4e2a\u6ed1\u5757\u63a7\u5236\u7f51\u7edc\u7ed3\u6784\uff1a</p> <pre><code>new_h  = ctrl.slider_int(\"Hidden\",    32, 256, default=128)\nnew_l  = ctrl.slider_int(\"Layers\",     2,   8, default=4)\nnew_pe = ctrl.slider_int(\"PE Levels\",  0,  10, default=6)\n</code></pre> <p>\u6539\u53d8\u4efb\u4f55\u6ed1\u5757\u4f1a\u8bbe\u7f6e <code>arch_dirty = True</code> \u5e76\u663e\u793a\u9ec4\u8272\u8b66\u544a\u3002\u7136\u540e\u70b9\u51fb Apply &amp; Reset \u91cd\u5efa\u6a21\u578b\uff1a</p> <pre><code>if S[\"arch_dirty\"]:\n    ctrl.text_colored(1, 0.8, 0, 1, \"  Architecture changed!\")\n    if ctrl.button(\"Apply &amp; Reset\", width=170):\n        model = make_model(S[\"hidden\"], S[\"layers\"], S[\"pe\"])\n        optimizer = make_optimizer(...)\n</code></pre> <p>\u8fd9\u79cd\u4e24\u6b65\u6a21\u5f0f\uff08\u6ed1\u5757 \u2192 \u786e\u8ba4\u6309\u94ae\uff09\u9632\u6b62\u4f60\u62d6\u52a8\u6ed1\u5757\u65f6\u6bcf\u5e27\u90fd\u91cd\u5efa\u6a21\u578b\u3002</p>"},{"location":"zh/12_neural_workstation/#_4","title":"\u53ef\u8c03\u589e\u76ca\u7684\u8bef\u5dee\u70ed\u529b\u56fe","text":"<pre><code>err = (gt - pr).abs().mean(dim=-1)       # \u9010\u50cf\u7d20 L1\nerr_t[:, :, :3] = apply_turbo(\n    (err * S[\"err_gain\"]).clamp_(0, 1))\n</code></pre> <p>Error Gain \u6ed1\u5757\uff081\u201320 \u500d\uff09\u653e\u5927\u7ec6\u5fae\u8bef\u5dee\u3002\u589e\u76ca\u4e3a 1 \u65f6\u5927\u90e8\u5206\u50cf\u7d20 \u770b\u8d77\u6765\u662f\u84dd\u8272\u7684\uff1b\u589e\u76ca\u4e3a 15 \u65f6\u4f60\u80fd\u7cbe\u786e\u770b\u5230\u6a21\u578b\u5728\u54ea\u91cc\u8fd8\u5728\u6323\u624e\u3002</p>"},{"location":"zh/12_neural_workstation/#_5","title":"\u6682\u505c\u3001\u5feb\u7167\u3001\u901f\u5ea6","text":"\u529f\u80fd \u539f\u7406 \u6682\u505c <code>if not S[\"paused\"]:</code> \u8df3\u8fc7\u8bad\u7ec3\u5faa\u73af\uff1b\u7a97\u53e3\u3001\u63a7\u4ef6\u3001\u663e\u793a\u7ee7\u7eed\u8fd0\u884c \u4fdd\u5b58\u5feb\u7167 <code>rgb_cv.save(\"snapshot_pred.png\")</code> \u901a\u8fc7 stb_image_write \u628a canvas \u7684 GPU tensor \u5199\u5230\u78c1\u76d8 \u901f\u5ea6\u8ba1\u6570\u5668 <code>(\u5f53\u524d\u8fed\u4ee3 - \u4e0a\u6b21\u8fed\u4ee3) / \u65f6\u95f4\u5dee</code> \u6bcf 0.5 \u79d2\u6d4b\u4e00\u6b21"},{"location":"zh/12_neural_workstation/#_6","title":"\u6307\u6807\u9762\u677f","text":"<pre><code>@met_pan.on_frame\ndef draw_met():\n    met_pan.text(f\"Loss: {S['loss']:.6f}   PSNR: {S['psnr']:.1f} dB   \"\n                 f\"Speed: {S['its_sec']:.0f} it/s\")\n    met_pan.separator()\n    if S[\"loss_h\"]:\n        met_pan.plot(S[\"loss_h\"], label=\"##loss\",\n                     overlay=f\"loss {S['loss']:.5f}\", height=70)\n    if S[\"psnr_h\"]:\n        met_pan.plot(S[\"psnr_h\"], label=\"##psnr\",\n                     overlay=f\"PSNR {S['psnr']:.1f} dB\", height=70)\n</code></pre> <p><code>panel.plot()</code> \u4ece Python \u5217\u8868\u6e32\u67d3\u8ff7\u4f60\u6298\u7ebf\u56fe\u3002\u4fdd\u7559\u6700\u8fd1 500 \u4e2a\u503c\uff0c \u7ed9\u4f60\u4e00\u4e2a\u6eda\u52a8\u7684\u5b9e\u65f6\u56fe\u8868\u2014\u2014\u5185\u7f6e\u5728\u8bad\u7ec3\u7a97\u53e3\u91cc\u7684 TensorBoard\u3002</p>"},{"location":"zh/12_neural_workstation/#_7","title":"\u5b8c\u6574\u4ee3\u7801","text":"examples/12_neural_workstation.py<pre><code>--8&lt;-- \"examples/12_neural_workstation.py\"\n</code></pre>"},{"location":"zh/12_neural_workstation/#_8","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<p>\u7528\u4e00\u4e2a Python \u6587\u4ef6\uff0c\u4f60\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u53ef\u53d1\u5e03\u7684 demo\uff1a</p> <ol> <li>\u4ece\u64cd\u4f5c\u7cfb\u7edf\u6253\u5f00\u4efb\u610f\u56fe\u50cf</li> <li>\u53ef\u8c03\u9891\u7387\u7ea7\u522b\u7684\u4f4d\u7f6e\u7f16\u7801</li> <li>\u5b9e\u65f6\u67b6\u6784\u8c03\u4f18 \u2014\u2014 \u6539\u53d8\u9690\u85cf\u5c42\u5927\u5c0f\u3001\u6df1\u5ea6\u3001PE \u7ea7\u522b</li> <li>\u4e09\u79cd\u635f\u5931\u51fd\u6570 \u2014\u2014 MSE\u3001L1\u3001Huber \u2014\u2014 \u8fd0\u884c\u65f6\u53ef\u5207\u6362</li> <li>\u4e09\u79cd\u4f18\u5316\u5668 \u2014\u2014 Adam\u3001SGD\u3001AdamW \u2014\u2014 \u70ed\u5207\u6362</li> <li>\u5e26 turbo \u8272\u56fe\u548c\u53ef\u8c03\u589e\u76ca\u7684\u8bef\u5dee\u70ed\u529b\u56fe</li> <li>\u6682\u505c/\u6062\u590d \u4e0d\u6740\u8fdb\u7a0b</li> <li>\u4fdd\u5b58\u5feb\u7167 \u2014\u2014 \u9884\u6d4b\u548c\u8bef\u5dee\u5bfc\u51fa\u4e3a PNG</li> <li>\u8bad\u7ec3\u901f\u5ea6\u8ba1\u6570\u5668\uff08\u8fed\u4ee3/\u79d2\uff09</li> <li>Loss &amp; PSNR \u66f2\u7ebf \u2014\u2014 \u6eda\u52a8\u5b9e\u65f6\u56fe\u8868</li> </ol> <p>\u6ca1\u6709 matplotlib\u3002\u6ca1\u6709 TensorBoard\u3002\u6ca1\u6709 Jupyter\u3002\u6ca1\u6709\u6d4f\u89c8\u5668\u3002 \u4e00\u4e2a\u7a97\u53e3\uff0c\u4e00\u4e2a\u811a\u672c\uff0c\u4e00\u5207\u4ee5 GPU \u901f\u5ea6\u540c\u6b65\u3002</p> <p>\u8fd9\u5c31\u662f\u300cVultorch = \u4f60\u7684\u795e\u7ecf\u6e32\u67d3 IDE\u300d\u7684\u6837\u5b50\u3002</p>"},{"location":"zh/12_neural_workstation/#_9","title":"\u6838\u5fc3\u8981\u70b9","text":"\u6982\u5ff5 \u4ee3\u7801 \u7528\u9014 \u4f4d\u7f6e\u7f16\u7801 <code>positional_encoding(x, L)</code> \u5085\u91cc\u53f6\u7279\u5f81\u5b66\u4e60\u9ad8\u9891\u7ec6\u8282 \u6587\u4ef6\u5bf9\u8bdd\u6846 \u7ebf\u7a0b\u4e2d\u7684 <code>tkinter.filedialog</code> \u4e0d\u963b\u585e\u5730\u52a0\u8f7d\u4efb\u610f\u56fe\u7247 \u67b6\u6784\u6ed1\u5757 <code>slider_int(\"Hidden\", ...)</code> \u5b9e\u65f6\u62d3\u6251\u8c03\u4f18 \u8bef\u5dee\u589e\u76ca <code>(err * gain).clamp_(0, 1)</code> \u653e\u5927\u7ec6\u5fae\u91cd\u5efa\u8bef\u5dee \u6682\u505c <code>checkbox(\"Pause Training\")</code> \u51bb\u7ed3\u8bad\u7ec3\uff0cUI \u4fdd\u6301\u6d3b\u8dc3 \u5feb\u7167 <code>canvas.save(\"file.png\")</code> \u628a GPU tensor \u5199\u5230\u78c1\u76d8 \u901f\u5ea6 <code>(it - it_last) / dt</code> \u8fed\u4ee3/\u79d2 \u8ba1\u6570\u5668 step()/end_step() \u8bad\u7ec3\u5faa\u73af\u62e5\u6709\u6e32\u67d3\u6743 \u4f60\u63a7\u5236\u5916\u5c42\u5faa\u73af <p>\u606d\u559c\uff01</p> <p>\u4f60\u5df2\u7ecf\u5b8c\u6210\u4e86\u5168\u90e8 12 \u4e2a Vultorch \u6559\u7a0b\u3002\u73b0\u5728\u4f60\u62e5\u6709\u4e86\u5c06\u5b9e\u65f6\u3001 GPU \u52a0\u901f\u7684\u53ef\u89c6\u5316\u96c6\u6210\u5230\u795e\u7ecf\u6e32\u67d3\u7814\u7a76\u5de5\u4f5c\u6d41\u4e2d\u6240\u9700\u7684\u4e00\u5207\u5de5\u5177\u2014\u2014 \u4ee5\u53ca\u53d1\u5e03\u7cbe\u7f8e\u4ea4\u4e92\u5f0f demo \u7684\u5b8c\u6574\u6a21\u677f\u3002</p>"},{"location":"zh/13_snake_rl/","title":"13 \u2014 \u8d2a\u5403\u86c7 RL","text":"<p>\u4e0d\u53ea\u662f\u795e\u7ecf\u6e32\u67d3</p> <p>Vultorch \u4e0d\u53ea\u80fd\u505a NeRF \u548c 3DGS\u3002\u4efb\u4f55\u4f60\u80fd\u5199\u8fdb <code>torch.Tensor</code> \u7684\u4e1c\u897f \u2014\u2014\u5f3a\u5316\u5b66\u4e60\u3001\u7269\u7406\u4eff\u771f\u3001\u4fe1\u53f7\u5904\u7406\u2014\u2014\u90fd\u80fd\u7528\u96f6\u62f7\u8d1d GPU \u5f20\u91cf\u4ee5 60 fps \u53ef\u89c6\u5316\u3002</p>"},{"location":"zh/13_snake_rl/#_1","title":"\u672c\u7ae0\u65b0\u670b\u53cb","text":"\u540d\u79f0 \u4f5c\u7528 \u7c7b\u6bd4 Snake \u73af\u5883 \u57fa\u4e8e\u683c\u5b50\u7684\u6e38\u620f\uff0c\u7eaf Python + deque OpenAI Gym\uff0c\u4f46\u53ea\u6709 50 \u884c DQN \u667a\u80fd\u4f53 3 \u5c42 MLP\uff0c\u628a\u89c2\u6d4b\u6620\u5c04\u5230 Q \u503c \u6700\u7b80\u5355\u7684\u6df1\u5ea6 RL \u7b97\u6cd5 \u7ecf\u9a8c\u56de\u653e \u5b58\u50a8\u8f6c\u6362\uff0c\u968f\u673a\u62bd\u6837\u6279\u6b21 \u4e00\u4e2a\u8ba9 RL \u7a33\u5b9a\u7684 <code>deque</code> \u03b5-\u8d2a\u5fc3 \u4ee5\u8870\u51cf\u6982\u7387\u968f\u673a\u884c\u52a8 \u63a2\u7d22 vs \u5229\u7528 Q \u503c\u70ed\u529b\u56fe \u989c\u8272\u7f16\u7801\u7684\u683c\u5b50\uff0c\u5c55\u793a\u667a\u80fd\u4f53\u7684\"\u601d\u8003\" \u7c7b\u4f3c\u6ce8\u610f\u529b\u56fe\uff0c\u4f46\u7528\u4e8e RL \u624b\u52a8\u6a21\u5f0f \u7528\u6309\u94ae\u4eb2\u81ea\u64cd\u63a7\u86c7 \u81ea\u5df1\u73a9\u4e00\u628a\u6765\u8c03\u8bd5\u73af\u5883"},{"location":"zh/13_snake_rl/#_2","title":"\u4e3a\u4ec0\u4e48\u9009\u8d2a\u5403\u86c7\uff1f","text":"<p>\u8d2a\u5403\u86c7\u662f\u5b8c\u7f8e\u7684 RL \u6f14\u793a\uff1a</p> <ul> <li>\u89c4\u5219\u7b80\u5355 \u2014\u2014\u5373\u4f7f\u975e ML \u4eba\u58eb\u4e5f\u80fd\u79d2\u61c2</li> <li>\u89c6\u89c9\u53cd\u9988 \u2014\u2014\u4f60\u80fd\u5728 32\u00d716 \u7684\u68cb\u76d8\u4e0a \u770b\u5230 \u667a\u80fd\u4f53\u53d8\u806a\u660e</li> <li>\u8bad\u7ec3\u5feb\u901f \u2014\u2014DQN \u5927\u7ea6 2000 \u5c40\u5c31\u80fd\u5b66\u4f1a\u57fa\u672c\u89c5\u98df</li> <li>\u4ee3\u7801\u7d27\u51d1 \u2014\u2014\u73af\u5883 + \u667a\u80fd\u4f53 + \u8bad\u7ec3 + \u53ef\u89c6\u5316\uff0c\u4e00\u4e2a\u6587\u4ef6\u641e\u5b9a</li> </ul> <p>\u800c\u4e14\u5b83\u8bc1\u660e\u4e86\u4e00\u4e2a\u91cd\u8981\u89c2\u70b9\uff1aVultorch \u662f\u901a\u7528\u7684 GPU \u53ef\u89c6\u5316\u5de5\u5177\uff0c \u4e0d\u53ea\u662f\u795e\u7ecf\u6e32\u67d3\u5e93\u3002</p>"},{"location":"zh/13_snake_rl/#50-python","title":"\u73af\u5883\u2014\u201450 \u884c\u7eaf Python","text":"<pre><code>class SnakeEnv:\n    def reset(self):\n        self.snake = deque([(ROWS // 2, COLS // 2)])\n        self.dir = RIGHT\n        self._place_food()\n        ...\n        return self._obs()\n\n    def step(self, action):\n        # action: 0=\u76f4\u8d70, 1=\u5de6\u8f6c, 2=\u53f3\u8f6c\n        ...\n        return obs, reward, done\n</code></pre> <p>\u89c2\u6d4b\u662f\u4e00\u4e2a 11 \u7ef4\u5411\u91cf\uff1a</p> \u7ef4\u5ea6 \u542b\u4e49 0\u20132 \u524d\u65b9 / \u5de6\u65b9 / \u53f3\u65b9 \u662f\u5426\u6709\u5371\u9669\uff08\u5899\u58c1\u6216\u8eab\u4f53\uff09 3\u20136 \u5f53\u524d\u65b9\u5411\uff08one-hot \u7f16\u7801\uff09 7\u201310 \u98df\u7269\u65b9\u5411\uff08\u4e0a / \u53f3 / \u4e0b / \u5de6\uff09 <p>\u8fd9\u5bf9\u4e00\u4e2a\u5c0f MLP \u6765\u8bf4\u5df2\u7ecf\u8db3\u591f\u4e86\u3002\u4e0d\u9700\u8981\u50cf\u7d20\u8f93\u5165\uff0c\u4e0d\u9700\u8981 CNN\u2014\u2014 \u53ea\u8981\u6700\u57fa\u672c\u7684\u7a7a\u95f4\u5173\u7cfb\u3002</p>"},{"location":"zh/13_snake_rl/#dqn-3-pytorch","title":"DQN \u667a\u80fd\u4f53\u2014\u20143 \u884c PyTorch","text":"<pre><code>class DQN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(11, 128), nn.ReLU(True),\n            nn.Linear(128, 128), nn.ReLU(True),\n            nn.Linear(128, 3),  # \u76f4\u8d70, \u5de6\u8f6c, \u53f3\u8f6c\n        )\n\n    def forward(self, x):\n        return self.net(x)\n</code></pre> <p>\u4e09\u4e2a\u52a8\u4f5c\uff08\u76f4\u8d70 / \u5de6\u8f6c / \u53f3\u8f6c\uff09\u800c\u4e0d\u662f\u56db\u4e2a\uff08\u4e0a\u4e0b\u5de6\u53f3\uff09\u2014\u2014\u8fd9\u8ba9\u5b66\u4e60 \u7b80\u5355\u5f97\u591a\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u4e0d\u9700\u8981\u5b66\u4e60\"\u4e0d\u8981\u5012\u9000\u54ac\u81ea\u5df1\"\u3002</p>"},{"location":"zh/13_snake_rl/#_3","title":"\u7ecf\u9a8c\u56de\u653e\u4e0e\u8bad\u7ec3","text":"<pre><code>class ReplayBuffer:\n    def __init__(self, cap=50000):\n        self.buf = deque(maxlen=cap)\n\n    def push(self, s, a, r, s2, done):\n        self.buf.append((s, a, r, s2, done))\n\n    def sample(self, n):\n        batch = random.sample(self.buf, min(n, len(self.buf)))\n        ...\n</code></pre> <p>DQN \u8bad\u7ec3\u6b65\u9aa4\u662f\u6559\u79d1\u4e66\u5f0f\u7684\uff1a</p> <pre><code>def train_dqn():\n    s, a, r, s2, d = buf.sample(BATCH)\n    with torch.no_grad():\n        q2 = target(s2).max(dim=1).values\n        y = r + GAMMA * q2 * (1 - d)\n    q = policy(s).gather(1, a.unsqueeze(1)).squeeze(1)\n    loss = F.smooth_l1_loss(q, y)\n    opt.zero_grad(); loss.backward(); opt.step()\n</code></pre> <p>\u76ee\u6807\u7f51\u7edc\uff08\u6bcf 500 \u6b65\u66f4\u65b0\u4e00\u6b21\uff09\u7a33\u5b9a\u5b66\u4e60\u8fc7\u7a0b\u3002 \u03b5 \u4ece 1.0 \u8870\u51cf\u5230 0.01\u2014\u2014\u667a\u80fd\u4f53\u4ece\u5b8c\u5168\u968f\u673a\u5f00\u59cb\uff0c \u9010\u6e10\u8f6c\u5411\u5229\u7528\u5b66\u5230\u7684\u77e5\u8bc6\u3002</p>"},{"location":"zh/13_snake_rl/#_4","title":"\u6e32\u67d3\u6e38\u620f\u9762\u677f","text":"<pre><code>def render_game():\n    board = torch.zeros(ROWS, COLS, 3)\n    board[:, :] = torch.tensor([0.05, 0.05, 0.08])  # \u6697\u8272\u80cc\u666f\n\n    # \u86c7\u8eab\uff08\u4ece\u4eae\u5230\u6697\u7684\u7eff\u8272\u6e10\u53d8\uff09\n    for i, (r, c) in enumerate(env.snake):\n        if i == 0:\n            board[r, c] = torch.tensor([1.0, 0.9, 0.0])  # \u86c7\u5934\uff1a\u9ec4\u8272\n        else:\n            t = 1.0 - i / max(len(env.snake), 1) * 0.5\n            board[r, c] = torch.tensor([0.0, t, 0.2])\n\n    # \u98df\u7269\uff1a\u7ea2\u8272\n    board[fr, fc] = torch.tensor([1.0, 0.15, 0.15])\n\n    game_t.copy_(cpu_rgba.to(disp_dev))\n</code></pre> <p>\u7f51\u683c\u662f 32\u00d716\uff082:1 \u6a2a\u5c4f\u6bd4\u4f8b\uff09\u3002\u753b\u5e03\u4e0a\u7684 <code>filter=\"nearest\"</code> \u8bbe\u7f6e\u9632\u6b62\u6a21\u7cca\u2014\u2014\u6bcf\u4e2a\u683c\u5b50\u90fd\u662f\u6e05\u6670\u7684\u50cf\u7d20\uff0c \u548c Conway \u751f\u547d\u6e38\u620f\u7684\u4f8b\u5b50\u5982\u51fa\u4e00\u8f99\u3002</p>"},{"location":"zh/13_snake_rl/#q","title":"Q \u503c\u70ed\u529b\u56fe","text":"<p>Q \u503c\u9762\u677f\u53ef\u89c6\u5316\u4e86\u667a\u80fd\u4f53\u7684\u51b3\u7b56\uff1a</p> <ul> <li>\u7eff\u8272 = \u667a\u80fd\u4f53\u60f3\u76f4\u8d70</li> <li>\u84dd\u8272 = \u667a\u80fd\u4f53\u60f3\u5de6\u8f6c</li> <li>\u6a59\u8272 = \u667a\u80fd\u4f53\u60f3\u53f3\u8f6c</li> <li>\u5de6\u4e0a\u89d2 3 \u4e2a\u683c\u5b50\u663e\u793a\u6bcf\u4e2a\u52a8\u4f5c\u7684 Q \u503c\u5927\u5c0f</li> </ul> <p>\u8fd9\u5728\u529f\u80fd\u4e0a\u7c7b\u4f3c\u4e8e Transformer \u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u2014\u2014\u5b83\u5c55\u793a\u7684\u662f \u6a21\u578b\u5728\u60f3\u4ec0\u4e48\uff0c\u800c\u4e0d\u53ea\u662f\u5b83\u505a\u4e86\u4ec0\u4e48\u3002</p>"},{"location":"zh/13_snake_rl/#_5","title":"\u624b\u52a8\u6a21\u5f0f","text":"<p>\u52fe\u9009 Manual Mode \u590d\u9009\u6846\uff0c\u4e09\u4e2a\u6309\u94ae\u5c31\u4f1a\u51fa\u73b0\uff1aLeft\u3001Fwd\u3001Right\u3002 \u73b0\u5728 \u4f60 \u6765\u64cd\u63a7\u8d2a\u5403\u86c7\u3002\u8fd9\u5bf9\u8c03\u8bd5 RL \u73af\u5883\u975e\u5e38\u6709\u7528\u2014\u2014\u5982\u679c\u4f60\u81ea\u5df1\u90fd \u89e3\u4e0d\u4e86\u8fd9\u4e2a\u6e38\u620f\uff0c\u667a\u80fd\u4f53\u5927\u6982\u4e5f\u4e0d\u884c\u3002</p>"},{"location":"zh/13_snake_rl/#_6","title":"\u8bad\u7ec3\u5faa\u73af","text":"<pre><code>while view.step():\n    for _ in range(S[\"speed\"]):\n        # \u03b5-\u8d2a\u5fc3\u52a8\u4f5c\u9009\u62e9\n        if random.random() &lt; S[\"eps\"]:\n            a = random.randint(0, 2)\n        else:\n            a = policy(obs).argmax().item()\n\n        next_obs, reward, done = env.step(a)\n        buf.push(obs, a, reward, next_obs, float(done))\n        train_dqn()\n\n        if done:\n            obs = env.reset()\n\n    render_game()\n    render_qvalues()\n    view.end_step()\n</code></pre> <p>Steps/Frame \u6ed1\u5757\u63a7\u5236\u6bcf\u5e27\u6267\u884c\u591a\u5c11\u4e2a\u73af\u5883\u6b65\u9aa4\u3002 \u8c03\u5230 50 \u52a0\u901f\u8bad\u7ec3\uff1b\u8bbe\u4e3a 1 \u6162\u52a8\u4f5c\u89c2\u5bdf\u6bcf\u4e00\u6b65\u3002</p>"},{"location":"zh/13_snake_rl/#_7","title":"\u5b8c\u6574\u4ee3\u7801","text":"examples/13_snake_rl.py<pre><code>--8&lt;-- \"examples/13_snake_rl.py\"\n</code></pre>"},{"location":"zh/13_snake_rl/#_8","title":"\u521a\u624d\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f","text":"<p>\u5728\u4e00\u4e2a Python \u6587\u4ef6\u91cc\u4f60\u6784\u5efa\u4e86\uff1a</p> <ol> <li>\u4e00\u4e2a\u5e26 \u5956\u52b1\u5851\u5f62\u7684\u8d2a\u5403\u86c7\u6e38\u620f\u73af\u5883</li> <li>\u4e00\u4e2a\u5e26\u7ecf\u9a8c\u56de\u653e\u548c\u76ee\u6807\u7f51\u7edc\u7684 DQN \u667a\u80fd\u4f53</li> <li>\u5b9e\u65f6\u53ef\u89c6\u5316 \u2014\u2014\u6e38\u620f\u9762\u677f + Q \u503c\u70ed\u529b\u56fe + \u5956\u52b1\u66f2\u7ebf</li> <li>\u624b\u52a8\u6a21\u5f0f \u2014\u2014\u4eb2\u81ea\u73a9\u6e38\u620f\u6765\u8c03\u8bd5\u73af\u5883</li> <li>\u03b5-\u8d2a\u5fc3\u63a2\u7d22 \u5e76\u5b9e\u65f6\u663e\u793a epsilon \u503c</li> </ol> <p>\u5168\u90e8\u901a\u8fc7 Vultorch \u7684\u96f6\u62f7\u8d1d\u5f20\u91cf\u663e\u793a\u4ee5 60 fps \u6e32\u67d3\u3002\u4e0d\u9700\u8981 \u5355\u72ec\u7684 Gym \u6e32\u67d3\u5668\uff0c\u4e0d\u9700\u8981 matplotlib\uff0c\u4e0d\u9700\u8981\u65e5\u5fd7\u5199\u76d8\u3002</p> <p>\u8981\u70b9\uff1a\u53ea\u8981\u4f60\u7684\u6570\u636e\u5728\u5f20\u91cf\u91cc\uff0cVultorch \u5c31\u80fd\u53ef\u89c6\u5316\u2014\u2014 \u65e0\u8bba\u662f\u795e\u7ecf\u8f90\u5c04\u573a\u3001\u9ad8\u65af\u6e85\u5c04\u3001\u7ec6\u80de\u81ea\u52a8\u673a\uff0c\u8fd8\u662f\u4e00\u6761\u5b66\u4e60\u89c5\u98df\u7684\u86c7\u3002</p>"},{"location":"zh/13_snake_rl/#_9","title":"\u5173\u952e\u6536\u83b7","text":"\u6982\u5ff5 \u4ee3\u7801 \u7528\u9014 Snake \u73af\u5883 <code>SnakeEnv</code> \u7c7b 32\u00d716 \u7f51\u683c RL \u73af\u5883 DQN <code>nn.Sequential(11 \u2192 128 \u2192 128 \u2192 3)</code> \u6df1\u5ea6 Q \u7f51\u7edc \u7ecf\u9a8c\u56de\u653e <code>deque(maxlen=50000)</code> \u7a33\u5b9a\u7684\u79bb\u7b56\u7565\u5b66\u4e60 \u03b5-\u8d2a\u5fc3 <code>random() &lt; eps</code> \u63a2\u7d22 vs \u5229\u7528 \u6700\u8fd1\u90bb\u8fc7\u6ee4 <code>filter=\"nearest\"</code> \u50cf\u7d20\u7ea7\u683c\u5b50\u663e\u793a Q \u503c\u70ed\u529b\u56fe \u989c\u8272\u7f16\u7801\u7684\u667a\u80fd\u4f53\u51b3\u7b56 \u89c6\u89c9\u8c03\u8bd5 \u624b\u52a8\u6a21\u5f0f <code>checkbox(\"Manual Mode\")</code> \u8c03\u8bd5\u73af\u5883 step()/end_step() \u8bad\u7ec3\u5faa\u73af\u63a7\u5236\u6e32\u67d3 RL \u5faa\u73af\u638c\u63a7\u8282\u594f"},{"location":"zh/api/","title":"API \u53c2\u8003\u6587\u6863","text":"<p><code>vultorch</code> \u5305\u6240\u6709\u516c\u5f00\u7c7b\u548c\u51fd\u6570\u7684\u5b8c\u6574\u53c2\u8003\u3002</p>"},{"location":"zh/api/#_1","title":"\u6a21\u5757\u7ea7\u5c5e\u6027","text":""},{"location":"zh/api/#vultorch__version__","title":"<code>vultorch.__version__</code>","text":"<pre><code>__version__: str\n</code></pre> <p>\u5305\u7248\u672c\u5b57\u7b26\u4e32\uff08\u4f8b\u5982 <code>\"0.5.0\"</code>\uff09\u3002</p>"},{"location":"zh/api/#vultorchhas_cuda","title":"<code>vultorch.HAS_CUDA</code>","text":"<pre><code>HAS_CUDA: bool\n</code></pre> <p>\u5982\u679c\u539f\u751f\u6269\u5c55\u6a21\u5757\u7f16\u8bd1\u65f6\u542f\u7528\u4e86 CUDA \u5219\u4e3a <code>True</code>\u3002\u4e3a <code>False</code> \u65f6\uff0c\u6240\u6709\u5f20\u91cf\u663e\u793a\u5c06\u56de\u9000\u5230 CPU \u6682\u5b58\u7f13\u51b2\u533a\uff08host-visible <code>memcpy</code>\uff09\u3002</p>"},{"location":"zh/api/#_2","title":"\u6838\u5fc3\u51fd\u6570","text":""},{"location":"zh/api/#vultorchshow","title":"<code>vultorch.show()</code>","text":"<pre><code>def show(\n    tensor: torch.Tensor,\n    *,\n    name: str = \"tensor\",\n    width: float = 0,\n    height: float = 0,\n    filter: str = \"linear\",\n    window: Window | None = None,\n) -&gt; None\n</code></pre> <p>\u5728\u5f53\u524d ImGui \u4e0a\u4e0b\u6587\u4e2d\u663e\u793a\u5f20\u91cf\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>tensor</code> <code>torch.Tensor</code> (\u5fc5\u9700) CUDA \u6216 CPU \u5f20\u91cf\u3002dtype\uff1a<code>float32</code>\u3001<code>float16</code> \u6216 <code>uint8</code>\u3002\u5f62\u72b6\uff1a<code>(H, W)</code> \u6216 <code>(H, W, C)</code>\uff0cC \u2208 {1, 3, 4}\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u552f\u4e00\u6807\u7b7e\uff0c\u7528\u4e8e\u540c\u65f6\u663e\u793a\u591a\u4e2a\u5f20\u91cf\u65f6\u7684\u7f13\u5b58\u3002 <code>width</code> <code>float</code> <code>0</code> \u663e\u793a\u5bbd\u5ea6\uff08\u50cf\u7d20\uff09\u3002<code>0</code> = \u81ea\u52a8\u9002\u5e94\u5f20\u91cf\u5927\u5c0f\u3002 <code>height</code> <code>float</code> <code>0</code> \u663e\u793a\u9ad8\u5ea6\uff08\u50cf\u7d20\uff09\u3002<code>0</code> = \u81ea\u52a8\u9002\u5e94\u5f20\u91cf\u5927\u5c0f\u3002 <code>filter</code> <code>str</code> <code>\"linear\"</code> \u91c7\u6837\u6ee4\u6ce2\u5668\uff1a<code>\"nearest\"</code> \u6216 <code>\"linear\"</code>\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\u3002\u9ed8\u8ba4\u4f7f\u7528 <code>Window._current</code>\u3002 <p>\u884c\u4e3a\uff1a</p> <ul> <li>1 \u901a\u9053\u548c 3 \u901a\u9053\u5f20\u91cf\u4f1a\u81ea\u52a8\u6269\u5c55\u4e3a RGBA\u3002</li> <li>RGBA \u6269\u5c55\u7f13\u51b2\u533a\u6309 <code>name</code> \u7f13\u5b58\uff0c\u907f\u514d\u6bcf\u5e27\u5206\u914d\u5185\u5b58\u3002</li> <li>CUDA\uff1a\u4f7f\u7528\u96f6\u62f7\u8d1d GPU\u2192GPU \u8def\u5f84\u3002CPU\uff1a\u4f7f\u7528 host-visible \u6682\u5b58\u7f13\u51b2\u533a\u3002</li> <li><code>uint8</code> \u5f20\u91cf\u9664\u4ee5 255\uff1b<code>float16</code> \u5f20\u91cf\u8f6c\u6362\u4e3a <code>float32</code>\u3002</li> </ul> <p>\u5f02\u5e38\uff1a \u5982\u679c\u6ca1\u6709\u6d3b\u8dc3\u7684 <code>Window</code> \u5219\u629b\u51fa <code>RuntimeError</code>\u3002</p>"},{"location":"zh/api/#vultorchcreate_tensor","title":"<code>vultorch.create_tensor()</code>","text":"<pre><code>def create_tensor(\n    height: int,\n    width: int,\n    channels: int = 4,\n    device: str = \"cuda:0\",\n    *,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>\u5206\u914d Vulkan \u5171\u4eab\u7684 CUDA \u5f20\u91cf\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u62f7\u8d1d\u663e\u793a\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>height</code> <code>int</code> (\u5fc5\u9700) \u5f20\u91cf\u9ad8\u5ea6\uff08\u50cf\u7d20\uff09\u3002 <code>width</code> <code>int</code> (\u5fc5\u9700) \u5f20\u91cf\u5bbd\u5ea6\uff08\u50cf\u7d20\uff09\u3002 <code>channels</code> <code>int</code> <code>4</code> \u901a\u9053\u6570\uff1a1\u30013 \u6216 4\u3002 <code>device</code> <code>str</code> <code>\"cuda:0\"</code> CUDA \u8bbe\u5907\u5b57\u7b26\u4e32\uff0c\u6216 <code>\"cpu\"</code>\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u7eb9\u7406\u69fd\u540d\u79f0\uff08\u5fc5\u987b\u4e0e <code>show(..., name=...)</code> \u5339\u914d\uff09\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\u3002\u9ed8\u8ba4\u4f7f\u7528 <code>Window._current</code>\u3002 <p>\u8fd4\u56de\uff1a \u5f62\u72b6\u4e3a <code>(height, width, channels)</code> \u7684 <code>torch.Tensor</code>\u3002</p> <p>Note</p> <p>\u53ea\u6709 <code>channels=4</code> \u624d\u80fd\u901a\u8fc7 Vulkan \u5916\u90e8\u5185\u5b58\u5b9e\u73b0\u771f\u6b63\u7684\u96f6\u62f7\u8d1d\u3002\u5bf9\u4e8e 1 \u6216 3 \u901a\u9053\uff0c\u8fd4\u56de\u666e\u901a CUDA \u5f20\u91cf\uff0c<code>show()</code> \u4f1a\u5904\u7406 RGBA \u6269\u5c55\u5e76\u8fdb\u884c GPU\u2192GPU \u62f7\u8d1d\u3002</p> <p>\u5f02\u5e38\uff1a \u5982\u679c\u6ca1\u6709\u6d3b\u8dc3\u7684 <code>Window</code> \u5219\u629b\u51fa <code>RuntimeError</code>\u3002</p>"},{"location":"zh/api/#vultorchimread","title":"<code>vultorch.imread()</code>","text":"<pre><code>def imread(\n    path: str,\n    *,\n    channels: int = 4,\n    size: tuple[int, int] | None = None,\n    device: str = \"cuda\",\n    shared: bool = False,\n    name: str = \"tensor\",\n    window: Window | None = None,\n) -&gt; torch.Tensor\n</code></pre> <p>\u5c06\u56fe\u7247\u6587\u4ef6\u52a0\u8f7d\u4e3a <code>float32</code> \u5f20\u91cf\u3002\u5185\u90e8\u4f7f\u7528 stb_image \u2014\u2014 \u4e0d\u9700\u8981 PIL \u6216 numpy\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>path</code> <code>str</code> \uff08\u5fc5\u9700\uff09 \u6587\u4ef6\u8def\u5f84\uff08PNG\u3001JPG\u3001BMP\u3001TGA\u3001HDR \u7b49\uff09\u3002 <code>channels</code> <code>int</code> <code>4</code> \u671f\u671b\u901a\u9053\u6570\uff1a1\uff08\u7070\u5ea6\uff09\u30013\uff08RGB\uff09\u62164\uff08RGBA\uff09\u3002 <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> \u53ef\u9009 <code>(\u9ad8\u5ea6, \u5bbd\u5ea6)</code>\uff0c\u4f7f\u7528\u53cc\u7ebf\u6027\u63d2\u503c\u7f29\u653e\u3002 <code>device</code> <code>str</code> <code>\"cuda\"</code> \u76ee\u6807\u8bbe\u5907\uff08<code>\"cuda\"</code> \u6216 <code>\"cpu\"</code>\uff09\u3002 <code>shared</code> <code>bool</code> <code>False</code> \u4e3a <code>True</code> \u65f6\u901a\u8fc7 <code>create_tensor</code> \u5206\u914d\uff0c\u5b9e\u73b0\u96f6\u62f7\u8d1d\u663e\u793a\u3002 <code>name</code> <code>str</code> <code>\"tensor\"</code> \u7eb9\u7406\u69fd\u540d\u79f0\uff08\u4ec5\u5728 <code>shared=True</code> \u65f6\u4f7f\u7528\uff09\u3002 <code>window</code> <code>Window \\| None</code> <code>None</code> \u76ee\u6807\u7a97\u53e3\uff08\u4ec5\u5728 <code>shared=True</code> \u65f6\u4f7f\u7528\uff09\u3002 <p>\u8fd4\u56de\uff1a \u5f62\u72b6\u4e3a <code>(H, W, C)</code> \u7684 <code>torch.Tensor</code>\uff0c\u503c\u8303\u56f4 <code>[0, 1]</code>\u3002</p> <p>\u793a\u4f8b\uff1a</p> <pre><code>import vultorch\ngt = vultorch.imread(\"photo.png\", channels=3, device=\"cuda\")\n</code></pre>"},{"location":"zh/api/#vultorchimwrite","title":"<code>vultorch.imwrite()</code>","text":"<pre><code>def imwrite(\n    path: str,\n    tensor: torch.Tensor,\n    *,\n    channels: int = 0,\n    size: tuple[int, int] | None = None,\n    quality: int = 95,\n) -&gt; None\n</code></pre> <p>\u5c06\u5f20\u91cf\u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6\u3002\u683c\u5f0f\u7531\u6269\u5c55\u540d\u63a8\u65ad\u3002</p> <p>\u53c2\u6570\uff1a</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>path</code> <code>str</code> \uff08\u5fc5\u9700\uff09 \u8f93\u51fa\u6587\u4ef6\u8def\u5f84\u3002\u6269\u5c55\u540d\u9009\u62e9\u683c\u5f0f\uff1a<code>.png</code>\u3001<code>.jpg</code>\u3001<code>.bmp</code>\u3001<code>.tga</code>\u3001<code>.hdr</code>\u3002 <code>tensor</code> <code>torch.Tensor</code> \uff08\u5fc5\u9700\uff09 <code>(H, W)</code>\u3001<code>(H, W, 1)</code>\u3001<code>(H, W, 3)</code> \u6216 <code>(H, W, 4)</code> \u5f20\u91cf\u3002 <code>channels</code> <code>int</code> <code>0</code> \u8986\u76d6\u8f93\u51fa\u901a\u9053\u6570\u3002<code>0</code> = \u4f7f\u7528\u5f20\u91cf\u81ea\u8eab\u7684\u901a\u9053\u6570\u3002 <code>size</code> <code>tuple[int, int] \\| None</code> <code>None</code> \u53ef\u9009 <code>(\u9ad8\u5ea6, \u5bbd\u5ea6)</code>\uff0c\u4fdd\u5b58\u524d\u7f29\u653e\u3002 <code>quality</code> <code>int</code> <code>95</code> JPEG \u8d28\u91cf\uff081\u2013100\uff09\u3002\u5176\u4ed6\u683c\u5f0f\u5ffd\u7565\u6b64\u53c2\u6570\u3002 <p>\u884c\u4e3a\uff1a</p> <ul> <li><code>.hdr</code> \u5199\u5165 32 \u4f4d\u6d6e\u70b9\u6570\u636e\uff1b\u5176\u4ed6\u683c\u5f0f\u91cf\u5316\u4e3a 8 \u4f4d\u3002</li> <li>\u5982\u679c\u5f20\u91cf\u901a\u9053\u6570\u591a\u4e8e <code>channels</code>\uff0c\u591a\u4f59\u901a\u9053\u88ab\u4e22\u5f03\u3002\u5982\u679c\u5c11\u4e8e\uff0c\u7f3a\u5c11\u7684\u901a\u9053\u4f1a\u88ab\u586b\u5145\uff08alpha \u2192 1.0\uff09\u3002</li> <li>\u5f20\u91cf\u4f1a\u88ab\u79fb\u5230 CPU \u5e76\u8f6c\u6362\u4e3a <code>float32</code> \u540e\u518d\u5199\u5165\u3002</li> </ul> <p>\u793a\u4f8b\uff1a</p> <pre><code>vultorch.imwrite(\"output.png\", pred_tensor, channels=3)\nvultorch.imwrite(\"output.jpg\", pred_tensor, quality=90)\n</code></pre>"},{"location":"zh/api/#_3","title":"\u7c7b","text":""},{"location":"zh/api/#vultorchwindow","title":"<code>vultorch.Window</code>","text":"<pre><code>class Window:\n    _current: Window | None   # \u5355\u4f8b\u5f15\u7528\n\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>Vulkan + SDL3 + ImGui \u5f15\u64ce\u7684\u9ad8\u5c42\u5c01\u88c5\u3002\u521b\u5efa <code>Window</code> \u4f1a\u81ea\u52a8\u5c06\u5176\u8bbe\u7f6e\u4e3a <code>show()</code> \u548c <code>create_tensor()</code> \u7684\u5f53\u524d\u76ee\u6807\u3002</p>"},{"location":"zh/api/#_4","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>poll()</code> <code>\u2192 bool</code> \u5904\u7406\u64cd\u4f5c\u7cfb\u7edf\u4e8b\u4ef6\u3002\u7a97\u53e3\u5e94\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>begin_frame()</code> <code>\u2192 bool</code> \u5f00\u59cb\u65b0\u7684 ImGui \u5e27\u3002\u5e27\u88ab\u8df3\u8fc7\uff08\u6700\u5c0f\u5316\uff09\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>end_frame()</code> <code>\u2192 None</code> \u63d0\u4ea4\u5e27\u5230 GPU \u5e76\u5448\u73b0\u3002 <code>activate()</code> <code>\u2192 None</code> \u5c06\u6b64\u7a97\u53e3\u8bbe\u4e3a\u6a21\u5757\u7ea7\u8f85\u52a9\u51fd\u6570\u7684\u5f53\u524d\u76ee\u6807\u3002 <code>upload_tensor(tensor, *, name)</code> <code>\u2192 None</code> \u4e0a\u4f20\u5f20\u91cf\u7528\u4e8e\u663e\u793a\uff08CUDA \u6216 CPU\uff09\u3002 <code>get_texture_id(name)</code> <code>\u2192 int</code> \u6307\u5b9a\u540d\u79f0\u5f20\u91cf\u7684 ImGui \u7eb9\u7406 ID\u3002 <code>get_texture_size(name)</code> <code>\u2192 (int, int)</code> \u6307\u5b9a\u540d\u79f0\u5f20\u91cf\u7684 <code>(\u5bbd\u5ea6, \u9ad8\u5ea6)</code>\u3002 <code>destroy()</code> <code>\u2192 None</code> \u91ca\u653e\u6240\u6709 GPU / \u7a97\u53e3\u8d44\u6e90\u3002\u53ef\u5b89\u5168\u591a\u6b21\u8c03\u7528\u3002"},{"location":"zh/api/#_5","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 <code>tensor_texture_id</code> <code>int</code> \u9ed8\u8ba4 <code>\"tensor\"</code> \u69fd\u7684 ImGui \u7eb9\u7406 ID\u3002 <code>tensor_size</code> <code>(int, int)</code> \u9ed8\u8ba4 <code>\"tensor\"</code> \u69fd\u7684 <code>(\u5bbd\u5ea6, \u9ad8\u5ea6)</code>\u3002"},{"location":"zh/api/#_6","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>import vultorch\nfrom vultorch import ui\n\nwin = vultorch.Window(\"Demo\", 1280, 720)\nwhile win.poll():\n    if not win.begin_frame():\n        continue\n    ui.begin(\"Panel\", True, 0)\n    vultorch.show(tensor)\n    ui.end()\n    win.end_frame()\nwin.destroy()\n</code></pre>"},{"location":"zh/api/#vultorchcamera","title":"<code>vultorch.Camera</code>","text":"<pre><code>class Camera:\n    azimuth: float     # \u6c34\u5e73\u89d2\u5ea6\uff08\u5f27\u5ea6\uff09\uff0c\u9ed8\u8ba4 0.0\n    elevation: float   # \u5782\u76f4\u89d2\u5ea6\uff08\u5f27\u5ea6\uff09\uff0c\u9ed8\u8ba4 0.6\n    distance: float    # \u5230\u76ee\u6807\u7684\u8ddd\u79bb\uff0c\u9ed8\u8ba4 3.0\n    target: tuple      # (x, y, z) \u6ce8\u89c6\u70b9\uff0c\u9ed8\u8ba4 (0, 0, 0)\n    fov: float         # \u89c6\u573a\u89d2\uff08\u5ea6\uff09\uff0c\u9ed8\u8ba4 45.0\n</code></pre> <p><code>SceneView</code> \u4f7f\u7528\u7684\u8f68\u9053\u76f8\u673a\u53c2\u6570\u3002\u8c03\u7528 <code>reset()</code> \u6062\u590d\u9ed8\u8ba4\u503c\u3002</p>"},{"location":"zh/api/#vultorchlight","title":"<code>vultorch.Light</code>","text":"<pre><code>class Light:\n    direction: tuple   # (x, y, z)\uff0c\u9ed8\u8ba4 (0.3, -1.0, 0.5)\n    color: tuple       # (r, g, b)\uff0c\u9ed8\u8ba4 (1, 1, 1)\n    intensity: float   # \u9ed8\u8ba4 1.0\n    ambient: float     # \u73af\u5883\u5149\u9879\uff0c\u9ed8\u8ba4 0.15\n    specular: float    # \u9ad8\u5149\u9879\uff0c\u9ed8\u8ba4 0.5\n    shininess: float   # Blinn-Phong \u6307\u6570\uff0c\u9ed8\u8ba4 32.0\n    enabled: bool      # \u9ed8\u8ba4 True\n</code></pre> <p><code>SceneView</code> \u4f7f\u7528\u7684 Blinn-Phong \u65b9\u5411\u5149\u53c2\u6570\u3002</p>"},{"location":"zh/api/#vultorchsceneview","title":"<code>vultorch.SceneView</code>","text":"<pre><code>class SceneView:\n    def __init__(self, name: str = \"SceneView\",\n                 width: int = 800, height: int = 600,\n                 msaa: int = 4) -&gt; None: ...\n</code></pre> <p>3D \u5f20\u91cf\u67e5\u770b\u5668 \u2014 \u5728\u5e26\u5149\u7167\u7684\u5e73\u9762\u4e0a\u6e32\u67d3\u5f20\u91cf\uff0c\u652f\u6301\u8f68\u9053\u76f8\u673a\u548c MSAA\u3002</p>"},{"location":"zh/api/#_7","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>name</code> <code>str</code> <code>\"SceneView\"</code> ImGui \u7a97\u53e3\u6807\u7b7e\u3002 <code>camera</code> <code>Camera</code> (\u81ea\u52a8) \u8f68\u9053\u76f8\u673a\uff08\u62d6\u62fd\u65cb\u8f6c\uff09\u3002 <code>light</code> <code>Light</code> (\u81ea\u52a8) \u65b9\u5411\u5149\u6e90\u3002 <code>background</code> <code>tuple</code> <code>(0.12, 0.12, 0.14)</code> \u80cc\u666f\u989c\u8272 <code>(r, g, b)</code>\u3002 <code>msaa</code> <code>int</code> <code>4</code> \u591a\u91cd\u91c7\u6837\u6297\u952f\u9f7f\u7ea7\u522b\uff081/2/4/8\uff09\u3002"},{"location":"zh/api/#_8","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u63cf\u8ff0 <code>set_tensor(tensor)</code> \u4e0a\u4f20\u5f20\u91cf\u5230\u573a\u666f\u7eb9\u7406\u3002 <code>render()</code> \u5904\u7406\u9f20\u6807\u4ea4\u4e92\uff0c\u6e32\u67d3\u573a\u666f\uff0c\u5e76\u4f5c\u4e3a ImGui \u56fe\u50cf\u663e\u793a\u3002"},{"location":"zh/api/#_9","title":"\u4f7f\u7528\u793a\u4f8b","text":"<pre><code>scene = vultorch.SceneView(\"3D \u89c6\u56fe\", 800, 600, msaa=4)\n# \u5728\u5e27\u5faa\u73af\u4e2d\uff1a\nscene.set_tensor(tensor)\nscene.render()\n</code></pre>"},{"location":"zh/api/#api_1","title":"\u58f0\u660e\u5f0f API","text":"<p>\u58f0\u660e\u5f0f API \u63d0\u4f9b\u4e86\u66f4\u9ad8\u5c42\u7684\u62bd\u8c61\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u9762\u677f\u53ef\u89c6\u5316\u5e94\u7528\u3002</p>"},{"location":"zh/api/#vultorchview","title":"<code>vultorch.View</code>","text":"<pre><code>class View:\n    def __init__(self, title: str = \"Vultorch\",\n                 width: int = 1280, height: int = 720) -&gt; None: ...\n</code></pre> <p>\u652f\u6301\u81ea\u52a8\u505c\u9760\u5e03\u5c40\u7684\u9876\u5c42\u7a97\u53e3\u3002</p>"},{"location":"zh/api/#_10","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>panel(name, *, side, width)</code> <code>\u2192 Panel</code> \u521b\u5efa\u6216\u83b7\u53d6\u53ef\u505c\u9760\u9762\u677f\u3002<code>side</code>\uff1a<code>\"left\"</code> / <code>\"right\"</code> / <code>\"bottom\"</code> / <code>\"top\"</code> / <code>None</code>\u3002 <code>on_frame(fn)</code> <code>\u2192 fn</code> \u88c5\u9970\u5668 \u2014 \u6ce8\u518c\u6bcf\u5e27\u56de\u8c03\u51fd\u6570\u3002 <code>run()</code> <code>\u2192 None</code> \u963b\u585e\u5f0f\u4e8b\u4ef6\u5faa\u73af\u3002 <code>step()</code> <code>\u2192 bool</code> \u975e\u963b\u585e\uff1a\u5904\u7406\u4e00\u5e27\u3002\u5173\u95ed\u65f6\u8fd4\u56de <code>False</code>\u3002 <code>end_step()</code> <code>\u2192 None</code> \u7ed3\u675f\u7531 <code>step()</code> \u5f00\u59cb\u7684\u5e27\u3002 <code>close()</code> <code>\u2192 None</code> \u9500\u6bc1\u7a97\u53e3\u3002"},{"location":"zh/api/#_11","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u63cf\u8ff0 <code>fps</code> <code>float</code> \u5f53\u524d\u6bcf\u79d2\u5e27\u6570\u3002 <code>time</code> <code>float</code> \u5df2\u8fc7\u79d2\u6570\u3002 <code>window</code> <code>Window</code> \u5e95\u5c42 <code>Window</code> \u5b9e\u4f8b\u3002"},{"location":"zh/api/#_12","title":"\u4f7f\u7528\u793a\u4f8b \u2014 \u963b\u585e\u6a21\u5f0f","text":"<pre><code>view = vultorch.View(\"Demo\", 1280, 720)\nview.panel(\"Viewer\").canvas(\"img\").bind(tensor)\n\n@view.on_frame\ndef update():\n    speed = controls.slider(\"Speed\", 0, 10)\n    tensor[:,:,0] = (x + view.time * speed).sin()\n\nview.run()\n</code></pre>"},{"location":"zh/api/#_13","title":"\u4f7f\u7528\u793a\u4f8b \u2014 \u8bad\u7ec3\u5faa\u73af","text":"<pre><code>view = vultorch.View(\"Train\", 1024, 768)\noutput = view.panel(\"Output\").canvas(\"result\")\nfor epoch in range(100):\n    result = model(input)\n    output.bind(result)\n    if not view.step():\n        break\n    view.end_step()\nview.close()\n</code></pre>"},{"location":"zh/api/#vultorchpanel","title":"<code>vultorch.Panel</code>","text":"<pre><code>class Panel:\n    # \u901a\u8fc7 View.panel() \u521b\u5efa \u2014 \u4e0d\u76f4\u63a5\u5b9e\u4f8b\u5316\n</code></pre> <p>\u5305\u542b\u753b\u5e03\u548c\u63a7\u4ef6\u7684\u53ef\u505c\u9760\u9762\u677f\u3002</p>"},{"location":"zh/api/#_14","title":"\u753b\u5e03\u5de5\u5382","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>canvas(name, *, filter, fit)</code> <code>\u2192 Canvas</code> \u521b\u5efa\u547d\u540d\u753b\u5e03\u3002<code>filter</code>\uff1a<code>\"linear\"</code> / <code>\"nearest\"</code>\u3002<code>fit</code>\uff1a\u81ea\u52a8\u586b\u5145\u9762\u677f\u7a7a\u95f4\u3002"},{"location":"zh/api/#_15","title":"\u9762\u677f\u56de\u8c03","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>on_frame(fn)</code> <code>\u2192 fn</code> \u88c5\u9970\u5668 \u2014 \u6ce8\u518c\u5728\u9762\u677f ImGui \u7a97\u53e3\u5185\u8fd0\u884c\u7684\u6bcf\u5e27\u56de\u8c03\u3002"},{"location":"zh/api/#_16","title":"\u5e03\u5c40","text":"\u65b9\u6cd5 \u63cf\u8ff0 <code>row()</code> \u4e0a\u4e0b\u6587\u7ba1\u7406\u5668 \u2014 \u5c06\u5b50\u63a7\u4ef6\u5e76\u6392\u653e\u7f6e\u3002"},{"location":"zh/api/#_17","title":"\u63a7\u4ef6","text":"<p>\u6240\u6709\u63a7\u4ef6\u65b9\u6cd5\u81ea\u52a8\u7ba1\u7406\u8de8\u5e27\u72b6\u6001\u3002</p> \u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>text(text)</code> <code>\u2192 None</code> \u9759\u6001\u6587\u672c\u3002 <code>text_colored(r, g, b, a, text)</code> <code>\u2192 None</code> \u6709\u989c\u8272\u7684\u6587\u672c\u3002 <code>text_wrapped(text)</code> <code>\u2192 None</code> \u81ea\u52a8\u6362\u884c\u6587\u672c\u3002 <code>separator()</code> <code>\u2192 None</code> \u6c34\u5e73\u5206\u9694\u7ebf\u3002 <code>button(label, width, height)</code> <code>\u2192 bool</code> \u6309\u94ae\u3002\u70b9\u51fb\u65f6\u8fd4\u56de <code>True</code>\u3002<code>width</code>/<code>height</code> \u9ed8\u8ba4 <code>0</code>\uff08\u81ea\u52a8\u5927\u5c0f\uff09\u3002 <code>checkbox(label, *, default)</code> <code>\u2192 bool</code> \u5e26\u72b6\u6001\u5207\u6362\u7684\u590d\u9009\u6846\u3002 <code>slider(label, min, max, *, default)</code> <code>\u2192 float</code> \u6d6e\u70b9\u6ed1\u5757\u3002 <code>slider_int(label, min, max, *, default)</code> <code>\u2192 int</code> \u6574\u6570\u6ed1\u5757\u3002 <code>color_picker(label, *, default)</code> <code>\u2192 (r, g, b)</code> \u989c\u8272\u9009\u62e9\u5668\uff083 \u6d6e\u70b9\u5143\u7ec4\uff09\u3002 <code>combo(label, items, *, default)</code> <code>\u2192 int</code> \u4e0b\u62c9\u7ec4\u5408\u6846\u3002\u8fd4\u56de\u9009\u4e2d\u7d22\u5f15\u3002 <code>input_text(label, *, default, max_length)</code> <code>\u2192 str</code> \u6587\u672c\u8f93\u5165\u6846\u3002 <code>plot(values, *, label, overlay, width, height)</code> <code>\u2192 None</code> \u6d6e\u70b9\u5217\u8868\u7684\u6298\u7ebf\u56fe\u3002 <code>progress(fraction, *, overlay)</code> <code>\u2192 None</code> \u8fdb\u5ea6\u6761\uff080.0 \u2013 1.0\uff09\u3002"},{"location":"zh/api/#vultorchcanvas","title":"<code>vultorch.Canvas</code>","text":"<pre><code>class Canvas:\n    # \u901a\u8fc7 Panel.canvas() \u521b\u5efa \u2014 \u4e0d\u76f4\u63a5\u5b9e\u4f8b\u5316\n</code></pre> <p>\u5c06\u7ed1\u5b9a\u5f20\u91cf\u6e32\u67d3\u4e3a ImGui \u56fe\u50cf\u7684\u663e\u793a\u8868\u9762\u3002</p>"},{"location":"zh/api/#_18","title":"\u65b9\u6cd5","text":"\u65b9\u6cd5 \u7b7e\u540d \u63cf\u8ff0 <code>bind(tensor)</code> <code>\u2192 Canvas</code> \u7ed1\u5b9a\u5f20\u91cf\u7528\u4e8e\u663e\u793a\u3002\u8fd4\u56de <code>self</code> \u4ee5\u652f\u6301\u94fe\u5f0f\u8c03\u7528\u3002 <code>alloc(height, width, channels, device)</code> <code>\u2192 torch.Tensor</code> \u5206\u914d Vulkan \u5171\u4eab\u5185\u5b58\u5e76\u81ea\u52a8\u7ed1\u5b9a\u3002\u8fd4\u56de\u5f20\u91cf\u3002 <code>save(path, *, channels, size, quality)</code> <code>\u2192 None</code> \u901a\u8fc7 <code>imwrite()</code> \u5c06\u7ed1\u5b9a\u7684\u5f20\u91cf\u4fdd\u5b58\u4e3a\u56fe\u7247\u6587\u4ef6\u3002"},{"location":"zh/api/#_19","title":"\u5c5e\u6027","text":"\u5c5e\u6027 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>filter</code> <code>str</code> <code>\"linear\"</code> <code>\"linear\"</code> \u6216 <code>\"nearest\"</code>\u3002 <code>fit</code> <code>bool</code> <code>True</code> \u81ea\u52a8\u586b\u5145\u53ef\u7528\u9762\u677f\u7a7a\u95f4\u3002"},{"location":"zh/api/#imgui-vultorchui","title":"ImGui \u7ed1\u5b9a (<code>vultorch.ui</code>)","text":"<p><code>vultorch.ui</code> \u5b50\u6a21\u5757\u66b4\u9732\u4e86 Dear ImGui \u51fd\u6570\uff08docking \u5206\u652f\uff09\u3002\u6240\u6709\u51fd\u6570\u76f4\u63a5\u6620\u5c04\u5230\u5bf9\u5e94\u7684 ImGui C++ \u63a5\u53e3\u3002</p>"},{"location":"zh/api/#_20","title":"\u7a97\u53e3","text":"<pre><code>ui.begin(name: str, opened: bool = True, flags: int = 0) -&gt; tuple[bool, bool]\nui.end() -&gt; None\nui.begin_child(id: str, width=0.0, height=0.0, child_flags=0, window_flags=0) -&gt; bool\nui.end_child() -&gt; None\n</code></pre>"},{"location":"zh/api/#_21","title":"\u6587\u672c","text":"<pre><code>ui.text(text: str) -&gt; None\nui.text_colored(r, g, b, a, text: str) -&gt; None\nui.text_disabled(text: str) -&gt; None\nui.text_wrapped(text: str) -&gt; None\nui.label_text(label: str, text: str) -&gt; None\nui.bullet_text(text: str) -&gt; None\n</code></pre>"},{"location":"zh/api/#_22","title":"\u6309\u94ae","text":"<pre><code>ui.button(label: str, width=0.0, height=0.0) -&gt; bool\nui.small_button(label: str) -&gt; bool\nui.invisible_button(id: str, width, height) -&gt; bool\nui.arrow_button(id: str, direction: int) -&gt; bool\nui.radio_button(label: str, active: bool) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_23","title":"\u8f93\u5165","text":"<pre><code>ui.checkbox(label, value: bool) -&gt; bool\nui.slider_float(label, value, min=0.0, max=1.0, format=\"%.3f\") -&gt; float\nui.slider_float2(label, v1, v2, min, max) -&gt; tuple[float, float]\nui.slider_float3(label, v1, v2, v3, min, max) -&gt; tuple[float, float, float]\nui.slider_float4(label, v1, v2, v3, v4, min, max) -&gt; tuple\nui.slider_int(label, value, min=0, max=100) -&gt; int\nui.slider_angle(label, value, min=-360, max=360) -&gt; float\nui.drag_float(label, value, speed=1.0) -&gt; float\nui.drag_float2(label, v1, v2, speed=1.0) -&gt; tuple\nui.drag_float3(label, v1, v2, v3, speed=1.0) -&gt; tuple\nui.drag_int(label, value, speed=1.0) -&gt; int\nui.input_float(label, value) -&gt; float\nui.input_float2(label, v1, v2) -&gt; tuple\nui.input_float3(label, v1, v2, v3) -&gt; tuple\nui.input_float4(label, v1, v2, v3, v4) -&gt; tuple\nui.input_int(label, value) -&gt; int\nui.input_text(label, text, max_length=256) -&gt; str\nui.input_text_multiline(label, text, max_length=1024) -&gt; str\n</code></pre>"},{"location":"zh/api/#_24","title":"\u989c\u8272","text":"<pre><code>ui.color_edit3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_edit4(label, r, g, b, a, flags=0) -&gt; tuple[float, float, float, float]\nui.color_picker3(label, r, g, b, flags=0) -&gt; tuple[float, float, float]\nui.color_picker4(label, r, g, b, a, flags=0) -&gt; tuple\n</code></pre>"},{"location":"zh/api/#_25","title":"\u9009\u62e9","text":"<pre><code>ui.combo(label, current: int, items: list[str]) -&gt; int\nui.listbox(label, current: int, items: list[str], height_items=-1) -&gt; int\nui.tree_node(label: str) -&gt; bool\nui.tree_pop() -&gt; None\nui.collapsing_header(label: str) -&gt; bool\nui.selectable(label: str, selected: bool = False) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_26","title":"\u6807\u7b7e\u9875","text":"<pre><code>ui.begin_tab_bar(id: str) -&gt; bool\nui.end_tab_bar() -&gt; None\nui.begin_tab_item(label: str) -&gt; bool\nui.end_tab_item() -&gt; None\n</code></pre>"},{"location":"zh/api/#_27","title":"\u663e\u793a","text":"<pre><code>ui.progress_bar(fraction, sx=-1.0, sy=0.0, overlay=\"\")\nui.image(texture_id: int, width, height, uv0x=0, uv0y=0, uv1x=1, uv1y=1)\nui.image_button(id: str, texture_id: int, width, height) -&gt; bool\nui.plot_lines(label, values: list[float], offset=0, overlay=\"\", ...)\nui.plot_histogram(label, values: list[float], offset=0, overlay=\"\", ...)\n</code></pre>"},{"location":"zh/api/#_28","title":"\u5e03\u5c40","text":"<pre><code>ui.separator()\nui.same_line(offset=0.0, spacing=-1.0)\nui.new_line()\nui.spacing()\nui.dummy(width, height)\nui.indent(width=0.0)\nui.unindent(width=0.0)\nui.begin_group()\nui.end_group()\nui.push_item_width(width)\nui.pop_item_width()\nui.columns(count=1, id=None, border=True)\nui.next_column()\n</code></pre>"},{"location":"zh/api/#_29","title":"\u8868\u683c","text":"<pre><code>ui.begin_table(id: str, columns: int, flags=0) -&gt; bool\nui.end_table()\nui.table_next_row(flags=0, min_row_height=0.0)\nui.table_next_column() -&gt; bool\nui.table_set_column_index(index: int) -&gt; bool\nui.table_setup_column(label: str, flags=0, init_width=0.0)\nui.table_headers_row()\n</code></pre>"},{"location":"zh/api/#_30","title":"\u83dc\u5355","text":"<pre><code>ui.begin_main_menu_bar() -&gt; bool\nui.end_main_menu_bar()\nui.begin_menu_bar() -&gt; bool\nui.end_menu_bar()\nui.begin_menu(label: str, enabled=True) -&gt; bool\nui.end_menu()\nui.menu_item(label: str, shortcut=\"\", selected=False, enabled=True) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_31","title":"\u5f39\u51fa\u7a97\u53e3","text":"<pre><code>ui.open_popup(id: str)\nui.begin_popup(id: str) -&gt; bool\nui.begin_popup_modal(name: str, flags=0) -&gt; bool\nui.end_popup()\nui.close_current_popup()\n</code></pre>"},{"location":"zh/api/#_32","title":"\u63d0\u793a\u6846","text":"<pre><code>ui.begin_tooltip()\nui.end_tooltip()\nui.set_tooltip(text: str)\n</code></pre>"},{"location":"zh/api/#id","title":"ID \u6808","text":"<pre><code>ui.push_id_str(id: str)\nui.push_id_int(id: int)\nui.pop_id()\nui.get_id(id: str) -&gt; int\n</code></pre>"},{"location":"zh/api/#_33","title":"\u6837\u5f0f","text":"<pre><code>ui.push_style_color(idx: int, r, g, b, a)\nui.pop_style_color(count=1)\nui.push_style_var_float(idx: int, value: float)\nui.push_style_var_vec2(idx: int, x: float, y: float)\nui.pop_style_var(count=1)\nui.style_colors_dark()\nui.style_colors_light()\nui.style_colors_classic()\n</code></pre>"},{"location":"zh/api/#_34","title":"\u5149\u6807\u4e0e\u7a97\u53e3\u4fe1\u606f","text":"<pre><code>ui.get_cursor_pos() -&gt; tuple[float, float]\nui.set_cursor_pos(x, y)\nui.get_content_region_avail() -&gt; tuple[float, float]\nui.get_window_size() -&gt; tuple[float, float]\nui.get_window_pos() -&gt; tuple[float, float]\nui.set_next_window_pos(x, y, cond=0)\nui.set_next_window_size(width, height, cond=0)\n</code></pre>"},{"location":"zh/api/#_35","title":"\u505c\u9760","text":"<pre><code>ui.dock_space_over_viewport(flags=0) -&gt; int\nui.dock_space(id: int, sx=0.0, sy=0.0, flags=0) -&gt; int\nui.set_next_window_dock_id(dock_id: int, cond=0)\nui.dock_builder_add_node(node_id=0, flags=0) -&gt; int\nui.dock_builder_remove_node(node_id: int)\nui.dock_builder_set_node_size(node_id, width, height)\nui.dock_builder_set_node_pos(node_id, x, y)\nui.dock_builder_split_node(node_id, split_dir, ratio) -&gt; tuple[int, int]\nui.dock_builder_dock_window(window_name: str, node_id: int)\nui.dock_builder_finish(node_id: int)\nui.dock_builder_get_node(node_id: int) -&gt; int\n</code></pre>"},{"location":"zh/api/#_36","title":"\u7ed8\u56fe","text":"<pre><code>ui.draw_line(x1, y1, x2, y2, col=0xFFFFFFFF, thickness=1.0)\nui.draw_rect(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_rect_filled(x1, y1, x2, y2, col=0xFFFFFFFF)\nui.draw_circle(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_circle_filled(cx, cy, radius, col=0xFFFFFFFF)\nui.draw_text(x, y, col: int, text: str)\nui.bg_draw_image(texture_id, x1, y1, x2, y2)\n</code></pre>"},{"location":"zh/api/#_37","title":"\u8f93\u5165\u72b6\u6001","text":"<pre><code>ui.is_item_hovered() -&gt; bool\nui.is_item_active() -&gt; bool\nui.is_item_clicked() -&gt; bool\nui.is_item_focused() -&gt; bool\nui.is_item_edited() -&gt; bool\nui.is_item_deactivated_after_edit() -&gt; bool\nui.get_mouse_pos() -&gt; tuple[float, float]\nui.is_mouse_clicked(button: int) -&gt; bool\nui.is_mouse_double_clicked(button: int) -&gt; bool\nui.is_mouse_dragging(button: int, lock_threshold=-1.0) -&gt; bool\nui.get_mouse_drag_delta(button=0, lock_threshold=-1.0) -&gt; tuple[float, float]\nui.is_key_pressed(key: int) -&gt; bool\nui.is_key_down(key: int) -&gt; bool\n</code></pre>"},{"location":"zh/api/#_38","title":"\u5de5\u5177\u51fd\u6570","text":"<pre><code>ui.get_io_framerate() -&gt; float\nui.get_io_delta_time() -&gt; float\nui.get_time() -&gt; float\nui.get_frame_count() -&gt; int\nui.get_display_size() -&gt; tuple[float, float]\nui.col32(r: int, g: int, b: int, a: int = 255) -&gt; int\nui.show_demo_window()\nui.show_metrics_window()\n</code></pre>"},{"location":"zh/api/#_39","title":"\u5185\u90e8\u8f85\u52a9\u51fd\u6570","text":""},{"location":"zh/api/#vultorch_normalize_tensor","title":"<code>vultorch._normalize_tensor()</code>","text":"<pre><code>def _normalize_tensor(tensor) -&gt; tuple[Tensor, int, int, int]\n</code></pre> <p>\u89c4\u8303\u5316\u5f20\u91cf\u7684 dtype \u548c\u5f62\u72b6\u4ee5\u7528\u4e8e\u663e\u793a\u3002\u8fd4\u56de <code>(tensor, height, width, channels)</code>\u3002</p> <ul> <li><code>uint8</code> \u2192 <code>float32</code>\uff08\u00f7 255\uff09\uff0c<code>float16</code> \u2192 <code>float32</code>\u3002</li> <li>\u63a5\u53d7 2D <code>(H, W)</code> \u548c 3D <code>(H, W, C)</code>\uff0cC \u2208 {1, 3, 4}\u3002</li> <li>\u4e0d\u652f\u6301\u7684 dtype\u3001\u5f62\u72b6\u6216\u901a\u9053\u6570\u4f1a\u629b\u51fa <code>ValueError</code>\u3002</li> </ul>"}]}